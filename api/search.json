[{"id":"417b7d418cc13368077cc6eee9e1cd72","title":"大语言模型引言","content":"从这几年的工作经验说起，接触最多的莫过于思考大模型在工业制造领域的落地可能性。目前工业制造领域引入大模型，从降本提效角度出发，加速促进AI技术落地。\n但遇见了很多问题，无论是人文还是技术层面，很难进展。\n人文\n\n大部分企业并不看好LLM，只是期望从这个话题中引入资金\n从基础软件阶段，跳过大数据时代直接迈入人工智能时代是一个悖论\n数据管理过于拉垮，很多央国企数据中心还未将数据资产化\n央国企技术人员多为底层，无任何激励，无资源支持，工作之余无精力为事业进一步探索，全靠责任心无法进展\n二次分配不合理，大部分开发迫于工作压力无精力转行，LLM是一个长期投资方向，如果短期无收益，所在企业也不支持\n\n最终很多行业迫于项目验收压力，采购一系列显卡服务器硬件，以及阿里百度等研发的集成框架软件，美其名曰引入LLM。实则与本公司开发员工相距甚远，最终不了了之。\n技术\n\n企业数据资产化落伍严重，无数据支撑无法挖掘LLM经济潜力，而数据资产化也是长期投资\n动态数据和静态数据划分缺方法论。导致落地方为探索第一步。初期划分不合理，会导致严重资源浪费，经常会了解到有真正在落地的项目，每几个月就要重新训练一次大模型\n技术壁垒高，方法不确定，框架设计需要从顶层设计，大部分牛马没这个地位\n目前集成框架软件化，如OLLAMA、XInference等等，大部分企业一步慢步步慢，如果开发不懂底层集成逻辑，更难进一步调优，技术完全黑盒化\n\n现状就写到这里吧，越写越气。\n那么从个人角度出发，这是一个很好的接触新科技的方法。将分为以下几步进行\n\n学习集成框架底层\n学习模型调优、微调、训练手段\n学有余力研究剪枝等方法\n自己尝试做算法集管理\n尝试跟随智谱AI，做产品化\n\n","slug":"大语言模型引言","date":"2023-08-22T01:36:50.000Z","categories_index":"大语言模型LLM","tags_index":"人工智能,大语言模型LLM","author_index":"Quanito"},{"id":"6588ccbfb741c3887af0529d54933115","title":"SNN-Spike Neural Network","content":"谨以此博客纪念一下近一个月以来的意难平。\n踏足这个领域源于本周二的一封邮件。年轻人总是不屈于现状想在四方debuff中。时隔两年，回国工作之后语言水平直线下降，基础知识已然模糊。这些都是借口，奔三路上近乎一事无成，各种滋味，环境迫使，犹如溺水之孩。\n意难平意难平，为资质平庸所累，言语再多只有悲喜自渡。\nemo结束，言归正传，面试SNN的一个Phd offer。为图卢兹计算机研究院全奖课题Hybrid AI based on Spiking Neural Networks (SNNs)。同时了解了一点脉冲神经网络的基础知识。\n论文首先，第一篇论文出自于IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 14, NO. 6  题目为Simple Model of Spiking Neurons。下发为按要求整理的文章小结。\nIn this paper, a simple spiking model is presented. This model depends on 4 parameters which reproduces spiking and bursting behavior of known types of cortical neurons by reducing biophysically accurate Hodgkin-Huxley-type neuronal models to a 2D system:\n在本文中，提出了一个简单的脉冲模型。 该模型取决于 4 个参数，这些参数通过将生物物理学上准确的 Hodgkin-Huxley 型神经元模型简化为 2D 系统来重现已知类型皮层神经元的尖峰和爆发行为：\n \nWhere v,u,a,b,c,d are dimensionless.\n其中 v、u、a、b、c、d 是无量纲的。\nAs shown in Fig2, each inset shows a voltage response of the model neuron to a step of dc-current. Various choices of the parameters result in various patterns of spiking and bursting which can easily reproduce behavior of excitatory cortical cells (RS, IB, CH), inhibitory cortical cells (FS, LTS), thalamo-cortical neurons (TC) and other types of neurons (RZ). Dynamics of other neuronal types, including those in brainstem, hippocampus, basal ganglia, and olfactory bulb, can also be described.\n如图 2 所示，每个插图显示模型神经元对直流电流阶跃的电压响应。 参数的各种选择导致各种尖峰和爆发模式，可以轻松重现兴奋性皮层细胞（RS、IB、CH）、抑制性皮层细胞（FS、LTS）、丘脑-皮层神经元（TC）和其他类型神经元的行为 神经元 (RZ)。 还可以描述其他神经元类型的动力学，包括脑干、海马、基底神经节和嗅球中的神经元类型。\nExcitatory cortical neurons fire low frequency trains of action potentials with a spike frequency adaptation (called the spike frequency adaptation). TZ has 2 firing regimes depends on the +&#x2F;- current step delivered. RZ can be switched between resting and repetitive spiking states by an appropriately timed brief stimulus.\n兴奋性皮层神经元发射具有尖峰频率适应（称为尖峰频率适应）的低频动作电位序列。 TZ 有 2 种发射机制，具体取决于交付的 +&#x2F;- 当前步长。 RZ 可以通过适当定时的短暂刺激在静止状态和重复尖峰状态之间切换。\nA sparse network of randomly connected 1000 spiking cortical neurons in real time has been simulated using this model.  The ratio of excitatory to inhibitory neurons to be 4 to 1 and they used RS cells to model all excitatory neurons and FS cells to model all inhibitory neurons.\n使用该模型实时模拟了一个由随机连接的 1000 个尖峰皮层神经元组成的稀疏网络。 兴奋性神经元与抑制性神经元的比例为 4 比 1，他们使用 RS 细胞模拟所有兴奋性神经元，使用 FS 细胞模拟所有抑制性神经元。\nThe result shows that neurons fire Poisson spike trains with mean firing rates around 8 Hz and there are occasional episodes of synchronized firings in the alpha and gamma frequency range (10 and 40 Hz, respectively). This simple spiking model describes accurately dynamics of known types of cortical neurons. Thus, there is no contradiction between biological plausibility and computational efficiency of model neural networks\n结果表明，神经元以大约 8 Hz 的平均发射率发射泊松尖峰序列，并且偶尔会在 alpha 和 gamma 频率范围（分别为 10 和 40 Hz）内同步发射。 这个简单的尖峰模型准确地描述了已知类型的皮层神经元的动力学。 因此，模型神经网络的生物学合理性和计算效率之间不存在矛盾\nThe authors have also used this model to simulate a sparse network of 10 000 spiking cortical neurons with 1 000 000 synaptic connections in real time using 1 GHZ desktop PC which can simulate thalamo-cortical networks consisting of tens of thousands of spiking neurons in real time with 1 ms resolution.\n作者还使用该模型使用 1 GHZ 台式电脑实时模拟了由 10 000 个尖峰皮层神经元和 1 000 000 个突触连接组成的稀疏网络，该 PC 可以实时模拟由数万个尖峰神经元组成的丘脑-皮质网络 分辨率为 1 毫秒。\nSNN基础知识首先，SNN作为第三代神经网络，目前没有可用的package，模型构造方式和评估手段没有一个统一的标准。由于神经元采用脉冲函数，不具备线性算法的连续性和可导性，因此传统的BP无法使用。尽管近几年有人模拟出相似算法并应用于CIFAR和MNIST数据集，但是效果较于上一代BP神经网络还是略弱，但精确度随着研究的发展逐步提升。由于脉冲神经网络参数量级和能耗较上一代神经网络框架优势极大，因此相关的研究是很有必要的。\n参考项目：https://github.com/1250681923/Spiking-Neural-Network\n参考调查论文：https://www.researchgate.net/publication/361276255_Spiking_Neural_Networks_A_Survey（[IEEE Access](https://www.researchgate.net/journal/IEEE-Access-2169-3536) 10， June 2022）\nPrincipe SNN \n \nCompared with the ANN network with large amount of computation, SNN network can usually obtain lower power consumption.\n \n \nMethod of converting picture signal into spike signal:\n\nAs (a), we change the original pixel intensity of the sample to a binary value (usually normalized to [0,1])\n\nAs 3 (b). Use an encoder to generate a global spike signal. Each neuron of this encoder receives the intensity signal of multiple pixels of the picture as input and generates spike as output.\n\n\nHow to train SNNs\nIterative version of LIF model\n \nThe parameters of the iterative version of the LIF model can be updated (Using STBP) as follows:\n \nEvaluation\n\nRecognition accuracy\n\n \nWe first calculate the fire rate of each output neuron, that is, the spike rate, within the given time window T. Then take the neuron with the highest fire rate as the output.\nmeans the output of the neuron i in layer N at t\n\nCompute cost\n\nAlso as shown in https://github.com/1250681923/Spiking-Neural-Network\nSpike Time Dependent Plasticity\nSTDP is actually a biological process used by brain to modify its neural connections (synapses). Molding of weights is based on the following two rules:\n\nAny synapse that contribute to the firing of a post-synaptic neuron should be made strong. its value should be increased.\n\nSynapses that don’t contribute to the firing of a post-synaptic neuron should be limited. its value should be decreased.\n\n\n","slug":"SNN-Spike-Neural-Network","date":"2023-02-24T02:00:15.000Z","categories_index":"Deep Learning","tags_index":"深度学习,SNN","author_index":"Quanito"},{"id":"4a22d0b29df90690f72b32998c550a24","title":"XGBoost","content":"XGBoostXGBoost是一个非常高效、应用广泛的GBDT机器学习库，详细信息可参考xgbbostXGBoost提供了高效简洁的python接口，可用于分类、回归任务。在本实验中使用了xgboost的分类接口。\n下面是一个股票预测的例子：\n所需要的包 Module Requiredimport math\nimport matplotlib\nimport numpy as np\nimport pandas  as pd\nimport seaborn as sns\nimport time\n\nfrom datetime import date\nfrom matplotlib import pyplot as plt\nfrom pylab import rcParams\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm import tqdm\nfrom xgboost import XGBRegressor\n\n参数 Parameterstest_size &#x3D; 0.2                # proportion of dataset to be used as test set\nN &#x3D; 3                          # for feature at day t, we use lags from t-1, t-2, ..., t-N as features\n\nmodel_seed &#x3D; 100\n\n数据加载 Data Loadingdef _load_data():\n\n    #stk_path &#x3D; &quot;&#x2F;content&#x2F;drive&#x2F;MyDrive&#x2F;Colab Notebooks&#x2F;data_samples&#x2F;VTI.csv&quot;\n    stk_path &#x3D; &quot;.&#x2F;VTI.csv&quot;\n    df &#x3D; pd.read_csv(stk_path, sep&#x3D;&quot;,&quot;)\n    # Convert Date column to datetime\n    df.loc[:, &#39;Date&#39;] &#x3D; pd.to_datetime(df[&#39;Date&#39;], format&#x3D;&#39;%Y-%m-%d&#39;)\n    # Change all column headings to be lower case, and remove spacing\n    df.columns &#x3D; [str(x).lower().replace(&#39; &#39;, &#39;_&#39;) for x in df.columns]\n    # Get month of each sample\n    df[&#39;month&#39;] &#x3D; df[&#39;date&#39;].dt.month\n    # Sort by datetime\n    df.sort_values(by&#x3D;&#39;date&#39;, inplace&#x3D;True, ascending&#x3D;True)\n\n    return df\n\n特征工程 Feature engineering生成新特征：最高最低价差、开盘收盘价差；前N天的信息拼入当天，作为当天的特征Generate new features: highest and lowest price difference, opening and closing price difference; The information of the previous N days is put into the current day as the characteristics of the current day\ndef feature_engineer(df):\n\n    df[&#39;range_hl&#39;] &#x3D; df[&#39;high&#39;] - df[&#39;low&#39;]\n    df[&#39;range_oc&#39;] &#x3D; df[&#39;open&#39;] - df[&#39;close&#39;]\n\n    lag_cols &#x3D; [&#39;adj_close&#39;, &#39;range_hl&#39;, &#39;range_oc&#39;, &#39;volume&#39;]\n    shift_range &#x3D; [x + 1 for x in range(N)]\n    for col in lag_cols:\n        for i in shift_range:\n            new_col&#x3D;&#39;&#123;&#125;_lag_&#123;&#125;&#39;.format(col, i)   # 格式化字符串\n            df[new_col]&#x3D;df[col].shift(i)\n\n    return df[N:]\n\n数据标准化 Data standardizationXGBoost的本质为树模型，树模型本身无需做数据标准化。但是，在树模型中，若训练集不能代表总体，即测试集中出现训练集中从未出现的特征及回归值（训练数据过少或股价前所未有的高&#x2F;底），模型的效果基本没有效果。\n因此，需要对数据做“非常规”的标准化做折中，以使模型能正常运行。这里的“非常规”指计算前N天内的标准差和均值，为当天数据的标准化作准备。这样做会丢失一部分数据信息，仅仅是为了让树模型正常运行的一种取巧。\ndef get_mov_avg_std(df, col, N):\n    &quot;&quot;&quot;\n    Given a dataframe, get mean and std dev at timestep t using values from t-1, t-2, ..., t-N.\n    Inputs\n        df         : dataframe. Can be of any length.\n        col        : name of the column you want to calculate mean and std dev\n        N          : get mean and std dev at timestep t using values from t-1, t-2, ..., t-N\n    Outputs\n        df_out     : same as df but with additional column containing mean and std dev\n    &quot;&quot;&quot;\n    mean_list &#x3D; df[col].rolling(window&#x3D;N, min_periods&#x3D;1).mean()  # len(mean_list) &#x3D; len(df)\n    std_list &#x3D; df[col].rolling(window&#x3D;N, min_periods&#x3D;1).std()  # first value will be NaN, because normalized by N-1\n\n    # Add one timestep to the predictions ,这里又shift了一步\n    mean_list &#x3D; np.concatenate((np.array([np.nan]), np.array(mean_list[:-1])))\n    std_list &#x3D; np.concatenate((np.array([np.nan]), np.array(std_list[:-1])))\n\n    # Append mean_list to df\n    df_out &#x3D; df.copy()\n    df_out[col + &#39;_mean&#39;] &#x3D; mean_list\n    df_out[col + &#39;_std&#39;] &#x3D; std_list\n\n    return df_out\n\n\ndef scale_row(row, feat_mean, feat_std):\n    &quot;&quot;&quot;\n    Given a pandas series in row, scale it to have 0 mean and var 1 using feat_mean and feat_std\n    Inputs\n        row      : pandas series. Need to scale this.\n        feat_mean: mean\n        feat_std : standard deviation\n    Outputs\n        row_scaled : pandas series with same length as row, but scaled\n    &quot;&quot;&quot;\n    # If feat_std &#x3D; 0 (this happens if adj_close doesn&#39;t change over N days),\n    # set it to a small number to avoid division by zero\n    feat_std &#x3D; 0.001 if feat_std &#x3D;&#x3D; 0 else feat_std\n    row_scaled &#x3D; (row - feat_mean) &#x2F; feat_std\n\n    return row_scaled\n\n计算绝对百分比误差 MAPEdef get_mape(y_true, y_pred):\n    &quot;&quot;&quot;\n    Compute mean absolute percentage error (MAPE)\n    &quot;&quot;&quot;\n    y_true, y_pred &#x3D; np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) &#x2F; y_true)) * 100\n\n整体流程 Main Process第一步：获取数据 Step 1: Data Loadingdata_df&#x3D;_load_data()\n\n\ndata_df.head()\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n\n.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n\n\n\n  \n    \n      \n      date\n      open\n      high\n      low\n      close\n      adj_close\n      volume\n      month\n    \n  \n  \n    \n      0\n      2015-11-25\n      107.510002\n      107.660004\n      107.250000\n      107.470001\n      101.497200\n      1820300\n      11\n    \n    \n      1\n      2015-11-27\n      107.589996\n      107.760002\n      107.220001\n      107.629997\n      101.648300\n      552400\n      11\n    \n    \n      2\n      2015-11-30\n      107.779999\n      107.849998\n      107.110001\n      107.169998\n      101.213867\n      3618100\n      11\n    \n    \n      3\n      2015-12-01\n      107.589996\n      108.209999\n      107.370003\n      108.180000\n      102.167740\n      2443600\n      12\n    \n    \n      4\n      2015-12-02\n      108.099998\n      108.269997\n      106.879997\n      107.050003\n      101.100533\n      2937200\n      12\n    \n  \n\n\n\n\n\n\n第二步：特征工程 Step 2: Feature engineeringdf&#x3D;feature_engineer(data_df)\n\n\ndf.head()\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n\n.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n\n\n\n  \n    \n      \n      date\n      open\n      high\n      low\n      close\n      adj_close\n      volume\n      month\n      range_hl\n      range_oc\n      ...\n      adj_close_lag_3\n      range_hl_lag_1\n      range_hl_lag_2\n      range_hl_lag_3\n      range_oc_lag_1\n      range_oc_lag_2\n      range_oc_lag_3\n      volume_lag_1\n      volume_lag_2\n      volume_lag_3\n    \n  \n  \n    \n      3\n      2015-12-01\n      107.589996\n      108.209999\n      107.370003\n      108.180000\n      102.167740\n      2443600\n      12\n      0.839996\n      -0.590004\n      ...\n      101.497200\n      0.739997\n      0.540001\n      0.410004\n      0.610001\n      -0.040001\n      0.040001\n      3618100.0\n      552400.0\n      1820300.0\n    \n    \n      4\n      2015-12-02\n      108.099998\n      108.269997\n      106.879997\n      107.050003\n      101.100533\n      2937200\n      12\n      1.390000\n      1.049995\n      ...\n      101.648300\n      0.839996\n      0.739997\n      0.540001\n      -0.590004\n      0.610001\n      -0.040001\n      2443600.0\n      3618100.0\n      552400.0\n    \n    \n      5\n      2015-12-03\n      107.290001\n      107.480003\n      105.059998\n      105.449997\n      99.589470\n      3345600\n      12\n      2.420005\n      1.840004\n      ...\n      101.213867\n      1.390000\n      0.839996\n      0.739997\n      1.049995\n      -0.590004\n      0.610001\n      2937200.0\n      2443600.0\n      3618100.0\n    \n    \n      6\n      2015-12-04\n      105.809998\n      107.540001\n      105.620003\n      107.389999\n      101.421646\n      4520000\n      12\n      1.919998\n      -1.580001\n      ...\n      102.167740\n      2.420005\n      1.390000\n      0.839996\n      1.840004\n      1.049995\n      -0.590004\n      3345600.0\n      2937200.0\n      2443600.0\n    \n    \n      7\n      2015-12-07\n      107.230003\n      107.269997\n      106.059998\n      106.550003\n      100.628342\n      3000500\n      12\n      1.209999\n      0.680000\n      ...\n      101.100533\n      1.919998\n      2.420005\n      1.390000\n      -1.580001\n      1.840004\n      1.049995\n      4520000.0\n      3345600.0\n      2937200.0\n    \n  \n\n5 rows × 22 columns\n\n\n\n\n\n第三步：数据标准化 Step 3: Data standardization# 先统一计算出标准化的数据，在对其进行数据切分。\ncols_list &#x3D; [\n    &quot;adj_close&quot;,\n    &quot;range_hl&quot;,\n    &quot;range_oc&quot;,\n    &quot;volume&quot;\n]\nfor col in cols_list:\n    df &#x3D; get_mov_avg_std(df, col, N)\n\n\ndf.head()\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n\n.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n\n\n\n  \n    \n      \n      date\n      open\n      high\n      low\n      close\n      adj_close\n      volume\n      month\n      range_hl\n      range_oc\n      ...\n      volume_lag_2\n      volume_lag_3\n      adj_close_mean\n      adj_close_std\n      range_hl_mean\n      range_hl_std\n      range_oc_mean\n      range_oc_std\n      volume_mean\n      volume_std\n    \n  \n  \n    \n      3\n      2015-12-01\n      107.589996\n      108.209999\n      107.370003\n      108.180000\n      102.167740\n      2443600\n      12\n      0.839996\n      -0.590004\n      ...\n      552400.0\n      1820300.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      2015-12-02\n      108.099998\n      108.269997\n      106.879997\n      107.050003\n      101.100533\n      2937200\n      12\n      1.390000\n      1.049995\n      ...\n      3618100.0\n      552400.0\n      102.167740\n      NaN\n      0.839996\n      NaN\n      -0.590004\n      NaN\n      2.443600e+06\n      NaN\n    \n    \n      5\n      2015-12-03\n      107.290001\n      107.480003\n      105.059998\n      105.449997\n      99.589470\n      3345600\n      12\n      2.420005\n      1.840004\n      ...\n      2443600.0\n      3618100.0\n      101.634136\n      0.754629\n      1.114998\n      0.388912\n      0.229995\n      1.159654\n      2.690400e+06\n      349027.907194\n    \n    \n      6\n      2015-12-04\n      105.809998\n      107.540001\n      105.620003\n      107.389999\n      101.421646\n      4520000\n      12\n      1.919998\n      -1.580001\n      ...\n      2937200.0\n      2443600.0\n      100.952581\n      1.295487\n      1.550000\n      0.802064\n      0.766665\n      1.239533\n      2.908800e+06\n      451670.145128\n    \n    \n      7\n      2015-12-07\n      107.230003\n      107.269997\n      106.059998\n      106.550003\n      100.628342\n      3000500\n      12\n      1.209999\n      0.680000\n      ...\n      3345600.0\n      2937200.0\n      100.703883\n      0.978374\n      1.910001\n      0.515075\n      0.436666\n      1.790597\n      3.600933e+06\n      821711.806738\n    \n  \n\n5 rows × 30 columns\n\n\n\n\n\n第四步：生成训练数据和测试数据 Step 4: Train&#x2F;Test Data Generation# 因训练数据和测试数据的标准化方式不同，因此需切分训练和测试数据。\nnum_test &#x3D; int(test_size * len(df))\nnum_train &#x3D; len(df) - num_test\ntrain &#x3D; df[:num_train]\ntest &#x3D; df[num_train:]\n\n\ntrain.head()\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n\n.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n\n\n\n  \n    \n      \n      date\n      open\n      high\n      low\n      close\n      adj_close\n      volume\n      month\n      range_hl\n      range_oc\n      ...\n      volume_lag_2\n      volume_lag_3\n      adj_close_mean\n      adj_close_std\n      range_hl_mean\n      range_hl_std\n      range_oc_mean\n      range_oc_std\n      volume_mean\n      volume_std\n    \n  \n  \n    \n      3\n      2015-12-01\n      107.589996\n      108.209999\n      107.370003\n      108.180000\n      102.167740\n      2443600\n      12\n      0.839996\n      -0.590004\n      ...\n      552400.0\n      1820300.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      2015-12-02\n      108.099998\n      108.269997\n      106.879997\n      107.050003\n      101.100533\n      2937200\n      12\n      1.390000\n      1.049995\n      ...\n      3618100.0\n      552400.0\n      102.167740\n      NaN\n      0.839996\n      NaN\n      -0.590004\n      NaN\n      2.443600e+06\n      NaN\n    \n    \n      5\n      2015-12-03\n      107.290001\n      107.480003\n      105.059998\n      105.449997\n      99.589470\n      3345600\n      12\n      2.420005\n      1.840004\n      ...\n      2443600.0\n      3618100.0\n      101.634136\n      0.754629\n      1.114998\n      0.388912\n      0.229995\n      1.159654\n      2.690400e+06\n      349027.907194\n    \n    \n      6\n      2015-12-04\n      105.809998\n      107.540001\n      105.620003\n      107.389999\n      101.421646\n      4520000\n      12\n      1.919998\n      -1.580001\n      ...\n      2937200.0\n      2443600.0\n      100.952581\n      1.295487\n      1.550000\n      0.802064\n      0.766665\n      1.239533\n      2.908800e+06\n      451670.145128\n    \n    \n      7\n      2015-12-07\n      107.230003\n      107.269997\n      106.059998\n      106.550003\n      100.628342\n      3000500\n      12\n      1.209999\n      0.680000\n      ...\n      3345600.0\n      2937200.0\n      100.703883\n      0.978374\n      1.910001\n      0.515075\n      0.436666\n      1.790597\n      3.600933e+06\n      821711.806738\n    \n  \n\n5 rows × 30 columns\n\n\n\n\n\n\ntest.head()\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n\n.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n\n\n\n  \n    \n      \n      date\n      open\n      high\n      low\n      close\n      adj_close\n      volume\n      month\n      range_hl\n      range_oc\n      ...\n      volume_lag_2\n      volume_lag_3\n      adj_close_mean\n      adj_close_std\n      range_hl_mean\n      range_hl_std\n      range_oc_mean\n      range_oc_std\n      volume_mean\n      volume_std\n    \n  \n  \n    \n      605\n      2018-04-24\n      138.100006\n      138.190002\n      134.860001\n      135.800003\n      134.584366\n      3053500\n      4\n      3.330001\n      2.300003\n      ...\n      1669600.0\n      4003900.0\n      136.616023\n      0.644186\n      1.383331\n      0.241734\n      0.589997\n      0.407800\n      2.569067e+06\n      1.255867e+06\n    \n    \n      606\n      2018-04-25\n      135.770004\n      136.250000\n      134.610001\n      135.949997\n      134.733032\n      2275400\n      4\n      1.639999\n      -0.179993\n      ...\n      2033700.0\n      1669600.0\n      135.691040\n      0.958728\n      2.106669\n      1.069313\n      1.230001\n      0.995943\n      2.252267e+06\n      7.173725e+05\n    \n    \n      607\n      2018-04-26\n      136.520004\n      137.679993\n      136.250000\n      137.240005\n      136.011490\n      1284600\n      4\n      1.429993\n      -0.720001\n      ...\n      3053500.0\n      2033700.0\n      135.179001\n      0.904249\n      2.106669\n      1.069313\n      0.816671\n      1.309668\n      2.454200e+06\n      5.328931e+05\n    \n    \n      608\n      2018-04-27\n      137.539993\n      137.740005\n      136.800003\n      137.330002\n      136.100677\n      1133600\n      4\n      0.940002\n      0.209991\n      ...\n      2275400.0\n      3053500.0\n      135.109629\n      0.784564\n      2.133331\n      1.041653\n      0.466670\n      1.610508\n      2.204500e+06\n      8.865788e+05\n    \n    \n      609\n      2018-04-30\n      137.690002\n      137.990005\n      136.250000\n      136.330002\n      135.109634\n      2277100\n      4\n      1.740005\n      1.360000\n      ...\n      1284600.0\n      2275400.0\n      135.615066\n      0.765165\n      1.336665\n      0.359210\n      -0.230001\n      0.467008\n      1.564533e+06\n      6.202409e+05\n    \n  \n\n5 rows × 30 columns\n\n\n\n\n\n第五步：标签和特征的标准化 Step 5: Label &amp; Feature Standardization# 此步的目的是为了对在训练集不能代表总体的情况下，使树模型正确运行的一种取巧\ncols_to_scale &#x3D; [\n    &quot;adj_close&quot;\n]\nfor i in range(1, N + 1):\n    cols_to_scale.append(&quot;adj_close_lag_&quot; + str(i))\n    cols_to_scale.append(&quot;range_hl_lag_&quot; + str(i))\n    cols_to_scale.append(&quot;range_oc_lag_&quot; + str(i))\n    cols_to_scale.append(&quot;volume_lag_&quot; + str(i))\n\nscaler &#x3D; StandardScaler() # 启示三：标准化也不应带测试集，以避免信息泄漏\ntrain_scaled &#x3D; scaler.fit_transform(train[cols_to_scale])\n# Convert the numpy array back into pandas dataframe\ntrain_scaled &#x3D; pd.DataFrame(train_scaled, columns&#x3D;cols_to_scale)\ntrain_scaled[[&#39;date&#39;, &#39;month&#39;]] &#x3D; train.reset_index()[[&#39;date&#39;, &#39;month&#39;]]\n\ntest_scaled &#x3D; test[[&#39;date&#39;]]\nfor col in tqdm(cols_list):\n    feat_list &#x3D; [col + &#39;_lag_&#39; + str(shift) for shift in range(1, N + 1)]\n    temp &#x3D; test.apply(lambda row: scale_row(row[feat_list], row[col + &#39;_mean&#39;], row[col + &#39;_std&#39;]), axis&#x3D;1)\n    test_scaled &#x3D; pd.concat([test_scaled, temp], axis&#x3D;1)\n\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00&lt;00:00, 26.77it/s]\n\n第六步：建立样本 Step 6: Sample Creationfeatures &#x3D; []\nfor i in range(1, N + 1):\n    features.append(&quot;adj_close_lag_&quot; + str(i))\n    features.append(&quot;range_hl_lag_&quot; + str(i))\n    features.append(&quot;range_oc_lag_&quot; + str(i))\n    features.append(&quot;volume_lag_&quot; + str(i))\n\ntarget &#x3D; &quot;adj_close&quot;\n\nX_train &#x3D; train[features]\ny_train &#x3D; train[target]\nX_sample &#x3D; test[features]\ny_sample &#x3D; test[target]\n\nX_train_scaled &#x3D; train_scaled[features]\ny_train_scaled &#x3D; train_scaled[target]\nX_sample_scaled &#x3D; test_scaled[features]\n\n第七步：训练 Step 7: Trainingfrom sklearn.model_selection import GridSearchCV\n\n\nparameters&#x3D;&#123;&#39;n_estimators&#39;:[90],\n            &#39;max_depth&#39;:[7],\n            &#39;learning_rate&#39;: [0.3],\n            &#39;min_child_weight&#39;:range(5, 21, 1),\n            #&#39;subsample&#39;:[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1],\n            #&#39;gamma&#39;:[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1],\n            #&#39;colsample_bytree&#39;:[0.5, 0.6, 0.7, 0.8, 0.9, 1],\n            #&#39;colsample_bylevel&#39;:[0.5, 0.6, 0.7, 0.8, 0.9, 1]\n            &#125;\n#parameters&#x3D;&#123;&#39;max_depth&#39;:range(2,10,1)&#125;\nmodel&#x3D;XGBRegressor(seed&#x3D;model_seed,\n                     n_estimators&#x3D;100,\n                     max_depth&#x3D;3,\n                     eval_metric&#x3D;&#39;rmse&#39;,\n                     learning_rate&#x3D;0.1,\n                     min_child_weight&#x3D;1,\n                     subsample&#x3D;1,\n                     colsample_bytree&#x3D;1,\n                     colsample_bylevel&#x3D;1,\n                     gamma&#x3D;0)\ngs&#x3D;GridSearchCV(estimator&#x3D; model,param_grid&#x3D;parameters,cv&#x3D;5,refit&#x3D; True,scoring&#x3D;&#39;neg_mean_squared_error&#39;)\n\ngs.fit(X_train_scaled,y_train_scaled)\nprint (&#39;最优参数: &#39; + str(gs.best_params_))\n\nest_scaled &#x3D; gs.predict(X_train_scaled)\ntrain[&#39;est&#39;] &#x3D; est_scaled * math.sqrt(scaler.var_[0]) + scaler.mean_[0]\n\npre_y_scaled &#x3D; gs.predict(X_sample_scaled)\ntest[&#39;pre_y_scaled&#39;] &#x3D; pre_y_scaled\ntest[&#39;pre_y&#39;]&#x3D;test[&#39;pre_y_scaled&#39;] * test[&#39;adj_close_std&#39;] + test[&#39;adj_close_mean&#39;]\n\nplt.figure()\nax &#x3D; test.plot(x&#x3D;&#39;date&#39;, y&#x3D;&#39;adj_close&#39;, style&#x3D;&#39;b-&#39;, grid&#x3D;True)\nax &#x3D; test.plot(x&#x3D;&#39;date&#39;, y&#x3D;&#39;pre_y&#39;, style&#x3D;&#39;r-&#39;, grid&#x3D;True, ax&#x3D;ax)\nplt.show()\n\nrmse&#x3D;math.sqrt(mean_squared_error(y_sample, test[&#39;pre_y&#39;]))\nprint(&quot;RMSE on dev set &#x3D; %0.3f&quot; % rmse)\nmape &#x3D; get_mape(y_sample, test[&#39;pre_y&#39;])\nprint(&quot;MAPE on dev set &#x3D; %0.3f%%&quot; % mape)\n\nimp &#x3D; list(zip(train[features], gs.best_estimator_.feature_importances_))\nimp.sort(key&#x3D;lambda tup: tup[1])\nfor i in range(-1,-10,-1):\n    print(imp[i])\n\n最优参数: &#123;&#39;learning_rate&#39;: 0.3, &#39;max_depth&#39;: 7, &#39;min_child_weight&#39;: 7, &#39;n_estimators&#39;: 90&#125;\n\n\n/var/folders/5m/mg8r4rhs2hgg96ylwdvgvxrr0000gn/T/ipykernel_200/735584386.py:27: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  train[&#39;est&#39;] = est_scaled * math.sqrt(scaler.var_[0]) + scaler.mean_[0]\n/var/folders/5m/mg8r4rhs2hgg96ylwdvgvxrr0000gn/T/ipykernel_200/735584386.py:30: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test[&#39;pre_y_scaled&#39;] = pre_y_scaled\n/var/folders/5m/mg8r4rhs2hgg96ylwdvgvxrr0000gn/T/ipykernel_200/735584386.py:31: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test[&#39;pre_y&#39;]=test[&#39;pre_y_scaled&#39;] * test[&#39;adj_close_std&#39;] + test[&#39;adj_close_mean&#39;]\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\nRMSE on dev set = 1.149\nMAPE on dev set = 0.581%\n(&#39;adj_close_lag_1&#39;, 0.8624093)\n(&#39;adj_close_lag_2&#39;, 0.10811194)\n(&#39;adj_close_lag_3&#39;, 0.026851863)\n(&#39;range_oc_lag_1&#39;, 0.00043455773)\n(&#39;volume_lag_2&#39;, 0.0003508884)\n(&#39;volume_lag_3&#39;, 0.00034092006)\n(&#39;range_hl_lag_2&#39;, 0.00028984202)\n(&#39;range_oc_lag_2&#39;, 0.00027450564)\n(&#39;range_oc_lag_3&#39;, 0.0002714092)\n\n","slug":"XGBoost","date":"2023-02-10T07:21:14.000Z","categories_index":"机器学习Machine Learning","tags_index":"Python,机器学习,推荐系统,XGBoost","author_index":"Quanito"},{"id":"41e8b66af73631eb10964784b17cc5fa","title":"AutoXGB(XGboost + optuna)","content":"简介 Introduction源代码地址（Source code url）\nXGBoost + Optuna: no brainer\n\nauto train xgboost directly from CSV files\nauto tune xgboost using optuna \nauto serve best xgboost model using fastapi\n\nAutoXGB是XGBoost和Optuna的结合，允许\n\n在CSV文件上自动训练xgboost模型\n对xgboost使用optuna自动参数调优\n使用fastapi提供最优xgboost模型\n\n安装方法 Installation使用pip进行安装Install using pip\npip install autoxgb\n\n使用 Usagefrom autoxgb import AutoXGB\n\n参数 Parameters###############################################################################\n### required parameters\n###############################################################################\n\n# path to training data\ntrain_filename &#x3D; &quot;data_samples&#x2F;binary_classification.csv&quot;\n\n# The path on my Colab\n# train_filename &#x3D; &quot;&#x2F;content&#x2F;drive&#x2F;MyDrive&#x2F;Colab Notebooks&#x2F;data_samples&#x2F;binary_classification.csv&quot;\n\n\n# path to output folder to store artifacts\noutput &#x3D; &quot;output&quot;\n\n###############################################################################\n### optional parameters\n###############################################################################\n\n# path to test data. if specified, the model will be evaluated on the test data\n# and test_predictions.csv will be saved to the output folder\n# if not specified, only OOF predictions will be saved\n# test_filename &#x3D; &quot;test.csv&quot;\ntest_filename &#x3D; None\n\n# task: classification or regression\n# if not specified, the task will be inferred automatically\n# task &#x3D; &quot;classification&quot;\n# task &#x3D; &quot;regression&quot;\ntask &#x3D; None\n\n# an id column\n# if not specified, the id column will be generated automatically with the name &#96;id&#96;\n# idx &#x3D; &quot;id&quot;\nidx &#x3D; None\n\n# target columns are list of strings\n# if not specified, the target column be assumed to be named &#96;target&#96;\n# and the problem will be treated as one of: binary classification, multiclass classification,\n# or single column regression\n# targets &#x3D; [&quot;target&quot;]\n# targets &#x3D; [&quot;target1&quot;, &quot;target2&quot;]\ntargets &#x3D; [&quot;income&quot;]\n\n# features columns are list of strings\n# if not specified, all columns except &#96;id&#96;, &#96;targets&#96; &amp; &#96;kfold&#96; columns will be used\n# features &#x3D; [&quot;col1&quot;, &quot;col2&quot;]\nfeatures &#x3D; None\n\n# categorical_features are list of strings\n# if not specified, categorical columns will be inferred automatically\n# categorical_features &#x3D; [&quot;col1&quot;, &quot;col2&quot;]\ncategorical_features &#x3D; None\n\n# use_gpu is boolean\n# if not specified, GPU is not used\n# use_gpu &#x3D; True\n# use_gpu &#x3D; False\nuse_gpu &#x3D; True\n\n# number of folds to use for cross-validation\n# default is 5\nnum_folds &#x3D; 5\n\n# random seed for reproducibility\n# default is 42\nseed &#x3D; 42\n\n# number of optuna trials to run\n# default is 1000\n# num_trials &#x3D; 1000\nnum_trials &#x3D; 100\n\n# time_limit for optuna trials in seconds\n# if not specified, timeout is not set and all trials are run\n# time_limit &#x3D; None\ntime_limit &#x3D; 360\n\n# if fast is set to True, the hyperparameter tuning will use only one fold\n# however, the model will be trained on all folds in the end\n# to generate OOF predictions and test predictions\n# default is False\n# fast &#x3D; False\nfast &#x3D; False\n\n用python API训练# Now its time to train the model!\naxgb &#x3D; AutoXGB(\n    train_filename&#x3D;train_filename,\n    output&#x3D;output,\n    test_filename&#x3D;test_filename,\n    task&#x3D;task,\n    idx&#x3D;idx,\n    targets&#x3D;targets,\n    features&#x3D;features,\n    categorical_features&#x3D;categorical_features,\n    use_gpu&#x3D;use_gpu,\n    num_folds&#x3D;num_folds,\n    seed&#x3D;seed,\n    num_trials&#x3D;num_trials,\n    time_limit&#x3D;time_limit,\n    fast&#x3D;fast,\n)\naxgb.train()\n\n用CLI训练# Train the model using the autoxgb train command. The parameters are same as above.\nautoxgb train \\\n --train_filename datasets&#x2F;30train.csv \\\n --output outputs&#x2F;30days \\\n --test_filename datasets&#x2F;30test.csv \\\n --use_gpu\n\n\n# You can also serve the trained model using the autoxgb serve command.\nautoxgb serve --model_path outputs&#x2F;mll --host 0.0.0.0 --debug\n\n\n&#96;autoxgb &lt;command&gt; --help&#96; \n\n训练过程 Training Process\n","slug":"AutoXGB","date":"2023-02-09T10:23:39.000Z","categories_index":"机器学习Machine Learning","tags_index":"Python,机器学习,推荐系统,XGBoost","author_index":"Quanito"},{"id":"939229761d9e9e6b8785ea116cde36b3","title":"DLNotes","content":"iris-鸢尾花# 导入相关的库\nimport seaborn as sns\nimport numpy as np\n\n\n# 机器学习：sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegressionCV\n\n\n# 深度学习：tf.keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras import utils\n\n数据处理# 加载数据\niris &#x3D; sns.load_dataset(&#39;iris&#39;)\n\n\niris.shape\n\n\n\n\n(150, 5)\n\n\n\n\niris.head()\n\n\n.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n\n\nsns.pairplot(iris,hue&#x3D;&quot;species&quot;)\n\n\n\n\n&lt;seaborn.axisgrid.PairGrid at 0x11fcb1240&gt;\n\n\n\n# 获得数据集的特征值和目标值\nX &#x3D; iris.values[:,:4]\ny &#x3D; iris.values[:,4]\n\n\n# 数据集划分\ntrain_x,test_x,train_y,test_y &#x3D; train_test_split(X,y,test_size &#x3D; 0.5,random_state&#x3D;0)\n\n\ntrain_x.shape\n\n\n\n\n(75, 4)\n\n\n\n\ntest_x.shape\n\n\n\n\n(75, 4)\n\n\n\nsklearn 实现鸢尾花# 使用逻辑回归\n# 实例化估计器\nlr &#x3D; LogisticRegressionCV()\n\n\n# 模型训练\nlr.fit(train_x,train_y)\n\n&#x2F;opt&#x2F;anaconda3&#x2F;envs&#x2F;dlcv&#x2F;lib&#x2F;python3.6&#x2F;site-packages&#x2F;sklearn&#x2F;linear_model&#x2F;_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status&#x3D;1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https:&#x2F;&#x2F;scikit-learn.org&#x2F;stable&#x2F;modules&#x2F;preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https:&#x2F;&#x2F;scikit-learn.org&#x2F;stable&#x2F;modules&#x2F;linear_model.html#logistic-regression\n  extra_warning_msg&#x3D;_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n&#x2F;opt&#x2F;anaconda3&#x2F;envs&#x2F;dlcv&#x2F;lib&#x2F;python3.6&#x2F;site-packages&#x2F;sklearn&#x2F;linear_model&#x2F;_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status&#x3D;1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https:&#x2F;&#x2F;scikit-learn.org&#x2F;stable&#x2F;modules&#x2F;preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https:&#x2F;&#x2F;scikit-learn.org&#x2F;stable&#x2F;modules&#x2F;linear_model.html#logistic-regression\n  extra_warning_msg&#x3D;_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n&#x2F;opt&#x2F;anaconda3&#x2F;envs&#x2F;dlcv&#x2F;lib&#x2F;python3.6&#x2F;site-packages&#x2F;sklearn&#x2F;linear_model&#x2F;_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status&#x3D;1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https:&#x2F;&#x2F;scikit-learn.org&#x2F;stable&#x2F;modules&#x2F;preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https:&#x2F;&#x2F;scikit-learn.org&#x2F;stable&#x2F;modules&#x2F;linear_model.html#logistic-regression\n  extra_warning_msg&#x3D;_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n&#x2F;opt&#x2F;anaconda3&#x2F;envs&#x2F;dlcv&#x2F;lib&#x2F;python3.6&#x2F;site-packages&#x2F;sklearn&#x2F;linear_model&#x2F;_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status&#x3D;1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https:&#x2F;&#x2F;scikit-learn.org&#x2F;stable&#x2F;modules&#x2F;preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https:&#x2F;&#x2F;scikit-learn.org&#x2F;stable&#x2F;modules&#x2F;linear_model.html#logistic-regression\n  extra_warning_msg&#x3D;_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n&#x2F;opt&#x2F;anaconda3&#x2F;envs&#x2F;dlcv&#x2F;lib&#x2F;python3.6&#x2F;site-packages&#x2F;sklearn&#x2F;linear_model&#x2F;_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status&#x3D;1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https:&#x2F;&#x2F;scikit-learn.org&#x2F;stable&#x2F;modules&#x2F;preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https:&#x2F;&#x2F;scikit-learn.org&#x2F;stable&#x2F;modules&#x2F;linear_model.html#logistic-regression\n  extra_warning_msg&#x3D;_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n\n\n\n\n\nLogisticRegressionCV()\n\n\n\n\n# 模型评估\nlr.score(test_x,test_y)\n\n\n\n\n0.9333333333333333\n\n\n\ntf,keras实现数据处理# 目标值的热编码\ndef one_hot_encode(arr):\n    # 获取目标值中的所有类别斌进行热编码\n    uniques,ids &#x3D; np.unique(arr,return_inverse&#x3D;True)\n    return utils.to_categorical(ids,len(uniques))\n\n\n# 对目标值进行编码\ntrain_y_ohe &#x3D; one_hot_encode(train_y)\n\n\ntest_y_ohe &#x3D; one_hot_encode(test_y)\n\n模型构建# 通过squential进行构建\nmodel &#x3D; Sequential([\n    # 隐藏层\n    Dense(10,activation&#x3D;&quot;relu&quot;,input_shape&#x3D;(4,)),\n    # 隐藏层\n    Dense(10,activation&#x3D;&quot;relu&quot;),\n    # 输出层\n    Dense(3,activation&#x3D;&quot;softmax&quot;)\n])\n\n\nmodel.summary()\n\nModel: &quot;sequential&quot;\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\ndense (Dense)                (None, 10)                50        \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                110       \n_________________________________________________________________\ndense_2 (Dense)              (None, 3)                 33        \n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\nTotal params: 193\nTrainable params: 193\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nutils.plot_model(model,show_shapes&#x3D;True)\n\n\n模型预测与评估# 模型编译\nmodel.compile(optimizer&#x3D;&quot;adam&quot;,loss&#x3D;&quot;categorical_crossentropy&quot;,metrics&#x3D;[&quot;accuracy&quot;])\n\n\n# 类型准换\ntrain_x &#x3D; np.array(train_x,dtype&#x3D; np.float32)\ntest_x &#x3D; np.array(test_x,dtype&#x3D;np.float32)\n\n\n# 模型训练\nmodel.fit(train_x,train_y_ohe,epochs&#x3D;10,batch_size&#x3D;1,verbose&#x3D;1)\n\nEpoch 1&#x2F;10\n75&#x2F;75 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 0s 582us&#x2F;step - loss: 1.6949 - accuracy: 0.2667\nEpoch 2&#x2F;10\n75&#x2F;75 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 0s 498us&#x2F;step - loss: 1.0696 - accuracy: 0.3867\nEpoch 3&#x2F;10\n75&#x2F;75 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 0s 523us&#x2F;step - loss: 0.7988 - accuracy: 0.7200\nEpoch 4&#x2F;10\n75&#x2F;75 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 0s 516us&#x2F;step - loss: 0.6704 - accuracy: 0.7467\nEpoch 5&#x2F;10\n75&#x2F;75 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 0s 480us&#x2F;step - loss: 0.5823 - accuracy: 0.7733\nEpoch 6&#x2F;10\n75&#x2F;75 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 0s 478us&#x2F;step - loss: 0.5058 - accuracy: 0.8000\nEpoch 7&#x2F;10\n75&#x2F;75 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 0s 468us&#x2F;step - loss: 0.4638 - accuracy: 0.8667\nEpoch 8&#x2F;10\n75&#x2F;75 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 0s 489us&#x2F;step - loss: 0.4091 - accuracy: 0.8533\nEpoch 9&#x2F;10\n75&#x2F;75 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 0s 472us&#x2F;step - loss: 0.3607 - accuracy: 0.9067\nEpoch 10&#x2F;10\n75&#x2F;75 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 0s 478us&#x2F;step - loss: 0.3224 - accuracy: 0.9467\n\n\n\n\n\n&lt;tensorflow.python.keras.callbacks.History at 0x14329a400&gt;\n\n\n\n\n# 模型评估\nloss,accuracy &#x3D; model.evaluate(test_x,test_y_ohe,verbose&#x3D;1)\n\n3&#x2F;3 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 0s 702us&#x2F;step - loss: 0.4016 - accuracy: 0.7467\n\n\n\nloss\n\n\n\n\n0.4016245901584625\n\n\n\n\naccuracy\n\n\n\n\n0.746666669845581\n\n\n\n\n深度学习基础激活函数# 导入所需要的库\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nsigmoidx &#x3D; np.linspace(-10,10,1000)\ny &#x3D; tf.nn.sigmoid(x)\n\n\nplt.plot(x,y)\nplt.grid()\n\n\n\ntanhx &#x3D; np.linspace(-10,10,100)\ny &#x3D; tf.nn.tanh(x)\n\n\nplt.plot(x,y)\nplt.grid()\n\n\n\nrelux &#x3D; np.linspace(-10,10,100)\ny &#x3D; tf.nn.relu(x)\n\n\nplt.plot(x,y)\nplt.grid()\n\n\n\nleakyrelux &#x3D; np.linspace(-10,10,100)\ny &#x3D; tf.nn.leaky_relu(x)\n\n\nplt.plot(x,y)\nplt.grid()\n\n \nsoftmaxx &#x3D; tf.constant([0.2,0.02,0.15,1.3,0.5,0.06,1.1,0.05,3.75])\ny &#x3D; tf.nn.softmax(x)\n\n\ny\n\n\n\n\n&lt;tf.Tensor: shape&#x3D;(9,), dtype&#x3D;float32, numpy&#x3D;\narray([0.02167152, 0.01810158, 0.02061459, 0.06510484, 0.02925349,\n       0.01884031, 0.05330333, 0.01865285, 0.75445753], dtype&#x3D;float32)&gt;\n\n\n\n参数初始化import tensorflow as tf\n\nXavizer初始化# 正态分布的\n# 实例化\ninitializer &#x3D; tf.keras.initializers.glorot_normal()\nvalues &#x3D; initializer((9,1))\n\n\nvalues\n\n\n\n\n&lt;tf.Tensor: shape&#x3D;(9, 1), dtype&#x3D;float32, numpy&#x3D;\narray([[-0.24432576],\n       [-0.14220025],\n       [ 0.03321752],\n       [ 0.56275785],\n       [ 0.17512582],\n       [-0.18233004],\n       [-0.44970375],\n       [-0.2163621 ],\n       [ 0.37192827]], dtype&#x3D;float32)&gt;\n\n\n\n\n# 标准化：均匀分布\ninitializern &#x3D; tf.keras.initializers.glorot_uniform()\nvalues &#x3D; initializern((9,1))\n\n\nvalues\n\n\n\n\n&lt;tf.Tensor: shape&#x3D;(9, 1), dtype&#x3D;float32, numpy&#x3D;\narray([[ 0.5761199 ],\n       [-0.42968088],\n       [ 0.5325227 ],\n       [-0.18232065],\n       [-0.59441626],\n       [ 0.7678894 ],\n       [-0.6589678 ],\n       [-0.75305176],\n       [-0.24894887]], dtype&#x3D;float32)&gt;\n\n\n\nHe初始化# 正态分布\n# 实例化\ninitializer &#x3D; tf.keras.initializers.he_normal()\n# 采样得到权重\nvalues &#x3D; initializer((9,1))\n\n\nvalues\n\n\n\n\n&lt;tf.Tensor: shape&#x3D;(9, 1), dtype&#x3D;float32, numpy&#x3D;\narray([[ 0.64234483],\n       [ 0.8432255 ],\n       [ 0.56904906],\n       [ 0.2679259 ],\n       [ 0.06764679],\n       [-0.36957997],\n       [ 0.27991033],\n       [ 0.51744246],\n       [-0.15655899]], dtype&#x3D;float32)&gt;\n\n\n\n\n# 标准化：均匀分布\ninitializer &#x3D; tf.keras.initializers.he_uniform()\nvalues &#x3D; initializer((9,1))\n\n\nvalues\n\n\n\n\n&lt;tf.Tensor: shape&#x3D;(9, 1), dtype&#x3D;float32, numpy&#x3D;\narray([[ 0.1383363 ],\n       [ 0.5817473 ],\n       [-0.24477297],\n       [-0.4477986 ],\n       [ 0.6623007 ],\n       [-0.60182106],\n       [-0.6880068 ],\n       [-0.5368612 ],\n       [-0.04563677]], dtype&#x3D;float32)&gt;\n\n\n\n神经网络的搭建sequential方式# 导入工具包\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport tensorflow.keras.layers as layers\n\n\n# 定义model,构建模型\nmodel &#x3D; keras.Sequential([\n    # 第一个隐层\n    layers.Dense(3, activation&#x3D;&quot;relu&quot;, kernel_initializer&#x3D;&quot;he_normal&quot;, name&#x3D;&quot;layer1&quot;,input_shape&#x3D;(3,)),\n    # 第二个隐层\n    layers.Dense(2, activation&#x3D;&quot;relu&quot;,\n                 kernel_initializer&#x3D;&quot;he_normal&quot;, name&#x3D;&quot;layer2&quot;),\n    # 输出层\n    layers.Dense(2, activation&#x3D;&quot;sigmoid&quot;,\n                 kernel_initializer&#x3D;&quot;he_normal&quot;, name&#x3D;&quot;layer3&quot;)\n    ],\n    name&#x3D;&quot;sequential&quot;\n)\n\n\nmodel.summary()\n\nModel: &quot;sequential&quot;\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\nlayer1 (Dense)               (None, 3)                 12        \n_________________________________________________________________\nlayer2 (Dense)               (None, 2)                 8         \n_________________________________________________________________\nlayer3 (Dense)               (None, 2)                 6         \n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\nTotal params: 26\nTrainable params: 26\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nkeras.utils.plot_model(model)\n\n\n\n\n\n利用functional API构建模型# 定义模型的输入\ninputs &#x3D; keras.Input(shape&#x3D;(3,), name&#x3D;&#39;input&#39;)\n# 第一个隐层\nx &#x3D; layers.Dense(3, activation&#x3D;&quot;relu&quot;, name&#x3D;&quot;layer1&quot;)(inputs)\n# 第二个隐层\nx &#x3D; layers.Dense(2, activation&#x3D;&quot;relu&quot;, name&#x3D;&quot;layer2&quot;)(x)\n# 输出层\noutputs &#x3D; layers.Dense(2, activation&#x3D;&quot;sigmoid&quot;, name&#x3D;&quot;output&quot;)(x)\n# 创建模型\nmodel &#x3D; keras.Model(inputs&#x3D;inputs, outputs&#x3D;outputs,\n                    name&#x3D;&quot;Functional API Model&quot;)\n\n\nmodel.summary()\n\nModel: &quot;Functional API Model&quot;\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\ninput (InputLayer)           [(None, 3)]               0         \n_________________________________________________________________\nlayer1 (Dense)               (None, 3)                 12        \n_________________________________________________________________\nlayer2 (Dense)               (None, 2)                 8         \n_________________________________________________________________\noutput (Dense)               (None, 2)                 6         \n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\nTotal params: 26\nTrainable params: 26\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nkeras.utils.plot_model(model,show_shapes&#x3D;True)\n\n\n通过model子类构建模型# 定义一个model的子类\nclass MyModel(keras.Model):\n    # 定义网络的层结构\n    def __init__(self):\n        super(MyModel,self).__init__()\n        # 第一层隐层\n        self.layer1 &#x3D; layers.Dense(3,activation&#x3D;&quot;relu&quot;,name&#x3D;&quot;layer1&quot;)\n        # 第二个隐层\n        self.layer2 &#x3D; layers.Dense(2,activation&#x3D;&quot;relu&quot;,name&#x3D;&quot;layer2&quot;)\n        # 输出层\n        self.layer3 &#x3D; layers.Dense(2,activation&#x3D;&quot;sigmoid&quot;,name &#x3D; &quot;layer3&quot;)\n    # 定义网络的前向传播\n    def call(self,inputs):\n        x &#x3D; self.layer1(inputs)\n        x &#x3D; self.layer2(x)\n        outputs &#x3D; self.layer3(x)\n        return outputs\n\n\n# 实例化moxing\nmodel &#x3D; MyModel()\n# 设置输入\nx &#x3D; tf.ones((1,3))\ny &#x3D; model(x)\n\n\ny\n\n\n\n\n&lt;tf.Tensor: shape&#x3D;(1, 2), dtype&#x3D;float32, numpy&#x3D;array([[0.5810978, 0.5847459]], dtype&#x3D;float32)&gt;\n\n\n\n\nmodel.summary()\n\nModel: &quot;my_model&quot;\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\nlayer1 (Dense)               multiple                  12        \n_________________________________________________________________\nlayer2 (Dense)               multiple                  8         \n_________________________________________________________________\nlayer3 (Dense)               multiple                  6         \n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\nTotal params: 26\nTrainable params: 26\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nkeras.utils.plot_model(model)\n\n\n\n\n损失函数交叉熵损失import tensorflow as tf\n\n\n# 设置真实值和预测值\ny_true &#x3D; [[0,1,0],[0,0,1]]\ny_pre &#x3D; [[0.05,0.9,0.05],[0.05,0.05,0.9]]\n# 实例化交叉熵损失\ncce &#x3D; tf.keras.losses.CategoricalCrossentropy()\n# 计算损失结果\ncce(y_true,y_pre)\n\n二分类的交叉熵损失函数# 设置真实值和预测值\ny_true &#x3D; [[0],[1]]\ny_pre &#x3D; [[0.1],[0.9]]\n# 实例化\nbce &#x3D; tf.keras.losses.BinaryCrossentropy()\n# 计算损失函数\nbce(y_true,y_pre).numpy()\n\n\n\n\n0.10536041\n\n\n\nMAE(L1 LOSS)# 设置真实值和预测值\ny_true &#x3D; [[0.],[1.]]\ny_pre &#x3D; [[0.],[1.]]\n# 实例化MAE损失\nmae &#x3D; tf.keras.losses.MeanAbsoluteError()\nmae(y_true,y_pre)\n\n\n\n\n&lt;tf.Tensor: shape&#x3D;(), dtype&#x3D;float32, numpy&#x3D;0.0&gt;\n\n\n\nMSE(L2 loss)# 设置真实值和预测值\ny_true &#x3D; [[0.],[1.]]\ny_pre &#x3D; [[0.],[1.]]\n# 实例化MSE\nmse &#x3D; tf.keras.losses.MeanSquaredError()\nmse(y_true,y_pre)\n\n\n\n\n&lt;tf.Tensor: shape&#x3D;(), dtype&#x3D;float32, numpy&#x3D;0.0&gt;\n\n\n\nsmoothL1# 设置真实值和预测值\ny_true &#x3D; [[0.2],[1.]]\ny_pre &#x3D; [[0.2],[0.6]]\n# 实例化损失\nsmooth &#x3D; tf.keras.losses.Huber()\nsmooth(y_true,y_pre)\n\n\n\n\n&lt;tf.Tensor: shape&#x3D;(), dtype&#x3D;float32, numpy&#x3D;0.039999995&gt;\n\n\n优化方法梯度下降算法# 导入工具包\nimport tensorflow as tf\n\n\n# 实例化SGD\nopt &#x3D; tf.keras.optimizers.SGD(learning_rate&#x3D;0.1)\n# 定义要更新的参数\nvar &#x3D; tf.Variable(1.0)\n# 定义损失函数\nloss &#x3D; lambda:(var**2)&#x2F;2.0\n# 计算损失梯度，并进行参数更新\nopt.minimize(loss,[var]).numpy()\n# 参数更新结果\nvar.numpy()\n\n\n\n\n0.9\n\nmomentum# 实例化\nopt &#x3D; tf.keras.optimizers.SGD(learning_rate&#x3D;0.1,momentum&#x3D;0.9)\n# 定义要更新的参数\nvar &#x3D; tf.Variable(1.0)\nval0&#x3D; var.value()\n# 定义损失函数\nloss &#x3D; lambda:(var**2)&#x2F;2.0\n# 第一次更新\nopt.minimize(loss,[var]).numpy()\nval1 &#x3D; var.value()\n# 第二次更新\nopt.minimize(loss,[var]).numpy()\nval2 &#x3D; var.value()\n\n\nval0\n\n\n\n\n&lt;tf.Tensor: shape&#x3D;(), dtype&#x3D;float32, numpy&#x3D;1.0&gt;\n\n\n\n\nval1\n\n\n\n\n&lt;tf.Tensor: shape&#x3D;(), dtype&#x3D;float32, numpy&#x3D;0.9&gt;\n\n\n\n\nval2\n\n\n\n\n&lt;tf.Tensor: shape&#x3D;(), dtype&#x3D;float32, numpy&#x3D;0.71999997&gt;\n\n\n\n\nval0-val1\n\n\n\n\n&lt;tf.Tensor: shape&#x3D;(), dtype&#x3D;float32, numpy&#x3D;0.100000024&gt;\n\n\n\n\nval1-val2\n\n\n\n\n&lt;tf.Tensor: shape&#x3D;(), dtype&#x3D;float32, numpy&#x3D;0.18&gt;\n\n\n\nadagrad# 实例化\nopt &#x3D; tf.keras.optimizers.Adagrad(learning_rate&#x3D;0.1,initial_accumulator_value&#x3D;0.1,epsilon&#x3D;1e-06)\n# 定义要更新的参数\nvar &#x3D; tf.Variable(1.0)\n# 定义损失函数\ndef loss():\n    return (var**2)&#x2F;2.0\n# 进行更新\nopt.minimize(loss,[var]).numpy()\n\n\n\n\n1\n\n\n\n\nvar\n\n\n\n\n&lt;tf.Variable &#39;Variable:0&#39; shape&#x3D;() dtype&#x3D;float32, numpy&#x3D;0.90465385&gt;\n\n\n\nRMSprop# 实例化\nopt &#x3D; tf.keras.optimizers.RMSprop(learning_rate&#x3D;0.1,rho&#x3D;0.1)\n# 定义要更新的参数\nvar &#x3D; tf.Variable(1.0)\n# 定义损失函数\ndef loss():\n    return (var**2)&#x2F;2.0\n# 进行更新\nopt.minimize(loss,[var]).numpy()\nvar.numpy()\n\n\n\n\n0.89459074\n\n\n\nAdam# 实例化\nopt &#x3D; tf.keras.optimizers.Adam(learning_rate&#x3D;0.1)\n# 定义要调整的参数\nvar &#x3D; tf.Variable(1.0)\n# 定义损失函数\ndef loss():\n    return (var**2)&#x2F;2.0\n# 进行更新\nopt.minimize(loss,[var])\n# 显示结果\nvar.numpy()\n\n\n\n\n0.90000033\n\n\n\n\n正则化dropout# 导入工具包\nimport tensorflow as tf\nimport numpy as np\n\n\n# 定义dropout层\nlayer &#x3D; tf.keras.layers.Dropout(0,input_shape&#x3D;(2,))\n# 定义输入数据\ndata &#x3D; np.arange(1,11).reshape(5,2).astype(np.float32)\nprint(data)\n# 对输入数据进行随机失活\noutputs &#x3D; layer(data,training&#x3D;True)\nprint(outputs)\n\n[[ 1.  2.]\n [ 3.  4.]\n [ 5.  6.]\n [ 7.  8.]\n [ 9. 10.]]\ntf.Tensor(\n[[ 1.  2.]\n [ 3.  4.]\n [ 5.  6.]\n [ 7.  8.]\n [ 9. 10.]], shape&#x3D;(5, 2), dtype&#x3D;float32)\n\n\n提前停止# 定义回调函数\ncallback &#x3D; tf.keras.callbacks.EarlyStopping(monitor&#x3D;&quot;loss&quot;,patience&#x3D;3)\n# 定义一层的网络\nmodel &#x3D; tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\n# 模型编译\nmodel.compile(tf.keras.optimizers.SGD(),loss&#x3D;&#39;mse&#39;)\n# 模型训练\nhistory &#x3D; model.fit(np.arange(100).reshape(5,20),np.array([0,1,0,1,0]),epochs&#x3D;10,batch_size&#x3D;1,verbose&#x3D;1)\nlen(history.history[&#39;loss&#39;])\n\nEpoch 1&#x2F;10\n5&#x2F;5 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 0s 645us&#x2F;step - loss: 56302425635553280.0000\nEpoch 2&#x2F;10\n5&#x2F;5 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 0s 1ms&#x2F;step - loss: 3809886121814745262099957060468736.0000\nEpoch 3&#x2F;10\n5&#x2F;5 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 0s 868us&#x2F;step - loss: inf\nEpoch 4&#x2F;10\n5&#x2F;5 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 0s 964us&#x2F;step - loss: inf\nEpoch 5&#x2F;10\n5&#x2F;5 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 0s 761us&#x2F;step - loss: nan\nEpoch 6&#x2F;10\n5&#x2F;5 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 0s 709us&#x2F;step - loss: nan\nEpoch 7&#x2F;10\n5&#x2F;5 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 0s 805us&#x2F;step - loss: nan\nEpoch 8&#x2F;10\n5&#x2F;5 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 0s 746us&#x2F;step - loss: nan\nEpoch 9&#x2F;10\n5&#x2F;5 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 0s 801us&#x2F;step - loss: nan\nEpoch 10&#x2F;10\n5&#x2F;5 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 0s 555us&#x2F;step - loss: nan\n\n\n\n10\n\n\nminist数据集# 导入所需的工具包\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# tf中使用工具包\nimport tensorflow as tf\n# 构建模型\nfrom tensorflow.keras.models import Sequential\n# 相关的网络层，所有神经元在Dense层里，Dropout随机失活（最常用的正则化方法），激活函数，BN层\nfrom tensorflow.keras.layers import Dense,Dropout,Activation,BatchNormalization\n# 导入辅助工具包\nfrom tensorflow.keras import utils\n# 正则化\nfrom tensorflow.keras import regularizers\n# 数据集\nfrom tensorflow.keras.datasets import mnist\n\n数据加载# 加载数据集\n(x_train,y_train),(x_test,y_test) &#x3D; mnist.load_data()\n\n\nx_train.shape\n\n\n\n\n(60000, 28, 28)\n\n\n\n\nx_test.shape\n\n\n\n\n(10000, 28, 28)\n\n\n\n\ny_train\n\n\n\n\narray([5, 0, 4, ..., 5, 6, 8], dtype&#x3D;uint8)\n\n\n\n\n# 显示数据\n#创建画布\nplt.figure()\n#输出\nplt.imshow(x_train[10000],cmap&#x3D;&quot;gray&quot;)\n\n\n\n\n&lt;matplotlib.image.AxesImage at 0x182df7f6be0&gt;\n\n\n\n\ny_train[10000]\n\n\n\n\n3\n\n\n\n数据处理# 数据维度的调整\nx_train &#x3D; x_train.reshape(60000,784)\nx_test &#x3D; x_test.reshape(10000,784)\n\n\n# 数据类型调整\nx_train &#x3D; x_train.astype(&#39;float32&#39;)\nx_test &#x3D; x_test.astype(&quot;float32&quot;)\n\n\n# 归一化\nx_train &#x3D; x_train&#x2F;255\nx_test &#x3D; x_test&#x2F;255\n\n\n# 将目标值转换成热编码的形式\ny_train &#x3D; utils.to_categorical(y_train,10)\ny_test &#x3D; utils.to_categorical(y_test,10)\n\n\ny_train.shape\n\n\n(60000, 10)\n\n\n\n模型构建# 使用序列模型进行构建\nmodel &#x3D; Sequential()\n# 全连接层：2个隐层，一个输出层\n# 第一个隐层:512个神经元，先激活后BN，随机失活\nmodel.add(Dense(512,activation &#x3D; &quot;relu&quot;,input_shape&#x3D;(784,)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n# 第二个隐层：512个神经元，先BN后激活，随机失活\nmodel.add(Dense(512,kernel_regularizer&#x3D;regularizers.l2(0.01)))\nmodel.add(BatchNormalization())\nmodel.add(Activation(&quot;relu&quot;))\nmodel.add(Dropout(0.2))\n# 输出层\nmodel.add(Dense(10,activation&#x3D;&quot;softmax&quot;))\n\n\nmodel.summary()\n\nModel: &quot;sequential_1&quot;\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\ndense_2 (Dense)              (None, 512)               401920    \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 512)               2048      \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 512)               262656    \n_________________________________________________________________\nbatch_normalization_3 (Batch (None, 512)               2048      \n_________________________________________________________________\nactivation (Activation)      (None, 512)               0         \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 10)                5130      \n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\nTotal params: 673,802\nTrainable params: 671,754\nNon-trainable params: 2,048\n_________________________________________________________________\n\n\n\nutils.plot_model(model)\n\n\n\n\n模型编译# 损失函数，优化器，评价指标\nmodel.compile(loss&#x3D; tf.keras.losses.categorical_crossentropy,optimizer &#x3D; tf.keras.optimizers.Adam(),\n              metrics&#x3D;tf.keras.metrics.Accuracy())\n\n模型训练# 使用fit,指定训练集，epochs,batch_size,val,verbose\nhistory &#x3D; model.fit(x_train,y_train,epochs&#x3D;4,batch_size&#x3D;128,validation_data&#x3D;(x_test,y_test),verbose&#x3D;1)\n\nEpoch 1&#x2F;4\n469&#x2F;469 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 2s 4ms&#x2F;step - loss: 0.1235 - accuracy: 0.0088 - val_loss: 0.1619 - val_accuracy: 0.0124\nEpoch 2&#x2F;4\n469&#x2F;469 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 2s 4ms&#x2F;step - loss: 0.1225 - accuracy: 0.0084 - val_loss: 0.1584 - val_accuracy: 0.0093\nEpoch 3&#x2F;4\n469&#x2F;469 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 2s 4ms&#x2F;step - loss: 0.1179 - accuracy: 0.0084 - val_loss: 0.1595 - val_accuracy: 0.0120\nEpoch 4&#x2F;4\n469&#x2F;469 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 2s 5ms&#x2F;step - loss: 0.1113 - accuracy: 0.0091 - val_loss: 0.1399 - val_accuracy: 0.0057\n\n\n\nhistory.history\n\n\n\n\n&#123;&#39;loss&#39;: [0.12347722053527832,\n  0.1224995031952858,\n  0.11792848259210587,\n  0.11130338907241821],\n &#39;accuracy&#39;: [0.008786666207015514,\n  0.00840499997138977,\n  0.008430000394582748,\n  0.009065000340342522],\n &#39;val_loss&#39;: [0.16186854243278503,\n  0.15844473242759705,\n  0.15949705243110657,\n  0.13993136584758759],\n &#39;val_accuracy&#39;: [0.012380000203847885,\n  0.009259999729692936,\n  0.011950000189244747,\n  0.0056500001810491085]&#125;\n\n\n\n\n# 损失函数\nplt.figure()\nplt.plot(history.history[&#39;loss&#39;],label&#x3D;&quot;train&quot;)\nplt.plot(history.history[&quot;val_loss&quot;],label&#x3D;&quot;val&quot;)\nplt.legend()\nplt.grid()\n\n\n\n# 准确率\nplt.figure()\nplt.plot(history.history[&#39;accuracy&#39;],label&#x3D;&quot;train&quot;)\nplt.plot(history.history[&quot;val_accuracy&quot;],label&#x3D;&quot;val&quot;)\nplt.legend()\nplt.grid()\n\n\n\n# 回调函数\ntensorboard &#x3D; tf.keras.callbacks.TensorBoard(log_dir &#x3D; &quot;.&#x2F;graph&quot;)\n\n\n# 训练\nhistory &#x3D; model.fit(x_train,y_train,epochs&#x3D;4,validation_data&#x3D;(x_test,y_test),batch_size&#x3D;128,\n                    verbose&#x3D;1,callbacks&#x3D;[tensorboard])\n\nEpoch 1&#x2F;4\n  1&#x2F;469 [..............................] - ETA: 0s - loss: 0.1043 - accuracy: 0.0109WARNING:tensorflow:From &#x2F;opt&#x2F;anaconda3&#x2F;envs&#x2F;dlcv&#x2F;lib&#x2F;python3.6&#x2F;site-packages&#x2F;tensorflow&#x2F;python&#x2F;ops&#x2F;summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\nInstructions for updating:\nuse &#96;tf.profiler.experimental.stop&#96; instead.\nWARNING:tensorflow:Callbacks method &#96;on_train_batch_end&#96; is slow compared to the batch time (batch time: 0.0062s vs &#96;on_train_batch_end&#96; time: 0.0188s). Check your callbacks.\n469&#x2F;469 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 2s 5ms&#x2F;step - loss: 0.1092 - accuracy: 0.0093 - val_loss: 0.1541 - val_accuracy: 0.0091\nEpoch 2&#x2F;4\n469&#x2F;469 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 2s 4ms&#x2F;step - loss: 0.1070 - accuracy: 0.0096 - val_loss: 0.1400 - val_accuracy: 0.0076\nEpoch 3&#x2F;4\n469&#x2F;469 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 2s 4ms&#x2F;step - loss: 0.1059 - accuracy: 0.0097 - val_loss: 0.1472 - val_accuracy: 0.0090\nEpoch 4&#x2F;4\n469&#x2F;469 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 2s 5ms&#x2F;step - loss: 0.1018 - accuracy: 0.0111 - val_loss: 0.1462 - val_accuracy: 0.0136\n\n\n模型评估model.evaluate(x_test,y_test,verbose&#x3D;1)\n\n313&#x2F;313 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 0s 1ms&#x2F;step - loss: 0.1462 - accuracy: 0.0136\n\n[0.1461515724658966, 0.013629999943077564]\n\n\n\n模型保存# 保存\nmodel.save(&quot;model.h5&quot;)\n\n\n# 记载\nloadmodel &#x3D; tf.keras.models.load_model(&quot;model.h5&quot;)\n\n\nloadmodel.evaluate(x_test,y_test,verbose&#x3D;1)\n\n313&#x2F;313 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 0s 1ms&#x2F;step - loss: 0.1462 - accuracy: 0.0136\n\n[0.1461515724658966, 0.013629999943077564]\n\n\n\n\n\n\n\nLeNet-5import tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\n\n数据集加载(train_images,train_labels),(test_images,test_labels) &#x3D; mnist.load_data()\n\n\ntrain_images.shape\n\n\n(60000, 28, 28)\n\n\ntest_images.shape\n\n\n(10000, 28, 28)\n\n\ntrain_labels.shape\n\n\n(60000,)\n\n\n\n数据处理# 维度调整\ntrain_images &#x3D; tf.reshape(train_images,(train_images.shape[0],train_images.shape[1],train_images.shape[2],1))\n\n\ntrain_images.shape\n\n\nTensorShape([60000, 28, 28, 1])\n\n\ntest_images &#x3D; tf.reshape(test_images,(test_images.shape[0],test_images.shape[1],test_images.shape[2],1))\n\n\ntest_images.shape\n\n\nTensorShape([10000, 28, 28, 1])\n\n\n\n模型构建net &#x3D; tf.keras.models.Sequential([\n    # 卷积层：6个5*5的卷积 sigmoid\n    tf.keras.layers.Conv2D(filters&#x3D;6, kernel_size&#x3D;5,\n                           activation&#x3D;&quot;sigmoid&quot;, input_shape&#x3D;(28, 28, 1)),\n    # max pooling\n    tf.keras.layers.MaxPool2D(pool_size&#x3D;2, strides&#x3D;2),\n    # 卷积层：16 5*5 sigmoid\n    tf.keras.layers.Conv2D(filters&#x3D;16, kernel_size&#x3D;5, activation&#x3D;&quot;sigmoid&quot;),\n    # max pooling\n    tf.keras.layers.MaxPool2D(pool_size&#x3D;2, strides&#x3D;2),\n    # 维度调整\n    tf.keras.layers.Flatten(),\n    # 全连接层，sigmoid\n    tf.keras.layers.Dense(120, activation&#x3D;&quot;sigmoid&quot;),\n    # 全连接层，sigmoid\n    tf.keras.layers.Dense(84, activation&#x3D;&quot;sigmoid&quot;),\n    # 输出层 softmax\n    tf.keras.layers.Dense(10, activation&#x3D;&quot;softmax&quot;)\n])\n\n\nnet.summary()\n\nModel: &quot;sequential&quot;\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\nconv2d (Conv2D)              (None, 24, 24, 6)         156       \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 12, 12, 6)         0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 8, 8, 16)          2416      \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 4, 4, 16)          0         \n_________________________________________________________________\nflatten (Flatten)            (None, 256)               0         \n_________________________________________________________________\ndense (Dense)                (None, 120)               30840     \n_________________________________________________________________\ndense_1 (Dense)              (None, 84)                10164     \n_________________________________________________________________\ndense_2 (Dense)              (None, 10)                850       \n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\nTotal params: 44,426\nTrainable params: 44,426\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\ntf.keras.utils.plot_model(net)\n\n\n\n\n模型编译# 设置优化器，损失函数 评价指标\nnet.compile(optimizer&#x3D;tf.keras.optimizers.SGD(learning_rate&#x3D;0.9),\n           loss &#x3D; tf.keras.losses.sparse_categorical_crossentropy,\n           metrics&#x3D;[&quot;accuracy&quot;])\n\n模型训练net.fit(train_images,train_labels,epochs&#x3D;5,batch_size&#x3D;128,verbose&#x3D;1)\n\nEpoch 1&#x2F;5\n469&#x2F;469 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 8s 17ms&#x2F;step - loss: 0.0752 - accuracy: 0.9762\nEpoch 2&#x2F;5\n469&#x2F;469 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 8s 18ms&#x2F;step - loss: 0.0664 - accuracy: 0.9793\nEpoch 3&#x2F;5\n469&#x2F;469 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 8s 18ms&#x2F;step - loss: 0.0651 - accuracy: 0.9790\nEpoch 4&#x2F;5\n469&#x2F;469 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 9s 18ms&#x2F;step - loss: 0.0580 - accuracy: 0.9817\nEpoch 5&#x2F;5\n469&#x2F;469 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 9s 18ms&#x2F;step - loss: 0.0534 - accuracy: 0.9834\n\n&lt;tensorflow.python.keras.callbacks.History at 0x111d30940&gt;\n\n\n\n模型评估net.evaluate(test_images,test_labels,verbose&#x3D;1)\n\n313&#x2F;313 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 1s 2ms&#x2F;step - loss: 0.0579 - accuracy: 0.9806\n\n[0.05785652995109558, 0.9805999994277954]\n\n\n\ncifar数据集# CIFAR-10数据集5万张训练图像、1万张测试图像、10个类别、每个类别有6k个图像，图像大小32×32×3。\n# CIFAR-100数据集也是有5万张训练图像、1万张测试图像、包含100个类别、图像大小32×32×3。\n# 随后了解下 ImageNet，目前主流的数据集\nfrom tensorflow.keras.datasets import cifar10\n\n\n(train_images,train_labels),(test_images,test_labels) &#x3D; cifar10.load_data()\n\n\ntest_images.shape\n\n\n(10000, 32, 32, 3)\n\n\ntrain_images.shape\n\n\n(50000, 32, 32, 3)\n\n\nimport matplotlib.pyplot as plt\n\n\nplt.figure(figsize&#x3D;(3,3))\nplt.imshow(train_images[4])\n\n\n&lt;matplotlib.image.AxesImage at 0x14f8c5ef0&gt;\n\n\nAlexNetimport tensorflow as tf\n\n\n\n\n模型构建\n该网络的特点是：\n\nAlexNet包含8层变换，有5层卷积和2层全连接隐藏层，以及1个全连接输出层\nAlexNet第一层中的卷积核形状是11×1111×11。第二层中的卷积核形状减小到5×55×5，之后全采用3×33×3。所有的池化层窗口大小为3×33×3、步幅为2的最大池化。\nAlexNet将sigmoid激活函数改成了ReLU激活函数，使计算更简单，网络更容易训练\nAlexNet通过dropOut来控制全连接层的模型复杂度。\nAlexNet引入了大量的图像增强，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。\n\nnet &#x3D; tf.keras.models.Sequential([\n    # 卷积层：96 11*11 4 relu\n    tf.keras.layers.Conv2D(filters&#x3D;96, kernel_size&#x3D;11,\n                           strides&#x3D;4, activation&#x3D;&quot;relu&quot;),\n    # 池化：3*3 2\n    tf.keras.layers.MaxPool2D(pool_size&#x3D;3, strides&#x3D;2),\n    # 卷积：256 5*5 1 RELU same\n    tf.keras.layers.Conv2D(filters&#x3D;256, kernel_size&#x3D;5,\n                           padding&#x3D;&quot;same&quot;, activation&#x3D;&quot;relu&quot;),\n    # 池化： 3*3 2\n    tf.keras.layers.MaxPool2D(pool_size&#x3D;3, strides&#x3D;2),\n    # 卷积：384 3*3 1 RELU same\n    tf.keras.layers.Conv2D(filters&#x3D;384, kernel_size&#x3D;3, padding&#x3D;&quot;same&quot;, activation&#x3D;&quot;relu&quot;),\n    # 卷积：384 3*3 1 RELU same\n    tf.keras.layers.Conv2D(filters&#x3D;384, kernel_size&#x3D;3, padding&#x3D;&quot;same&quot;, activation&#x3D;&quot;relu&quot;),\n    # 卷积：256 3*3 1 RELU same\n    tf.keras.layers.Conv2D(filters&#x3D;256, kernel_size&#x3D;3, padding&#x3D;&quot;same&quot;, activation&#x3D;&quot;relu&quot;),\n    # 池化：3*3 2\n    tf.keras.layers.MaxPool2D(pool_size&#x3D;3, strides&#x3D;2),\n    # 展开\n    tf.keras.layers.Flatten(),\n    # 全连接层：4096 relu\n    tf.keras.layers.Dense(4096, activation&#x3D;&quot;relu&quot;),\n    # 随机失活\n    tf.keras.layers.Dropout(0.5),\n    # 全连接层：4096 relu\n    tf.keras.layers.Dense(4096, activation&#x3D;&quot;relu&quot;),\n    # 随机失活\n    tf.keras.layers.Dropout(0.5),\n    # 输出层：\n    tf.keras.layers.Dense(10, activation&#x3D;&quot;softmax&quot;)\n\n])\n\n\nX &#x3D; tf.random.uniform((1,227,227,1))\ny &#x3D; net(X)\nnet.summary()\n\nModel: &quot;sequential&quot;\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\nconv2d (Conv2D)              (1, 55, 55, 96)           11712     \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (1, 27, 27, 96)           0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (1, 27, 27, 256)          614656    \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (1, 13, 13, 256)          0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (1, 13, 13, 384)          885120    \n_________________________________________________________________\nconv2d_3 (Conv2D)            (1, 13, 13, 384)          1327488   \n_________________________________________________________________\nconv2d_4 (Conv2D)            (1, 13, 13, 256)          884992    \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (1, 6, 6, 256)            0         \n_________________________________________________________________\nflatten (Flatten)            (1, 9216)                 0         \n_________________________________________________________________\ndense (Dense)                (1, 4096)                 37752832  \n_________________________________________________________________\ndropout (Dropout)            (1, 4096)                 0         \n_________________________________________________________________\ndense_1 (Dense)              (1, 4096)                 16781312  \n_________________________________________________________________\ndropout_1 (Dropout)          (1, 4096)                 0         \n_________________________________________________________________\ndense_2 (Dense)              (1, 10)                   40970     \n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\nTotal params: 58,299,082\nTrainable params: 58,299,082\nNon-trainable params: 0\n_________________________________________________________________\n\n\n数据读取from tensorflow.keras.datasets import mnist\nimport numpy as np\n\n\n(train_images,train_label),(test_images,test_labels)&#x3D;mnist.load_data()\n\n\n# 维度调整\ntrain_images &#x3D; np.reshape(train_images,(train_images.shape[0],train_images.shape[1],train_images.shape[2],1))\ntest_images &#x3D; np.reshape(test_images,(test_images.shape[0],test_images.shape[1],test_images.shape[2],1))\n\n\n# 对训练数据进行抽样\ndef get_train(size):\n    # 随机生成index\n    index &#x3D; np.random.randint(0,train_images.shape[0],size)\n    # 选择图像并进行resize\n    resized_image &#x3D; tf.image.resize_with_pad(train_images[index],227,227)\n    return resized_image.numpy(),train_label[index]\n\n\n# 对测试数据进行抽样\ndef get_test(size):\n    # 随机生成index\n    index &#x3D; np.random.randint(0,test_images.shape[0],size)\n    # 选择图像并进行resize\n    resized_image &#x3D; tf.image.resize_with_pad(test_images[index],227,227)\n    return resized_image.numpy(),test_labels[index]\n\n\n# 抽样结果\ntrain_images,train_label &#x3D; get_train(256)\ntest_images,test_labels &#x3D; get_test(128)\n\n\nimport matplotlib.pyplot as plt\n\n\nplt.imshow(train_images[4].astype(np.int8).squeeze(),cmap&#x3D;&#39;gray&#39;)\n\n\n\n\n&lt;matplotlib.image.AxesImage at 0x146de3f60&gt;\n\n\n\n\n\n模型编译# 优化器,损失函数,评价指标\nnet.compile(optimizer&#x3D;tf.keras.optimizers.SGD(learning_rate&#x3D;0.01),loss&#x3D;tf.keras.losses.sparse_categorical_crossentropy\n           ,metrics&#x3D;[&#39;accuracy&#39;])\n\n模型训练net.fit(train_images,train_label,batch_size&#x3D;128,epochs&#x3D;3,validation_split&#x3D;0.1,verbose&#x3D;1)\n\nEpoch 1&#x2F;3\n2&#x2F;2 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 3s 2s&#x2F;step - loss: 197.2379 - accuracy: 0.1000 - val_loss: 15641.0518 - val_accuracy: 0.1538\nEpoch 2&#x2F;3\n2&#x2F;2 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 3s 2s&#x2F;step - loss: 630482274353152.0000 - accuracy: 0.1478 - val_loss: nan - val_accuracy: 0.0385\nEpoch 3&#x2F;3\n2&#x2F;2 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 3s 2s&#x2F;step - loss: nan - accuracy: 0.1435 - val_loss: nan - val_accuracy: 0.0385\n\n&lt;tensorflow.python.keras.callbacks.History at 0x1498ac278&gt;\n\n\n\n模型评估net.evaluate(test_images,test_labels,verbose&#x3D;1)\n\n4&#x2F;4 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 1s 168ms&#x2F;step - loss: nan - accuracy: 0.0938\n\n[nan, 0.09375]\n\n\nVGGimport tensorflow as tf \n\n模型构建VGG块的构建VGG可以看成是加深版的AlexNet，整个网络由卷积层和全连接层叠加而成，和AlexNet不同的是，VGG中使用的都是小尺寸的卷积核(3×3)，其网络架构如下图所示：\n\nVGGNet使用的全部都是3x3的小卷积核和2x2的池化核，通过不断加深网络来提升性能。VGG可以通过重复使用简单的基础块来构建深度模型。\n\n在tf.keras中实现VGG模型，首先来实现VGG块，它的组成规律是：连续使用多个相同的填充为1、卷积核大小为3×33×3的卷积层后接上一个步幅为2、窗口形状为2×22×2的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。我们使用vgg_block函数来实现这个基础的VGG块，它可以指定卷积层的数量num_convs和每层的卷积核个数num_filters：\ndef vgg_block(num_conv,num_filters):\n    # 序列模型\n    blk &#x3D; tf.keras.models.Sequential()\n    # 遍历卷积层\n    for _ in range(num_conv):\n        # 设置卷积层\n        blk.add(tf.keras.layers.Conv2D(num_filters,kernel_size&#x3D;3,padding&#x3D;&#39;same&#39;,activation&#x3D;&quot;relu&quot;))\n    # 池化层\n    blk.add(tf.keras.layers.MaxPool2D(pool_size&#x3D;2,strides&#x3D;2))\n    return blk\n\n构建模型def vgg(conv_arch):\n    # 序列模型\n    net &#x3D; tf.keras.models.Sequential()\n    # 生成卷积部分\n    for (num_convs,num_filters) in conv_arch:\n        net.add(vgg_block(num_convs,num_filters))\n    # 全连接层\n    net.add(tf.keras.models.Sequential([\n        # 展评\n        tf.keras.layers.Flatten(),\n        # 全连接层\n        tf.keras.layers.Dense(4096,activation&#x3D;&quot;relu&quot;),\n        # 随机失活\n        tf.keras.layers.Dropout(0.5),\n        # 全连接层\n        tf.keras.layers.Dense(4096,activation&#x3D;&quot;relu&quot;),\n        # 随机失活\n        tf.keras.layers.Dropout(0.5),\n        # 输出层\n        tf.keras.layers.Dense(10,activation&#x3D;&quot;softmax&quot;)\n    ]))\n    return net\n\n\n# 卷积块的参数\nconv_arch &#x3D; ((2,64),(2,128),(3,256),(3,512),(3,512))\n\n\nnet &#x3D; vgg(conv_arch)\n\n\nX &#x3D; tf.random.uniform((1,224,224,1))\ny &#x3D; net(X)\nnet.summary()\n\nModel: &quot;sequential&quot;\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\nsequential_1 (Sequential)    (1, 112, 112, 64)         37568     \n_________________________________________________________________\nsequential_2 (Sequential)    (1, 56, 56, 128)          221440    \n_________________________________________________________________\nsequential_3 (Sequential)    (1, 28, 28, 256)          1475328   \n_________________________________________________________________\nsequential_4 (Sequential)    (1, 14, 14, 512)          5899776   \n_________________________________________________________________\nsequential_5 (Sequential)    (1, 7, 7, 512)            7079424   \n_________________________________________________________________\nsequential_6 (Sequential)    (1, 10)                   119586826 \n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\nTotal params: 134,300,362\nTrainable params: 134,300,362\nNon-trainable params: 0\n_________________________________________________________________\n\n\n数据读取import numpy as np\nfrom tensorflow.keras.datasets import mnist\n# 获取手写数字数据集\n(train_images, train_labels), (test_images, test_labels) &#x3D; mnist.load_data()\n# 训练集数据维度的调整：N H W C\ntrain_images &#x3D; np.reshape(train_images,(train_images.shape[0],train_images.shape[1],train_images.shape[2],1))\n# 测试集数据维度的调整：N H W C\ntest_images &#x3D; np.reshape(test_images,(test_images.shape[0],test_images.shape[1],test_images.shape[2],1))\n\n\n# 定义两个方法随机抽取部分样本演示\n# 获取训练集数据\ndef get_train(size):\n    # 随机生成要抽样的样本的索引\n    index &#x3D; np.random.randint(0, np.shape(train_images)[0], size)\n    # 将这些数据resize成22*227大小\n    resized_images &#x3D; tf.image.resize_with_pad(train_images[index],224,224,)\n    # 返回抽取的\n    return resized_images.numpy(), train_labels[index]\n# 获取测试集数据 \ndef get_test(size):\n    # 随机生成要抽样的样本的索引\n    index &#x3D; np.random.randint(0, np.shape(test_images)[0], size)\n    # 将这些数据resize成224*224大小\n    resized_images &#x3D; tf.image.resize_with_pad(test_images[index],224,224,)\n    # 返回抽样的测试样本\n    return resized_images.numpy(), test_labels[index]\n\n\n# 获取训练样本和测试样本\ntrain_images,train_labels &#x3D; get_train(256)\ntest_images,test_labels &#x3D; get_test(128)\n\n模型编译# 指定优化器，损失函数和评价指标\noptimizer &#x3D; tf.keras.optimizers.SGD(learning_rate&#x3D;0.01, momentum&#x3D;0.0)\n\nnet.compile(optimizer&#x3D;optimizer,\n              loss&#x3D;&#39;sparse_categorical_crossentropy&#39;,\n              metrics&#x3D;[&#39;accuracy&#39;])\n\n模型训练# 模型训练：指定训练数据，batchsize,epoch,验证集\nnet.fit(train_images,train_labels,batch_size&#x3D;128,epochs&#x3D;3,verbose&#x3D;1,validation_split&#x3D;0.1)\n\nEpoch 1&#x2F;3\n2&#x2F;2 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 65s 32s&#x2F;step - loss: 2.3255 - accuracy: 0.0783 - val_loss: 2.1876 - val_accuracy: 0.3462\nEpoch 2&#x2F;3\n2&#x2F;2 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 52s 26s&#x2F;step - loss: 2.2593 - accuracy: 0.1435 - val_loss: 2.1084 - val_accuracy: 0.4231\nEpoch 3&#x2F;3\n2&#x2F;2 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 51s 26s&#x2F;step - loss: 2.1924 - accuracy: 0.2348 - val_loss: 2.0070 - val_accuracy: 0.2308\n\n&lt;tensorflow.python.keras.callbacks.History at 0x12d95bd68&gt;\n\n\n\n模型评估# 指定测试数据\nnet.evaluate(test_images,test_labels,verbose&#x3D;1)\n\n4&#x2F;4 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 11s 3s&#x2F;step - loss: 2.1049 - accuracy: 0.1484\n\n[2.1049132347106934, 0.1484375]\n\nGoogleNetimport tensorflow as tf\n\nGoogLeNet的名字不是GoogleNet，而是GoogLeNet，这是为了致敬LeNet。GoogLeNet和AlexNet&#x2F;VGGNet这类依靠加深网络结构的深度的思想不完全一样。GoogLeNet在加深度的同时做了结构上的创新，引入了一个叫做Inception的结构来代替之前的卷积加激活的经典组件。GoogLeNet在ImageNet分类比赛上的Top-5错误率降低到了6.7%。\nInception模块GoogLeNet中的基础卷积块叫作Inception块，得名于同名电影《盗梦空间》（Inception）。Inception块在结构比较复杂，如下图所示：\n\nInception块里有4条并行的线路。前3条线路使用窗口大小分别是1×11×1、3×33×3和5×55×5的卷积层来抽取不同空间尺寸下的信息，其中中间2个线路会对输入先做1×11×1卷积来减少输入通道数，以降低模型复杂度。第4条线路则使用3×33×3最大池化层，后接1×11×1卷积层来改变通道数。4条线路都使用了合适的填充来使输入与输出的高和宽一致。最后我们将每条线路的输出在通道维上连结,并向后进行传输。\n1×11×1卷积：\n它的计算方法和其他卷积核一样，唯一不同的是它的大小是1×11×1，没有考虑在特征图局部信息之间的关系。\n\n它的作用主要是：\n\n实现跨通道的交互和信息整合\n卷积核通道数的降维和升维，减少网络参数\n\n在tf.keras中实现Inception模块，各个卷积层卷积核的个数通过输入参数来控制，如下所示：\nclass Inception(tf.keras.layers.Layer):\n    # 设置模块的构成\n    def __init__(self,c1,c2,c3,c4):\n        super().__init__()\n        # 线路1:1*1 RELU same c1\n        self.p1_1 &#x3D; tf.keras.layers.Conv2D(c1,kernel_size&#x3D;1,activation&#x3D;&quot;relu&quot;,padding &#x3D;&quot;same&quot;)\n        # 线路2:1*1 RELU same c2[0]\n        self.p2_1 &#x3D; tf.keras.layers.Conv2D(c2[0],kernel_size&#x3D;1,activation&#x3D;&quot;relu&quot;,padding&#x3D;&quot;same&quot;)\n        # 线路2:3*3 RELU same c2[1]\n        self.p2_2 &#x3D; tf.keras.layers.Conv2D(c2[1],kernel_size&#x3D;3,activation&#x3D;&quot;relu&quot;,padding&#x3D;&#39;same&#39;)\n        # 线路3:1*1 RELU same c3[0]\n        self.p3_1 &#x3D; tf.keras.layers.Conv2D(c3[0],kernel_size&#x3D;1,activation&#x3D;&quot;relu&quot;,padding&#x3D;&quot;same&quot;)\n        # 线路3:5*5 RELU same c3[1]\n        self.p3_2 &#x3D; tf.keras.layers.Conv2D(c3[1],kernel_size&#x3D;5,activation&#x3D;&quot;relu&quot;,padding&#x3D;&#39;same&#39;)\n        # 线路4: max-pool \n        self.p4_1 &#x3D; tf.keras.layers.MaxPool2D(pool_size&#x3D;3,padding&#x3D;&quot;same&quot;,strides&#x3D;1)\n        # 线路4:1*1\n        self.p4_2 &#x3D; tf.keras.layers.Conv2D(c4,kernel_size&#x3D;1,activation&#x3D;&quot;relu&quot;,padding&#x3D;&quot;same&quot;)\n    # 前行传播过程\n    def call(self,x):\n        # 线路1\n        p1 &#x3D; self.p1_1(x)\n        # 线路2\n        p2 &#x3D; self.p2_2(self.p2_1(x))\n        # 线路3\n        p3 &#x3D; self.p3_2(self.p3_1(x))\n        # 线路4\n        p4 &#x3D; self.p4_2(self.p4_1(x))\n        # concat\n        outputs &#x3D; tf.concat([p1,p2,p3,p4],axis&#x3D;-1)\n        return outputs\n    \n\n\nInception(64,(96,128),(16,32),32)\n\n\n&lt;__main__.Inception at 0x1048a6828&gt;\n\n\n\nGoogLeNet构建\n整个网络架构我们分为五个模块，每个模块之间使用步幅为2的3×33×3最大池化层来减小输出高宽。\n\nB1模块inputs &#x3D; tf.keras.Input(shape&#x3D;(224,224,1),name&#x3D;&quot;input&quot;)\n# 卷积:7*7 64 \nx &#x3D; tf.keras.layers.Conv2D(64,kernel_size&#x3D;7,strides &#x3D; 2,padding&#x3D;&quot;same&quot;,activation&#x3D;&quot;relu&quot;)(inputs)\n# 池化层\nx &#x3D; tf.keras.layers.MaxPool2D(pool_size&#x3D;3,strides&#x3D;2,padding&#x3D;&quot;same&quot;)(x)\n\nB2模块# 卷积层:1*1\nx &#x3D; tf.keras.layers.Conv2D(64,kernel_size &#x3D; 1,padding&#x3D;&#39;same&#39;,activation&#x3D;&quot;relu&quot;)(x)\n# 卷积:3*3\nx &#x3D; tf.keras.layers.Conv2D(192,kernel_size&#x3D;3,padding&#x3D;&#39;same&#39;,activation&#x3D;&#39;relu&#39;)(x)\n# 池化层\nx &#x3D; tf.keras.layers.MaxPool2D(pool_size&#x3D;3,strides&#x3D;2,padding&#x3D;&quot;same&quot;)(x)\n\nB3模块# inception\nx &#x3D; Inception(64,(96,128),(16,32),32)(x)\n# inception\nx &#x3D; Inception(128,(128,192),(32,96),64)(x)\n# 池化\nx &#x3D; tf.keras.layers.MaxPool2D(pool_size&#x3D;3,strides&#x3D;2,padding&#x3D;&quot;same&quot;)(x)\n\nB4模块第四模块更加复杂。它串联了5个Inception块，其输出通道数分别是192+208+48+64&#x3D;512192+208+48+64&#x3D;512、160+224+64+64&#x3D;512160+224+64+64&#x3D;512、128+256+64+64&#x3D;512128+256+64+64&#x3D;512、112+288+64+64&#x3D;528112+288+64+64&#x3D;528和256+320+128+128&#x3D;832256+320+128+128&#x3D;832。并且增加了辅助分类器，根据实验发现网络的中间层具有很强的识别能力，为了利用中间层抽象的特征，在某些中间层中添加含有多层的分类器，如下图所示：\n\n# 辅助分类器\ndef aux_classifier(x,filter_size):\n    # 池化层\n    x &#x3D; tf.keras.layers.AveragePooling2D(pool_size&#x3D;5,strides &#x3D; 3,padding&#x3D;&#39;same&#39;)(x)\n    # 卷积层\n    x &#x3D; tf.keras.layers.Conv2D(filters &#x3D; filter_size[0],kernel_size&#x3D;1,strides&#x3D;1,padding &#x3D;&quot;valid&quot;,activation&#x3D;&quot;relu&quot;)(x)\n    # 展评\n    x &#x3D; tf.keras.layers.Flatten()(x)\n    # 全连接\n    x &#x3D; tf.keras.layers.Dense(units &#x3D; filter_size[1],activation&#x3D;&quot;relu&quot;)(x)\n    # 输出层:\n    x &#x3D; tf.keras.layers.Dense(units&#x3D;10,activation&#x3D;&quot;softmax&quot;)(x)\n    return x\n\n\n# Inception\nx &#x3D; Inception(192,(96,208),(16,48),64)(x)\n# 辅助输出\naux_output1 &#x3D; aux_classifier(x,[128,1024])\n# Inception\nx &#x3D; Inception(160,(112,224),(24,64),64)(x)\n# Inception\nx &#x3D; Inception(128,(128,256),(24,64),64)(x)\n# Inception\nx &#x3D; Inception(112,(144,288),(32,64),64)(x)\n# 辅助输出2\naux_output2 &#x3D; aux_classifier(x,[128,1024])\n# Inception\nx &#x3D;Inception(256,(160,320),(32,128),128)(x)\n# 最大池化\nx &#x3D; tf.keras.layers.MaxPool2D(pool_size&#x3D;3,strides&#x3D;2,padding&#x3D;&#39;same&#39;)(x)\n\nb5模块第五模块有输出通道数为256+320+128+128&#x3D;832256+320+128+128&#x3D;832和384+384+128+128&#x3D;1024384+384+128+128&#x3D;1024的两个Inception块。后面紧跟输出层，该模块使用全局平均池化层（GAP）来将每个通道的高和宽变成1。最后输出变成二维数组后接输出个数为标签类别数的全连接层。\n全局平均池化层（GAP）\n用来替代全连接层，将特征图每一通道中所有像素值相加后求平均，得到就是GAP的结果，在将其送入后续网络中进行计算\n\n# inception\nx &#x3D; Inception(256,(160,320),(32,128),128)(x)\nx &#x3D; Inception(384,(192,384),(48,128),128)(x)\n# GAP\nx &#x3D; tf.keras.layers.GlobalAvgPool2D()(x)\n# 输出层\noutput &#x3D; tf.keras.layers.Dense(10,activation&#x3D;&quot;softmax&quot;)(x)\n\n\n# 模型\nmodel &#x3D; tf.keras.Model(inputs&#x3D;inputs,outputs&#x3D;[output,aux_output1,aux_output2])\n\n\nmodel.summary()\n\nModel: &quot;functional_1&quot;\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\ninput (InputLayer)              [(None, 224, 224, 1) 0                                            \n__________________________________________________________________________________________________\nconv2d_6 (Conv2D)               (None, 112, 112, 64) 3200        input[0][0]                      \n__________________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)  (None, 56, 56, 64)   0           conv2d_6[0][0]                   \n__________________________________________________________________________________________________\nconv2d_7 (Conv2D)               (None, 56, 56, 64)   4160        max_pooling2d_1[0][0]            \n__________________________________________________________________________________________________\nconv2d_8 (Conv2D)               (None, 56, 56, 192)  110784      conv2d_7[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_2 (MaxPooling2D)  (None, 28, 28, 192)  0           conv2d_8[0][0]                   \n__________________________________________________________________________________________________\ninception_1 (Inception)         (None, 28, 28, 256)  163696      max_pooling2d_2[0][0]            \n__________________________________________________________________________________________________\ninception_2 (Inception)         (None, 28, 28, 480)  388736      inception_1[0][0]                \n__________________________________________________________________________________________________\nmax_pooling2d_5 (MaxPooling2D)  (None, 14, 14, 480)  0           inception_2[0][0]                \n__________________________________________________________________________________________________\ninception_3 (Inception)         (None, 14, 14, 512)  376176      max_pooling2d_5[0][0]            \n__________________________________________________________________________________________________\ninception_4 (Inception)         (None, 14, 14, 512)  449160      inception_3[0][0]                \n__________________________________________________________________________________________________\ninception_5 (Inception)         (None, 14, 14, 512)  510104      inception_4[0][0]                \n__________________________________________________________________________________________________\ninception_6 (Inception)         (None, 14, 14, 528)  605376      inception_5[0][0]                \n__________________________________________________________________________________________________\ninception_7 (Inception)         (None, 14, 14, 832)  868352      inception_6[0][0]                \n__________________________________________________________________________________________________\nmax_pooling2d_11 (MaxPooling2D) (None, 7, 7, 832)    0           inception_7[0][0]                \n__________________________________________________________________________________________________\naverage_pooling2d (AveragePooli (None, 5, 5, 512)    0           inception_3[0][0]                \n__________________________________________________________________________________________________\naverage_pooling2d_1 (AveragePoo (None, 5, 5, 528)    0           inception_6[0][0]                \n__________________________________________________________________________________________________\ninception_8 (Inception)         (None, 7, 7, 832)    1043456     max_pooling2d_11[0][0]           \n__________________________________________________________________________________________________\nconv2d_27 (Conv2D)              (None, 5, 5, 128)    65664       average_pooling2d[0][0]          \n__________________________________________________________________________________________________\nconv2d_46 (Conv2D)              (None, 5, 5, 128)    67712       average_pooling2d_1[0][0]        \n__________________________________________________________________________________________________\ninception_9 (Inception)         (None, 7, 7, 1024)   1444080     inception_8[0][0]                \n__________________________________________________________________________________________________\nflatten (Flatten)               (None, 3200)         0           conv2d_27[0][0]                  \n__________________________________________________________________________________________________\nflatten_1 (Flatten)             (None, 3200)         0           conv2d_46[0][0]                  \n__________________________________________________________________________________________________\nglobal_average_pooling2d (Globa (None, 1024)         0           inception_9[0][0]                \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 1024)         3277824     flatten[0][0]                    \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 1024)         3277824     flatten_1[0][0]                  \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 10)           10250       global_average_pooling2d[0][0]   \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 10)           10250       dense[0][0]                      \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 10)           10250       dense_2[0][0]                    \n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\nTotal params: 12,687,054\nTrainable params: 12,687,054\nNon-trainable params: 0\n__________________________________________________________________________________________________\n\n\n数据读取import numpy as np\nfrom tensorflow.keras.datasets import mnist\n# 获取手写数字数据集\n(train_images, train_labels), (test_images, test_labels) &#x3D; mnist.load_data()\n# 训练集数据维度的调整：N H W C\ntrain_images &#x3D; np.reshape(train_images,(train_images.shape[0],train_images.shape[1],train_images.shape[2],1))\n# 测试集数据维度的调整：N H W C\ntest_images &#x3D; np.reshape(test_images,(test_images.shape[0],test_images.shape[1],test_images.shape[2],1))\n\n\n# 定义两个方法随机抽取部分样本演示\n# 获取训练集数据\ndef get_train(size):\n    # 随机生成要抽样的样本的索引\n    index &#x3D; np.random.randint(0, np.shape(train_images)[0], size)\n    # 将这些数据resize成22*227大小\n    resized_images &#x3D; tf.image.resize_with_pad(train_images[index],224,224,)\n    # 返回抽取的\n    return resized_images.numpy(), train_labels[index]\n# 获取测试集数据 \ndef get_test(size):\n    # 随机生成要抽样的样本的索引\n    index &#x3D; np.random.randint(0, np.shape(test_images)[0], size)\n    # 将这些数据resize成224*224大小\n    resized_images &#x3D; tf.image.resize_with_pad(test_images[index],224,224,)\n    # 返回抽样的测试样本\n    return resized_images.numpy(), test_labels[index]\n\n\n# 获取训练样本和测试样本\ntrain_images,train_labels &#x3D; get_train(256)\ntest_images,test_labels &#x3D; get_test(128)\n\n模型编译# 指定优化器，损失函数和评价指标\noptimizer &#x3D; tf.keras.optimizers.SGD(learning_rate&#x3D;0.01, momentum&#x3D;0.0)\n# 模型有3个输出，所以指定损失函数对应的权重系数\nmodel.compile(optimizer&#x3D;optimizer,\n              loss&#x3D;&#39;sparse_categorical_crossentropy&#39;,\n              metrics&#x3D;[&#39;accuracy&#39;],loss_weights&#x3D;[1,0.3,0.3])\n\n模型训练# 模型训练：指定训练数据，batchsize,epoch,验证集\nmodel.fit(train_images,train_labels,batch_size&#x3D;128,epochs&#x3D;3,verbose&#x3D;1,validation_split&#x3D;0.1)\n\nEpoch 1&#x2F;3\n2&#x2F;2 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 7s 4s&#x2F;step - loss: 5.0465 - dense_4_loss: 2.5478 - dense_1_loss: 5.5701 - dense_3_loss: 2.7589 - dense_4_accuracy: 0.1174 - dense_1_accuracy: 0.0696 - dense_3_accuracy: 0.0739 - val_loss: 3.8758 - val_dense_4_loss: 2.2874 - val_dense_1_loss: 2.9486 - val_dense_3_loss: 2.3461 - val_dense_4_accuracy: 0.1538 - val_dense_1_accuracy: 0.1154 - val_dense_3_accuracy: 0.0769\nEpoch 2&#x2F;3\n2&#x2F;2 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 7s 3s&#x2F;step - loss: 3.8362 - dense_4_loss: 2.3122 - dense_1_loss: 2.7749 - dense_3_loss: 2.3050 - dense_4_accuracy: 0.1087 - dense_1_accuracy: 0.1043 - dense_3_accuracy: 0.1000 - val_loss: 3.7066 - val_dense_4_loss: 2.3002 - val_dense_1_loss: 2.3821 - val_dense_3_loss: 2.3059 - val_dense_4_accuracy: 0.1154 - val_dense_1_accuracy: 0.0769 - val_dense_3_accuracy: 0.0385\nEpoch 3&#x2F;3\n2&#x2F;2 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 8s 4s&#x2F;step - loss: 3.6779 - dense_4_loss: 2.3000 - dense_1_loss: 2.3131 - dense_3_loss: 2.2799 - dense_4_accuracy: 0.1043 - dense_1_accuracy: 0.1522 - dense_3_accuracy: 0.1522 - val_loss: 3.6978 - val_dense_4_loss: 2.3022 - val_dense_1_loss: 2.3475 - val_dense_3_loss: 2.3045 - val_dense_4_accuracy: 0.1154 - val_dense_1_accuracy: 0.0769 - val_dense_3_accuracy: 0.0385\n\n&lt;tensorflow.python.keras.callbacks.History at 0x141bf7f60&gt;\n\n模型评估# 指定测试数据\nmodel.evaluate(test_images,test_labels,verbose&#x3D;1)\n\n4&#x2F;4 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 1s 325ms&#x2F;step - loss: 3.6632 - dense_4_loss: 2.2931 - dense_1_loss: 2.2922 - dense_3_loss: 2.2749 - dense_4_accuracy: 0.1484 - dense_1_accuracy: 0.1719 - dense_3_accuracy: 0.1328\n\n[3.663177967071533,\n 2.2930545806884766,\n 2.2921969890594482,\n 2.274880886077881,\n 0.1484375,\n 0.171875,\n 0.1328125]\n\n\n\nResNetimport tensorflow as tf\nfrom tensorflow.keras import layers,activations\n\n\n\n网络越深，获取的信息就越多，特征也越丰富。但是在实践中，随着网络的加深，优化效果反而越差，测试数据和训练数据的准确率反而降低了。\n针对这一问题，何恺明等人提出了残差网络（ResNet）在2015年的ImageNet图像识别挑战赛夺魁，并深刻影响了后来的深度神经网络的设计。\n残差块假设 F(x) 代表某个只包含有两层的映射函数， x 是输入， F(x)是输出。假设他们具有相同的维度。在训练的过程中我们希望能够通过修改网络中的 w和b去拟合一个理想的 H(x)(从输入到输出的一个理想的映射函数)。也就是我们的目标是修改F(x) 中的 w和b逼近 H(x) 。如果我们改变思路，用F(x) 来逼近 H(x)-x ，那么我们最终得到的输出就变为 F(x)+x（这里的加指的是对应位置上的元素相加，也就是element-wise addition），这里将直接从输入连接到输出的结构也称为shortcut，那整个结构就是残差块，ResNet的基础模块。\n\nResNet沿用了VGG全3×33×3卷积层的设计。残差块里首先有2个有相同输出通道数的3×33×3卷积层。每个卷积层后接BN层和ReLU激活函数，然后将输入直接加在最后的ReLU激活函数前，这种结构用于层数较少的神经网络中，比如ResNet34。若输入通道数比较多，就需要引入1×11×1卷积层来调整输入的通道数，这种结构也叫作瓶颈模块，通常用于网络层数较多的结构中。如下图所示：\n\nclass Residual(tf.keras.Model):\n    # 定义网络结构\n    def __init__(self,num_channels,use_1x1conv&#x3D;False,strides&#x3D;1):\n        super(Residual,self).__init__()\n        # 卷积层\n        self.conv1 &#x3D; layers.Conv2D(num_channels,padding&#x3D;&#39;same&#39;,kernel_size&#x3D;3,strides&#x3D;strides)\n        # 卷积层\n        self.conv2 &#x3D; layers.Conv2D(num_channels,kernel_size&#x3D;3,padding&#x3D;&#39;same&#39;)\n        # 是否使用1*1的卷积\n        if use_1x1conv:\n            self.conv3 &#x3D; layers.Conv2D(num_channels,kernel_size&#x3D;1,strides&#x3D;strides)\n        else:\n            self.conv3 &#x3D; None\n        # BN层\n        self.bn1 &#x3D; layers.BatchNormalization()\n        self.bn2 &#x3D; layers.BatchNormalization()\n    # 定义前向传播过程  \n    def call(self,x):\n        Y &#x3D; activations.relu(self.bn1(self.conv1(x)))\n        Y &#x3D; self.bn2(self.conv2(Y))\n        if self.conv3:\n            x &#x3D; self.conv3(x)\n        outputs &#x3D; activations.relu(Y+x)\n        return outputs\n\n1*1卷积用来调整通道数。\n残差模块\nResNet网络中按照残差块的通道数分为不同的模块。第一个模块前使用了步幅为2的最大池化层，所以无须减小高和宽。之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。\nclass ResnetBlock(tf.keras.layers.Layer):\n    # 定义所需的网络结构\n    def __init__(self,num_channels,num_res,first_block&#x3D;False):\n        super(ResnetBlock,self).__init__()\n        # 存储残差块\n        self.listLayers&#x3D;[]\n        # 遍历残差数目生成模块\n        for i in range(num_res):\n            # 如果是第一个残差块而且不是第一个模块时\n            if i &#x3D;&#x3D;0 and not first_block:\n                self.listLayers.append(Residual(num_channels,use_1x1conv&#x3D;True,strides&#x3D;2))\n            else:\n                self.listLayers.append(Residual(num_channels))\n    # 定义前向传播\n    def call(self,X):\n        for layer in self.listLayers.layers:\n            X &#x3D; layer(X)\n        return X\n\n构建resNet网络ResNet的前两层跟之前介绍的GoogLeNet中的一样：在输出通道数为64、步幅为2的7×77×7卷积层后接步幅为2的3×33×3的最大池化层。不同之处在于ResNet每个卷积层后增加了BN层,接着是所有残差模块，最后，与GoogLeNet一样，加入全局平均池化层（GAP）后接上全连接层输出。\nclass ResNet(tf.keras.Model):\n    # 定义网络的构成\n    def __init__(self, num_blocks):\n        super(ResNet, self).__init__()\n        # 输入层\n        self.conv &#x3D; layers.Conv2D(64, kernel_size&#x3D;7, strides&#x3D;2, padding&#x3D;&#39;same&#39;)\n        # BN 层\n        self.bn &#x3D; layers.BatchNormalization()\n        # 激活层\n        self.relu &#x3D; layers.Activation(&#39;relu&#39;)\n        # 池化\n        self.mp &#x3D; layers.MaxPool2D(pool_size&#x3D;3, strides&#x3D;2, padding&#x3D;&quot;same&quot;)\n        # 残差模块\n        self.res_block1 &#x3D; ResnetBlock(64, num_blocks[0], first_block&#x3D;True)\n        self.res_block2 &#x3D; ResnetBlock(128, num_blocks[1])\n        self.res_block3 &#x3D; ResnetBlock(256, num_blocks[2])\n        self.res_block4 &#x3D; ResnetBlock(512, num_blocks[3])\n        # GAP\n        self.gap &#x3D; layers.GlobalAvgPool2D()\n        # 全连接层\n        self.fc &#x3D; layers.Dense(\n            units&#x3D;10, activation&#x3D;tf.keras.activations.softmax)\n    # 定义前向传播过程\n\n    def call(self, x):\n        # 输入部分的传输过程\n        x &#x3D; self.conv(x)\n        x &#x3D; self.bn(x)\n        x &#x3D; self.relu(x)\n        x &#x3D; self.mp(x)\n        # block\n        x &#x3D; self.res_block1(x)\n        x &#x3D; self.res_block2(x)\n        x &#x3D; self.res_block3(x)\n        x &#x3D; self.res_block4(x)\n        # 输出部分的传输\n        x &#x3D; self.gap(x)\n        x &#x3D; self.fc(x)\n        return x\n\n这里每个模块里有4个卷积层（不计算 1×1卷积层），加上最开始的卷积层和最后的全连接层，共计18层。这个模型被称为ResNet-18。通过配置不同的通道数和模块里的残差块数可以得到不同的ResNet模型，例如更深的含152层的ResNet-152。虽然ResNet的主体架构跟GoogLeNet的类似，但ResNet结构更简单，修改也更方便。这些因素都导致了ResNet迅速被广泛使用。 在训练ResNet之前，我们来观察一下输入形状在ResNe的架构：\n# 实例化\nmynet &#x3D; ResNet([2,2,2,2])\nX &#x3D; tf.random.uniform((1,224,224,1))\ny &#x3D; mynet(X)\nmynet.summary()\n\nModel: &quot;res_net_5&quot;\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\nconv2d_62 (Conv2D)           multiple                  3200      \n_________________________________________________________________\nbatch_normalization_53 (Batc multiple                  256       \n_________________________________________________________________\nactivation_5 (Activation)    multiple                  0         \n_________________________________________________________________\nmax_pooling2d_5 (MaxPooling2 multiple                  0         \n_________________________________________________________________\nresnet_block_14 (ResnetBlock multiple                  148736    \n_________________________________________________________________\nresnet_block_15 (ResnetBlock multiple                  526976    \n_________________________________________________________________\nresnet_block_16 (ResnetBlock multiple                  2102528   \n_________________________________________________________________\nresnet_block_17 (ResnetBlock multiple                  8399360   \n_________________________________________________________________\nglobal_average_pooling2d_3 ( multiple                  0         \n_________________________________________________________________\ndense_3 (Dense)              multiple                  5130      \n&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;\nTotal params: 11,186,186\nTrainable params: 11,178,378\nNon-trainable params: 7,808\n_________________________________________________________________\n\n\n数据读取import numpy as np\nfrom tensorflow.keras.datasets import mnist\n# 获取手写数字数据集\n(train_images, train_labels), (test_images, test_labels) &#x3D; mnist.load_data()\n# 训练集数据维度的调整：N H W C\ntrain_images &#x3D; np.reshape(train_images,(train_images.shape[0],train_images.shape[1],train_images.shape[2],1))\n# 测试集数据维度的调整：N H W C\ntest_images &#x3D; np.reshape(test_images,(test_images.shape[0],test_images.shape[1],test_images.shape[2],1))\n\n\n# 定义两个方法随机抽取部分样本演示\n# 获取训练集数据\ndef get_train(size):\n    # 随机生成要抽样的样本的索引\n    index &#x3D; np.random.randint(0, np.shape(train_images)[0], size)\n    # 将这些数据resize成22*227大小\n    resized_images &#x3D; tf.image.resize_with_pad(train_images[index],224,224,)\n    # 返回抽取的\n    return resized_images.numpy(), train_labels[index]\n# 获取测试集数据 \ndef get_test(size):\n    # 随机生成要抽样的样本的索引\n    index &#x3D; np.random.randint(0, np.shape(test_images)[0], size)\n    # 将这些数据resize成224*224大小\n    resized_images &#x3D; tf.image.resize_with_pad(test_images[index],224,224,)\n    # 返回抽样的测试样本\n    return resized_images.numpy(), test_labels[index]\n\n\n# 获取训练样本和测试样本\ntrain_images,train_labels &#x3D; get_train(256)\ntest_images,test_labels &#x3D; get_test(128)\n\n模型编译# 指定优化器，损失函数和评价指标\noptimizer &#x3D; tf.keras.optimizers.SGD(learning_rate&#x3D;0.01, momentum&#x3D;0.0)\n\nmynet.compile(optimizer&#x3D;optimizer,\n              loss&#x3D;&#39;sparse_categorical_crossentropy&#39;,\n              metrics&#x3D;[&#39;accuracy&#39;])\n\n模型训练# 模型训练：指定训练数据，batchsize,epoch,验证集\nmynet.fit(train_images,train_labels,batch_size&#x3D;128,epochs&#x3D;3,verbose&#x3D;1,validation_split&#x3D;0.1)\n\nEpoch 1&#x2F;3\n2&#x2F;2 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 8s 4s&#x2F;step - loss: 2.7646 - accuracy: 0.1348 - val_loss: 4.4041 - val_accuracy: 0.0769\nEpoch 2&#x2F;3\n2&#x2F;2 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 7s 4s&#x2F;step - loss: 2.1875 - accuracy: 0.2826 - val_loss: 5.0271 - val_accuracy: 0.0769\nEpoch 3&#x2F;3\n2&#x2F;2 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 7s 4s&#x2F;step - loss: 1.9738 - accuracy: 0.3435 - val_loss: 3.6854 - val_accuracy: 0.3077\n\n&lt;tensorflow.python.keras.callbacks.History at 0x13e4ef438&gt;\n\n模型评估# 指定测试数据\nmynet.evaluate(test_images,test_labels,verbose&#x3D;1)\n\n4&#x2F;4 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 1s 342ms&#x2F;step - loss: 4.9585 - accuracy: 0.1094\n\n[4.958542823791504, 0.109375]\n\n\n\nimageAUG 图像增强大规模数据集是成功应用深度神经网络的前提。例如，我们可以对图像进行不同方式的裁剪，使感兴趣的物体出现在不同位置，从而减轻模型对物体出现位置的依赖性。我们也可以调整亮度、色彩等因素来降低模型对色彩的敏感度。可以说，在当年AlexNet的成功中，图像增强技术功不可没\n1.常用的图像增强方法图像增强（image augmentation）指通过剪切、旋转&#x2F;反射&#x2F;翻转变换、缩放变换、平移变换、尺度变换、对比度变换、噪声扰动、颜色变换等一种或多种组合数据增强变换的方式来增加数据集的大小。图像增强的意义是通过对训练图像做一系列随机改变，来产生相似但又不同的训练样本，从而扩大训练数据集的规模，而且随机改变训练样本可以降低模型对某些属性的依赖，从而提高模型的泛化能力。\n常见的图像增强方式可以分为两类：几何变换类和颜色变换类\n\n几何变换类，主要是对图像进行几何变换操作，包括翻转，旋转，裁剪，变形，缩放等。\n\n\n颜色变换类，指通过模糊、颜色变换、擦除、填充等方式对图像进行处理\n\n\n\ntf.image进行增强import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n/opt/anaconda3/envs/dlcv/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n  return f(*args, **kwds)\n/opt/anaconda3/envs/dlcv/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n  return f(*args, **kwds)\n/opt/anaconda3/envs/dlcv/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n  return f(*args, **kwds)\n\ncat &#x3D; plt.imread(&#39;.&#x2F;cat.jpg&#39;)\n\n\nplt.imshow(cat)\n\n\n\n# 左右翻转并显示\ncat1 &#x3D; tf.image.random_flip_left_right(cat)\nplt.imshow(cat1)\n\n\n# 上下翻转\ncat2 &#x3D; tf.image.random_flip_up_down(cat)\nplt.imshow(cat2)\n\n\n# 随机裁剪\ncat3 &#x3D; tf.image.random_crop(cat,(200,200,3))\nplt.imshow(cat3)\n\n\n# 颜色变换\ncat4 &#x3D; tf.image.random_brightness(cat,0.5)\nplt.imshow(cat4)\n\n\n# 随机变化图像\ncat5 &#x3D; tf.image.random_hue(cat,0.5)\nplt.imshow(cat5)\n\n\n使用imagedataGenerator进行增强ImageDataGenerator()是keras.preprocessing.image模块中的DLNotes生成器，可以在batch中对数据进行增强，扩充数据集大小，增强模型的泛化能力。比如旋转，变形等，如下所示：\nfrom tensorflow.keras.datasets import mnist\n\n\n# 获取数据集\n(x_train, y_train), (x_test, y_test) &#x3D; tf.keras.datasets.mnist.load_data()\n# 将数据转换为4维的形式\nx_train &#x3D; x_train.reshape(x_train.shape[0],28,28,1)\nx_test &#x3D; x_test.reshape(x_test.shape[0],28,28,1)\n\n\n# 实例化\ndatagen &#x3D; tf.keras.preprocessing.image.ImageDataGenerator(shear_range&#x3D;10)\n\n\nfor x,y in datagen.flow(x_train,y_train,batch_size&#x3D;9):\n    plt.figure(figsize&#x3D;(8,8))\n    for i in range(0,9):\n        plt.subplot(330+1+i)\n        plt.imshow(x[i].reshape(28,28),cmap&#x3D;&#39;gray&#39;)\n        plt.title(y[i])\n    plt.show()\n    break\n\n\n微调如何在只有6万张图像的MNIST训练数据集上训练模型。学术界当下使用最广泛的大规模图像数据集ImageNet，它有超过1,000万的图像和1,000类的物体。然而，我们平常接触到数据集的规模通常在这两者之间。假设我们想从图像中识别出不同种类的椅子，然后将购买链接推荐给用户。一种可能的方法是先找出100种常见的椅子，为每种椅子拍摄1,000张不同角度的图像，然后在收集到的图像数据集上训练一个分类模型。另外一种解决办法是应用迁移学习（transfer learning），将从源数据集学到的知识迁移到目标数据集上。例如，虽然ImageNet数据集的图像大多跟椅子无关，但在该数据集上训练的模型可以抽取较通用的图像特征，从而能够帮助识别边缘、纹理、形状和物体组成等。这些类似的特征对于识别椅子也可能同样有效。\n微调由以下4步构成。\n\n在源数据集（如ImageNet数据集）上预训练一个神经网络模型，即源模型。\n创建一个新的神经网络模型，即目标模型。它复制了源模型上除了输出层外的所有模型设计及其参数。我们假设这些模型参数包含了源数据集上学习到的知识，且这些知识同样适用于目标数据集。我们还假设源模型的输出层跟源数据集的标签紧密相关，因此在目标模型中不予采用。\n为目标模型添加一个输出大小为目标数据集类别个数的输出层，并随机初始化该层的模型参数。\n在目标数据集（如椅子数据集）上训练目标模型。我们将从头训练输出层，而其余层的参数都是基于源模型的参数微调得到的。\n\n\n当目标数据集远小于源数据集时，微调有助于提升模型的泛化能力。\n例如：\n# 热狗识别。将基于一个小数据集对在ImageNet数据集上训练好的ResNet模型进行微调。该小数据集含有数千张热狗或者其他事物的图像。我们将使用微调得到的模型来识别一张图像中是否包含热狗。\nimport tensorflow as tf\nimport numpy as np\n# 通过以下方法读取图像文件，该方法以文件夹路径为参数,生成经过图像增强后的结果，并产生batch数据：\nflow_from_directory(self, directory,\n                            target_size&#x3D;(256, 256), color_mode&#x3D;&#39;rgb&#39;,\n                            classes&#x3D;None, class_mode&#x3D;&#39;categorical&#39;,\n                            batch_size&#x3D;32, shuffle&#x3D;True, seed&#x3D;None,\n                            save_to_dir&#x3D;None）\n\n主要参数：\n\ndirectory: 目标文件夹路径，对于每一个类对应一个子文件夹，该子文件夹中任何JPG、PNG、BNP、PPM的DLNotes都可以读取。\ntarget_size: 默认为(256, 256)，图像将被resize成该尺寸。\nbatch_size: batch数据的大小，默认32。\nshuffle: 是否打乱数据，默认为True。\n\n我们创建两个tf.keras.preprocessing.image.ImageDataGenerator实例来分别读取训练数据集和测试数据集中的所有图像文件。将训练集DLNotes全部处理为高和宽均为224像素的输入。此外，我们对RGB（红、绿、蓝）三个颜色通道的数值做标准化。\n# 获取数据集\nimport pathlib\ntrain_dir &#x3D; &#39;transferdata&#x2F;train&#39;\ntest_dir &#x3D; &#39;transferdata&#x2F;test&#39;\n# 获取训练集数据\ntrain_dir &#x3D; pathlib.Path(train_dir)\ntrain_count &#x3D; len(list(train_dir.glob(&#39;*&#x2F;*.jpg&#39;)))\n# 获取测试集数据\ntest_dir &#x3D; pathlib.Path(test_dir)\ntest_count &#x3D; len(list(test_dir.glob(&#39;*&#x2F;*.jpg&#39;)))\n# 创建imageDataGenerator进行图像处理\nimage_generator &#x3D; tf.keras.preprocessing.image.ImageDataGenerator(rescale&#x3D;1.&#x2F;255)\n# 设置参数\nBATCH_SIZE &#x3D; 32\nIMG_HEIGHT &#x3D; 224\nIMG_WIDTH &#x3D; 224\n# 获取训练数据\ntrain_data_gen &#x3D; image_generator.flow_from_directory(directory&#x3D;str(train_dir),\n                                                    batch_size&#x3D;BATCH_SIZE,\n                                                    target_size&#x3D;(IMG_HEIGHT, IMG_WIDTH),\n                                                    shuffle&#x3D;True)\n# 获取测试数据\ntest_data_gen &#x3D; image_generator.flow_from_directory(directory&#x3D;str(test_dir),\n                                                    batch_size&#x3D;BATCH_SIZE,\n                                                    target_size&#x3D;(IMG_HEIGHT, IMG_WIDTH),\n                                                    shuffle&#x3D;True)\n\n随机取1个batch的DLNotes然后绘制出来。\nimport matplotlib.pyplot as plt\n# 显示图像\ndef show_batch(image_batch, label_batch):\n    plt.figure(figsize&#x3D;(10,10))\n    for n in range(15):\n        ax &#x3D; plt.subplot(5,5,n+1)\n        plt.imshow(image_batch[n]）\n        plt.axis(&#39;off&#39;)\n# 随机选择一个batch的图像        \nimage_batch, label_batch &#x3D; next(train_data_gen)\n# 图像显示\nshow_batch(image_batch, label_batch)\n\n\n我们使用在ImageNet数据集上预训练的ResNet-50作为源模型。这里指定weights=&#39;imagenet&#39;来自动下载并加载预训练的模型参数。在第一次使用时需要联网下载模型参数。\nKeras应用程序（keras.applications）是具有预先训练权值的固定架构，该类封装了很多重量级的网络架构，如下图所示：\n\n实现时实例化模型架构：\ntf.keras.applications.ResNet50(\n    include_top&#x3D;True, weights&#x3D;&#39;imagenet&#39;, input_tensor&#x3D;None, input_shape&#x3D;None,\n    pooling&#x3D;None, classes&#x3D;1000, **kwargs\n)\n\n主要参数：\n\ninclude_top: 是否包括顶层的全连接层。\nweights: None 代表随机初始化， ‘imagenet’ 代表加载在 ImageNet 上预训练的权值。\ninput_shape: 可选，输入尺寸元组，仅当 include_top&#x3D;False 时有效，否则输入形状必须是 (224, 224, 3)（channels_last 格式）或 (3, 224, 224)（channels_first 格式）。它必须为 3 个输入通道，且宽高必须不小于 32，比如 (200, 200, 3) 是一个合法的输入尺寸。\n\n在该案例中我们使用resNet50预训练模型构建模型：\n# 加载预训练模型\nResNet50 &#x3D; tf.keras.applications.ResNet50(weights&#x3D;&#39;imagenet&#39;, input_shape&#x3D;(224,224,3))\n# 设置所有层不可训练\nfor layer in ResNet50.layers:\n    layer.trainable &#x3D; False\n# 设置模型\nnet &#x3D; tf.keras.models.Sequential()\n# 预训练模型\nnet.add(ResNet50)\n# 展开\nnet.add(tf.keras.layers.Flatten())\n# 二分类的全连接层\nnet.add(tf.keras.layers.Dense(2, activation&#x3D;&#39;softmax&#39;))\n# 模型编译：指定优化器，损失函数和评价指标\nnet.compile(optimizer&#x3D;&#39;adam&#39;,\n            loss&#x3D;&#39;categorical_crossentropy&#39;,\n            metrics&#x3D;[&#39;accuracy&#39;])\n# 模型训练：指定数据，每一个epoch中只运行10个迭代，指定验证数据集\nhistory &#x3D; net.fit(\n                    train_data_gen,\n                    steps_per_epoch&#x3D;10,\n                    epochs&#x3D;3,\n                    validation_data&#x3D;test_data_gen,\n                    validation_steps&#x3D;10\n                    )\n\n\nEpoch 1&#x2F;3\n10&#x2F;10 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 28s 3s&#x2F;step - loss: 0.6931 - accuracy: 0.5031 - val_loss: 0.6930 - val_accuracy: 0.5094\nEpoch 2&#x2F;3\n10&#x2F;10 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 29s 3s&#x2F;step - loss: 0.6932 - accuracy: 0.5094 - val_loss: 0.6935 - val_accuracy: 0.4812\nEpoch 3&#x2F;3\n10&#x2F;10 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;] - 31s 3s&#x2F;step - loss: 0.6935 - accuracy: 0.4844 - val_loss: 0.6933 - val_accuracy: 0.4875\n\n\n","slug":"深度学习笔记","date":"2022-08-01T14:10:05.000Z","categories_index":"Deep Learning","tags_index":"深度学习,DLNotes分类","author_index":"Quanito"},{"id":"53b940f6ea7d5afb2d81c3c681083d3b","title":"基于R推荐系统搭建","content":"前言首先，R语言牛皮之处就不过多阐述，总之很方便很方便。R语言就是为了数据处理而生的，可以轻松链接数据库，可以轻松对数据进行处理和分析。\n这篇博客主要归纳一下如何使用R语言搭建一个简单的推荐系统。首先导入包：\n#install.packages(&quot;RJDBC&quot;) 如果第一次使用，要下载所需要的包，然后使用。\nlibrary(RJDBC)\n#以下是一些画图使用的包，还有一些机器学习算法。\nlibrary(rvest)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(scales)\nlibrary(maps)\nlibrary(mapproj)\nlibrary(plotly)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(C50)\nlibrary(tree)\nlibrary(ROCR)\nlibrary(randomForest)\nlibrary(e1071)\nlibrary(naivebayes)\nlibrary(nnet)\nlibrary(kknn)\n链接Oracle数据库如果有小伙伴想复现这个项目，可以直接去我的github下载csv数据文件，用R导入csv数据也是一样的效果(Imm..表只给出了前几百条数据，因为数据文件太大，无法上传仓库)：\n项目仓库\n虽然有些公司可能节约成本选择使用Mysql，但是学习Oracle还是非常有必要的。在顶端DBA大师眼中，能在PDB中进行协作的Oracle有着无可取代的天然优势。更别提我们可以通过Hive工具创建Oracle内外表后使用Oracle sql, Oracle Nosql, Hadoop HDFS, MongDB搭建数据湖。这对企业的大数据架构的整合有着巨大的价值。\n在此之前需要下载jdbc的jar包：点击链接进行下载drives文件\n链接配置规则： \nconn&lt;-dbConnect(drv,“jdbc:oracle:thin:@主机IP:1521:数据库名称”,“用户名称”,“密码”)\n远程链接：\ndrv &lt;- RJDBC::JDBC(driverClass &#x3D; &quot;oracle.jdbc.OracleDriver&quot;, classPath &#x3D;  Sys.glob(&quot;C:&#x2F;Users&#x2F;12506&#x2F;OneDrive&#x2F;Desktop&#x2F;ESTIA3A&#x2F;R&#x2F;Oracle&#x2F;drivers&#x2F;*&quot;))\n\nconn &lt;- dbConnect(drv, &quot;jdbc:oracle:thin:@(DESCRIPTION&#x3D;(ADDRESS&#x3D;(PROTOCOL&#x3D;TCP)(HOST&#x3D;144.21.67.201)(PORT&#x3D;1521))(CONNECT_DATA&#x3D;(SERVICE_NAME&#x3D;pdbest21.631174089.oraclecloud.internal)))&quot;, &quot;ZHANG2B20&quot;, &quot;ZHANG2B2001&quot;)\n\nallTables &lt;- dbGetQuery(conn, &quot;SELECT owner, table_name FROM all_tables where owner &#x3D; &#39;ZHANG2B20&#39;&quot;)\n本地链接(我没有试过，你们可以参考如下例子)：\ndrv&lt;-JDBC(&quot;oracle.jdbc.driver.OracleDriver&quot;,&quot;ojdbc6_g.jar&quot;, identifier.quote&#x3D;&quot;\\&quot;&quot;)  ##java中JDBC的套路\nconn&lt;-dbConnect(drv,&quot;jdbc:oracle:thin:@10.0.0.214:1521:zlhis&quot;,&quot;zlhis1234&quot;,&quot;his123&quot;) ##建立一个连接\nEMP&lt;-dbReadTable(conn,&#39;EMP&#39;) ##根据连接和表名获取Oracle中的表\ntable1&lt;-dbGetQuery(conn,&quot;select * from user_tables&quot;)  ##根据sql记录获取Oracle中表的数据\n————————————————\n版权声明：本文为CSDN博主「dltan」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\n原文链接：https:&#x2F;&#x2F;blog.csdn.net&#x2F;tandelin&#x2F;article&#x2F;details&#x2F;99300248\n如果数据有乱码：\nnames(table1)&#x3D;iconv(names(table1),&quot;UTF-8&quot;,&quot;GBK&quot;) ##若是表中列名为中文，读取时出现乱码，可用这句来搞定乱码情况\n\n所以按着我的例子来吧~ \n数据接下来介绍下数据(这里可以用view函数直接看出来各个表的数据)：\ntableCatalogue &lt;- dbGetQuery(conn, &quot;select * from Catalogue&quot;)\ntableClients &lt;- dbGetQuery(conn, &quot;select * from Clients&quot;)\ntableIm &lt;- dbGetQuery(conn, &quot;select * from IMMATRICULATION&quot;)\ntableMar &lt;- dbGetQuery(conn, &quot;select * from MARKETING&quot;)\n\n因为这是一个之前做过的项目，我直接读取之前产生的数据就好了\nload(&quot;C:&#x2F;Users&#x2F;12506&#x2F;OneDrive&#x2F;Desktop&#x2F;ESTIA3A&#x2F;R&#x2F;Projet&#x2F;.RData&quot;)\n\n一共有四个表:\n\nCatalogue 记录着一共270种汽车型号，不同的颜色，品牌，马力等等。该列表的270种是该公司向外出售的车类总类\n\nprint(tableCatalogue[1,])\n\n  MARQUE NOM PUISSANCE    LONGUEUR NBPLACES NBPORTES COULEUR OCCASION PRIX\n1    BMW  M5       507 tres longue        5        5    gris     TRUE    2\n  CATALOGUEID\n1         238\n\nsummary(tableCatalogue)\n\n\n    MARQUE              NOM              PUISSANCE       LONGUEUR        \n Length:270         Length:270         Min.   : 55.0   Length:270        \n Class :character   Class :character   1st Qu.:109.0   Class :character  \n Mode  :character   Mode  :character   Median :147.0   Mode  :character  \n                                       Mean   :157.6                     \n                                       3rd Qu.:170.0                     \n                                       Max.   :507.0                     \n    NBPLACES        NBPORTES       COULEUR            OCCASION        \n Min.   :5.000   Min.   :3.000   Length:270         Length:270        \n 1st Qu.:5.000   1st Qu.:5.000   Class :character   Class :character  \n Median :5.000   Median :5.000   Mode  :character   Mode  :character  \n Mean   :5.222   Mean   :4.815                                        \n 3rd Qu.:5.000   3rd Qu.:5.000                                        \n Max.   :7.000   Max.   :5.000                                        \n      PRIX        CATALOGUEID    \n Min.   :1.000   Min.   :  1.00  \n 1st Qu.:1.000   1st Qu.: 68.25  \n Median :2.000   Median :135.50  \n Mean   :1.685   Mean   :135.50  \n 3rd Qu.:2.000   3rd Qu.:202.75  \n Max.   :3.000   Max.   :270.00  \n\n\nClient 记录着以往的购买记录，客户的信息对应所购买的车牌号。记录着用户的年龄，性别，年税（可以推断大致固定收入），家庭状态，正在照顾的孩子个数，是否是名下第二台车以及车牌号。数据已脱敏，无法根据数据来确定用户身份\n\nprint(tableClients[1,])\n\n  AGE SEXE TAUX SITUATIONFAMILIALE NBENFANTSACHARGE DEUXIEMEVOITURE\n1  55    M  561          En Couple                2            TRUE\n  IMMATRICULATION\n1      1889 KV 55\n\nsummary(tableClients)\n\n\n      AGE           SEXE               TAUX           SITUATIONFAMILIALE\n Min.   :18.0   Length:11065       Length:11065       Length:11065      \n 1st Qu.:28.0   Class :character   Class :character   Class :character  \n Median :41.0   Mode  :character   Mode  :character   Mode  :character  \n Mean   :43.7                                                           \n 3rd Qu.:56.0                                                           \n Max.   :84.0                                                           \n NBENFANTSACHARGE DEUXIEMEVOITURE    IMMATRICULATION   \n Min.   :0.000    Length:11065       Length:11065      \n 1st Qu.:0.000    Class :character   Class :character  \n Median :1.000    Mode  :character   Mode  :character  \n Mean   :1.247                                         \n 3rd Qu.:2.000                                         \n Max.   :4.000                                         \n\n\nImmatriculation 记录着车牌号对应的汽车信息，这些汽车的种类包含于上述270种。该数据为法国车管所提供，敏感数据已经剔除。从表中可以看出车牌号对应的车的品牌，型号，功率，长短，座位数，门数，颜色，是否是二手车以及价格。\n\nprint(tableIm[1,])  \n\n  IMMATRICULATION MARQUE       NOM PUISSANCE LONGUEUR NBPLACES NBPORTES COULEUR\n1      5407 HD 88   Fiat Croma 2.2       147   longue        5        5   rouge\n  OCCASION PRIX\n1     TRUE    1\n\nsummary(tableIm)\n\n\n IMMATRICULATION       MARQUE              NOM              PUISSANCE    \n Length:1048575     Length:1048575     Length:1048575     Min.   : 55.0  \n Class :character   Class :character   Class :character   1st Qu.: 75.0  \n Mode  :character   Mode  :character   Mode  :character   Median :150.0  \n                                                          Mean   :198.9  \n                                                          3rd Qu.:245.0  \n                                                          Max.   :507.0  \n   LONGUEUR            NBPLACES    NBPORTES       COULEUR         \n Length:1048575     Min.   :5   Min.   :3.000   Length:1048575    \n Class :character   1st Qu.:5   1st Qu.:5.000   Class :character  \n Mode  :character   Median :5   Median :5.000   Mode  :character  \n                    Mean   :5   Mean   :4.868                     \n                    3rd Qu.:5   3rd Qu.:5.000                     \n                    Max.   :5   Max.   :5.000                     \n   OCCASION              PRIX      \n Length:1048575     Min.   :1.000  \n Class :character   1st Qu.:1.000  \n Mode  :character   Median :2.000  \n                    Mean   :1.796  \n                    3rd Qu.:2.000  \n                    Max.   :3.000  \n\n\nMarketing 9个新客户，为他们提供一个车型的推荐，最终交付成果是给他们\n\nprint(tableMar[1,])  \n\n  AGE SEXE TAUX SITUATIONFAMILIALE NBENFANTSACHARGE DEUXIEMEVOITURE\n1  21    F 1396        celibataire                0           FALSE\n\nsummary(tableMar)\n\n\n      AGE            SEXE                TAUX        SITUATIONFAMILIALE\n Min.   :21.00   Length:9           Min.   : 559.0   Length:9          \n 1st Qu.:35.00   Class :character   1st Qu.: 588.0   Class :character  \n Median :58.00   Mode  :character   Median : 748.0   Mode  :character  \n Mean   :51.56                      Mean   : 859.7                     \n 3rd Qu.:59.00                      3rd Qu.:1112.0                     \n Max.   :79.00                      Max.   :1396.0                     \n NBENFANTSACHARGE DEUXIEMEVOITURE   \n Min.   :0.0000   Length:9          \n 1st Qu.:0.0000   Class :character  \n Median :0.0000   Mode  :character  \n Mean   :0.4444                     \n 3rd Qu.:0.0000                     \n Max.   :2.0000                     \n\n将Immatriculation表与Catalogue表合并，可以标记每条数据对应的种类序号\ntableIm_f &lt;- merge(tableIm, tableCatalogue, by &#x3D; c(&quot;MARQUE&quot;,&quot;NOM&quot;, &quot;PUISSANCE&quot;, &quot;LONGUEUR&quot;, &quot;NBPORTES&quot;,&quot;COULEUR&quot;,&quot;OCCASION&quot;,&quot;PRIX&quot;))\n\nsummary(tableIm_f)\n\n\n    MARQUE              NOM              PUISSANCE       LONGUEUR        \n Length:1048575     Length:1048575     Min.   : 55.0   Length:1048575    \n Class :character   Class :character   1st Qu.: 75.0   Class :character  \n Mode  :character   Mode  :character   Median :150.0   Mode  :character  \n                                       Mean   :198.9                     \n                                       3rd Qu.:245.0                     \n                                       Max.   :507.0                     \n    NBPORTES       COULEUR            OCCASION              PRIX      \n Min.   :3.000   Length:1048575     Length:1048575     Min.   :1.000  \n 1st Qu.:5.000   Class :character   Class :character   1st Qu.:1.000  \n Median :5.000   Mode  :character   Mode  :character   Median :2.000  \n Mean   :4.868                                         Mean   :1.796  \n 3rd Qu.:5.000                                         3rd Qu.:2.000  \n Max.   :5.000                                         Max.   :3.000  \n IMMATRICULATION      NBPLACES.x   NBPLACES.y  CATALOGUEID   \n Length:1048575     Min.   :5    Min.   :5    Min.   :  1.0  \n Class :character   1st Qu.:5    1st Qu.:5    1st Qu.: 78.0  \n Mode  :character   Median :5    Median :5    Median :165.0  \n                    Mean   :5    Mean   :5    Mean   :154.7  \n                    3rd Qu.:5    3rd Qu.:5    3rd Qu.:234.0  \n                    Max.   :5    Max.   :5    Max.   :270.0  \n\nprint(tableIm_f[1,])  \n\n  MARQUE    NOM PUISSANCE LONGUEUR NBPORTES COULEUR OCCASION PRIX\n1   Audi A2 1.4        75   courte        5   blanc    FALSE    1\n  IMMATRICULATION NBPLACES.x NBPLACES.y CATALOGUEID\n1      5874 MF 21          5          5         270\n\n确定标签这一步还是非常重要的，也是熟悉业务的基础。从标签的角度来讲，我们需要知道哪些参数是未来想要被预测的，并且知道他们值的类型，是离散还是连续，是二分类还是多分类；从特征的角度来讲，我们需要去掉对标签毫无影响的特征。由于算法已经封装完整，默认建模后所有输入特征均为变量，所以我们在运行前需要把不相关列给过滤掉。这个项目变量复杂，只列举其中几个需要预测的标签。\n\nPrix价格，我们将原本离散的价格分成三段，Economique，Moyen,Luxe，法语对应的意思为，经济，中等，奢侈\nOccasion二手，为二分类变量是或否\nLongueur长度，分成四段，coute, longue, moyenne, tres longue,法语对应的意思为，短，长，中等，特长\nNbportes车门数量，经过后期可视化观察，一共有两种，3车门和5车门（包括行李箱门）\nCouleur颜色，一共有五种，blanc,bleu,gris,noir,rouge。法语对应的意思为白色，蓝色，灰色，黑色，红色。\n\nPrix价格数据准备取Immatriculation第八九列，可以得到车牌号对应车的价格\ntableImPrix &lt;- tableIm_f[c(8, 9)]\nprint(tableImPrix[1,]) \n\n  PRIX IMMATRICULATION\n1    1      5874 MF 21\n\nboxplot(tablePrixFinal$PRIX, data&#x3D;tablePrixFinal, main&#x3D;&quot;Distrubution de Prix&quot;, ylab&#x3D;&quot;Prix&quot;)\n## 通过这个语句刚开始可以看出价格是离散的，但是我们希望他是三分类标签，因此我们需要对其进行重定义\n\n\nError in x[floor(d)] + x[ceiling(d)]: 二进列运算符中有非数值参数\nTraceback:\n\n\n1. boxplot(tablePrixFinal$PRIX, data = tablePrixFinal, main = &quot;Distrubution de Prix&quot;, \n .     ylab = &quot;Prix&quot;)\n\n2. boxplot.default(tablePrixFinal$PRIX, data = tablePrixFinal, main = &quot;Distrubution de Prix&quot;, \n .     ylab = &quot;Prix&quot;)\n\n3. boxplot.stats(unclass(groups[[i]]), range)\n\n4. stats::fivenum(x, na.rm = TRUE)\n\n赋值，将每段赋予所对应的含义\ntable(tableImPrix$PRIX)\n##J&#39;ai pas touve une methode qui nous permets de realiser la meme fonction de Update\ntableImPrix &lt;- within(tableImPrix,&#123;\n  PRIX[PRIX &#x3D;&#x3D; 3] &lt;- &quot;Luxe&quot;\n  PRIX[PRIX &#x3D;&#x3D; 2] &lt;- &quot;Moyen&quot;\n  PRIX[PRIX &#x3D;&#x3D; 1] &lt;- &quot;Economique&quot;\n&#125;)\n\n\n\n     1      2      3 \n355549 551284 141742 \n\n将所得结果和Clients表合并，可以得到每个用户的信息和所购买车辆的价格的对应关系\ntablePrixFinal &lt;- merge(tableClients, tableImPrix, by &#x3D; &quot;IMMATRICULATION&quot;, incomparables &#x3D; NA)\n#requête pour la distribution des données: Prix\nsummary(tablePrixFinal$PRIX)\n\n\n   Length     Class      Mode \n    11082 character character \n\nprint(tablePrixFinal[1,]) \n\n  IMMATRICULATION AGE SEXE TAUX SITUATIONFAMILIALE NBENFANTSACHARGE\n1         1 EF 20  18    F  594        celibataire                0\n  DEUXIEMEVOITURE  PRIX\n1           FALSE Moyen\n\n可视化数据## attach()在R语言中表示添加路径存储的索引，相当于绑定一个数据框，如果未绑定路径索引，可能导致数据读取错误。\n## 简单来说固定之后就可以直接通过 数据框中的变量名 来调用数据了\nattach(tablePrixFinal)\nqplot(AGE, data&#x3D;tablePrixFinal, fill&#x3D;PRIX)\nqplot(SEXE, data&#x3D;tablePrixFinal, fill&#x3D;PRIX)\nqplot(TAUX, data&#x3D;tablePrixFinal, fill&#x3D;PRIX)\n\n\ntablePrixFinal &lt;- subset(tablePrixFinal, select&#x3D;-IMMATRICULATION)\n\n\n## 年龄，性别，税缴和购买车辆价格的关系\nqplot(AGE, data&#x3D;tablePrixFinal, fill&#x3D;PRIX)\nqplot(SEXE, data&#x3D;tablePrixFinal, fill&#x3D;PRIX)\nqplot(TAUX, data&#x3D;tablePrixFinal, fill&#x3D;PRIX)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n## 年龄，税缴和购买车辆价格的关系\nqplot(AGE, TAUX, data&#x3D;tablePrixFinal,color&#x3D;PRIX)\n\n\n\n\n划分训练集和验证集通常情况下需要按照数据量的大小选择多次cross-validation（交叉验证）本项目测试集为每个单表104万余条数据，选择了五次的交叉验证接下来就以一次交叉验证为例子\nPrix_EA &lt;- tablePrixFinal[1:7388,]\nPrix_ET &lt;- tablePrixFinal[7389:11082,]\n\n\n## 建模前观察变量的类型，如果有不对的，比如数值为string格式，需要提前变回来\nstr(Prix_EA)\n\n&#39;data.frame&#39;:    7388 obs. of  7 variables:\n $ AGE               : num  18 33 26 76 20 27 45 28 28 38 ...\n $ SEXE              : chr  &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;F&quot; ...\n $ TAUX              : chr  &quot;594&quot; &quot;1380&quot; &quot;597&quot; &quot;587&quot; ...\n $ SITUATIONFAMILIALE: chr  &quot;celibataire&quot; &quot;En Couple&quot; &quot;En Couple&quot; &quot;En Couple&quot; ...\n $ NBENFANTSACHARGE  : num  0 4 0 1 0 0 0 2 2 0 ...\n $ DEUXIEMEVOITURE   : chr  &quot;FALSE&quot; &quot;FALSE&quot; &quot;FALSE&quot; &quot;FALSE&quot; ...\n $ PRIX              : chr  &quot;Moyen&quot; &quot;Luxe&quot; &quot;Moyen&quot; &quot;Luxe&quot; ...\n\nPrix_EA$TAUX &lt;- as.integer(Prix_EA$TAUX)\nPrix_ET$TAUX &lt;- as.integer(Prix_ET$TAUX)\n\n比较三种决策树的性能\nrpart\nc5.0\nCART分类与回归树\n\nPrixtree1 &lt;- rpart(PRIX~., Prix_EA)\nprp(Prixtree1, type&#x3D;4, extra&#x3D;1, box.col&#x3D;c(&quot;tomato&quot;, &quot;skyblue&quot;)[Prixtree1$frame$yval])\n\n\n\n\nPrix_EA$PRIX &lt;- as.factor(Prix_EA$PRIX)\nPrixtree2 &lt;- C5.0(PRIX~., Prix_EA)\nplot(Prixtree2, type&#x3D;&quot;simple&quot;)\n\nWarning message in partysplit(varid = as.integer(i), breaks = as.numeric(j[1]), :\n&quot;强制改变过程中产生了NA&quot;\nWarning message in partysplit(varid = as.integer(i), breaks = as.numeric(j[1]), :\n&quot;强制改变过程中产生了NA&quot;\nWarning message in partysplit(varid = as.integer(i), breaks = as.numeric(j[1]), :\n&quot;强制改变过程中产生了NA&quot;\nWarning message in .bincode(as.numeric(x), breaks = unique(c(-Inf, breaks_split(split), :\n&quot;强制改变过程中产生了NA&quot;\nWarning message in .bincode(as.numeric(x), breaks = unique(c(-Inf, breaks_split(split), :\n&quot;强制改变过程中产生了NA&quot;\nWarning message in .bincode(as.numeric(x), breaks = unique(c(-Inf, breaks_split(split), :\n&quot;强制改变过程中产生了NA&quot;\n\n\nPrixtree3 &lt;- tree(PRIX~., data&#x3D;Prix_EA)\nplot(Prixtree3)\ntext(Prixtree3, pretty&#x3D;0)\n\nWarning message in tree(PRIX ~ ., data = Prix_EA):\n&quot;强制改变过程中产生了NA&quot;\n\n\n# 应用验证集\npredPrix.tree1 &lt;- predict(Prixtree1, Prix_ET, type&#x3D;&quot;class&quot;)\npredPrix.tree2 &lt;- predict(Prixtree2, Prix_ET, type&#x3D;&quot;class&quot;)\npredPrix.tree3 &lt;- predict(Prixtree3, Prix_ET, type&#x3D;&quot;class&quot;)\n# 输出混淆矩阵\ntable(Prix_ET$PRIX, predPrix.tree1)\ntable(Prix_ET$PRIX, predPrix.tree2)\ntable(Prix_ET$PRIX, predPrix.tree3)\n\nWarning message in pred1.tree(object, tree.matrix(newdata)):\n&quot;强制改变过程中产生了NA&quot;\n\n\n\n            predPrix.tree1\n             Economique Luxe Moyen\n  Economique       1187    1     1\n  Luxe                1  541   346\n  Moyen             250   32  1335\n\n\n\n            predPrix.tree2\n             Economique Luxe Moyen\n  Economique       1074    1   114\n  Luxe                1  537   350\n  Moyen             115    1  1501\n\n\n\n            predPrix.tree3\n             Economique Luxe Moyen\n  Economique        739    1   449\n  Luxe               94  551   243\n  Moyen             197   32  1388\n\n随机森林# 学习\nrfPrix &lt;- randomForest(PRIX~., Prix_EA)\n\n\n# 应用验证集\nrf_classPrix &lt;- predict(rfPrix,Prix_ET, type&#x3D;&quot;response&quot;)\n\n\n# 混淆矩阵\ntable(Prix_ET$PRIX, rf_classPrix)\n\n\n            rf_classPrix\n             Economique Luxe Moyen\n  Economique       1184    1     4\n  Luxe                1  544   343\n  Moyen             247   14  1356\n\n# 将结果以概率的形式展现出来\nrf_probPrix &lt;- predict(rfPrix, Prix_ET, type&#x3D;&quot;prob&quot;)\n\n\n# 输出结果为对应的每条数据，三种结果的百分比可能性\nprint(rf_probPrix[1,])  \n\nEconomique       Luxe      Moyen \n     0.000      0.218      0.782 \n\n支持向量机# 学习\nsvmPrix &lt;- svm(PRIX~., Prix_EA, probability&#x3D;TRUE)\n# 应用验证集\nsvm_classPrix &lt;- predict(svmPrix, Prix_ET, type&#x3D;&quot;response&quot;)\n\n\n# 混淆矩阵\ntable(Prix_ET$PRIX, svm_classPrix)\n\n\n            svm_classPrix\n             Economique Luxe Moyen\n  Economique       1187    1     1\n  Luxe                1  516   371\n  Moyen             251    6  1360\n\n# 将结果以概率的形式展现出来\nsvm_prob &lt;- predict(svmPrix, Prix_ET, probability&#x3D;TRUE)\n\n\n# 由于支持向量机必须是二分类标签，所以需要做如下操作\nsvm_prob &lt;- attr(svm_prob, &quot;probabilities&quot;)\n# 转成 data frame \nsvm_prob &lt;- as.data.frame(svm_prob)\n\n\n# 输出结果为对应的每条数据，三种结果的百分比可能性\nprint(svm_prob[1,])  \n\n         Moyen      Luxe   Economique\n7389 0.8062865 0.1936253 8.821378e-05\n\n朴素贝叶斯# 学习\nnbPrix &lt;- naive_bayes(PRIX~., Prix_EA)\n\n\n# 应用验证集\nnbPrix_class &lt;- predict(nbPrix, Prix_ET, type&#x3D;&quot;class&quot;)\n\n\n# 混淆矩阵\ntable( Prix_ET$PRIX, nbPrix_class)\n\n\n            nbPrix_class\n             Economique Luxe Moyen\n  Economique       1060  103    26\n  Luxe               37  584   267\n  Moyen             301  269  1047\n\n# 输出结果为对应的每条数据，三种结果的百分比可能性\nnbPrix_prob &lt;- predict(nbPrix, Prix_ET, type&#x3D;&quot;prob&quot;)\n\n\nprint(nbPrix_prob[1,])  \n\nEconomique       Luxe      Moyen \n 0.1389895  0.2819994  0.5790110 \n\n神经网络# 学习\nnnPrix &lt;- nnet(PRIX~., Prix_EA, size&#x3D;12)\n\n\n# 应用验证集\nnnPrix_class &lt;- predict(nnPrix, Prix_ET, type&#x3D;&quot;class&quot;)\n\n\n# 混淆矩阵\ntable(Prix_ET$PRIX, nnPrix_class)\n\n\n            nnPrix_class\n             Moyen\n  Economique  1189\n  Luxe         888\n  Moyen       1617\n\n# 输出结果为对应的每条数据，三种结果的百分比可能性\nnnPrix_prob &lt;- predict(nnPrix, Prix_ET, type&#x3D;&quot;raw&quot;)\n\n\nprint(nnPrix_prob[1,])  \n\nEconomique       Luxe      Moyen \n 0.3256632  0.2581213  0.4162155 \n\nK近邻# 学习，直接应用于验证集\nknnPrix &lt;- kknn(PRIX~., Prix_EA, Prix_ET)\n\n\n# 混淆矩阵\ntable(Prix_ET$PRIX, knnPrix$fitted.values)\n\n\n\n             Economique Luxe Moyen\n  Economique       1105    1    83\n  Luxe                3  631   254\n  Moyen             142  203  1272\n\n# 输出结果为对应的每条数据，三种结果的百分比可能性\nknnPrix_prob &lt;- as.data.frame(knnPrix$prob)\n\n\nprint(knnPrix_prob[1,])  \n\n  Economique      Luxe     Moyen\n1          0 0.5883995 0.4116005\n\n将表现最好的模型应用于将要预测的数据集# 可视化将要验证的数据集\nView(tableMar) \n#&#x3D;&#x3D;&#x3D; C5.0 概率&#x3D;&#x3D;&#x3D;#\nclass.treeC50 &lt;- predict(Prixtree2, tableMar, probability&#x3D;TRUE)\nprob.treeC50 &lt;- attr(class.treeC50, &quot;probabilities&quot;)\nprob.treeC50 &lt;- as.data.frame(prob.treeC50)\nresultatPrix &lt;- data.frame(tableMar$ID, class.treeC50, prob.treeC50)\n\n\n#&#x3D;&#x3D;&#x3D; C5.0分类 &#x3D;&#x3D;&#x3D;#\nclass.treeC50 &lt;- predict(Prixtree2, tableMar, type&#x3D;&quot;class&quot;)\nprob.treeC50 &lt;- predict(Prixtree2, tableMar, type&#x3D;&quot;prob&quot;)\nresultatPrix &lt;- data.frame(tableMar, class.treeC50, prob.treeC50)\nresultatPrix &lt;- data.frame(tableMar, class.treeC50)\n\n\n# 重命名预测列名称\nnames(resultatPrix)[7] &lt;- &quot;PRIX&quot;\n\n# 将结果保存到csv文件\nwrite.table(resultat1, file&#x3D;&#39;predictions.csv&#39;, sep&#x3D;&quot;\\t&quot;, dec&#x3D;&quot;.&quot;, row.names &#x3D; F)\n\nOccasion二手数据准备取Immatriculation第七九列，可以得到车牌号对应车的价格\ntableImOCCA &lt;- tableIm_f[c(7, 9)]\nprint(tableImOCCA[1,]) \n\n  OCCASION IMMATRICULATION\n1    FALSE      5874 MF 21\n\n## 将所得结果和Clients表合并，可以得到每个用户的信息和所购买车辆的二手情况的对应关系\ntableOCCAFinal &lt;- merge(tableClients, tableImOCCA, by &#x3D; &quot;IMMATRICULATION&quot;, incomparables &#x3D; NA)\n\n## 建模前观察变量的类型，如果有不对的，比如数值为string格式，需要提前变回来\nstr(tableOCCAFinal)\ntableOCCAFinal$TAUX &lt;- as.integer(tableOCCAFinal$TAUX)\n\n&#39;data.frame&#39;:    11082 obs. of  8 variables:\n $ IMMATRICULATION   : chr  &quot;1 EF 20&quot; &quot;10 ZM 12&quot; &quot;100 XL 72&quot; &quot;100 YT 70&quot; ...\n $ AGE               : num  18 33 26 76 20 27 45 28 28 38 ...\n $ SEXE              : chr  &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;F&quot; ...\n $ TAUX              : chr  &quot;594&quot; &quot;1380&quot; &quot;597&quot; &quot;587&quot; ...\n $ SITUATIONFAMILIALE: chr  &quot;celibataire&quot; &quot;En Couple&quot; &quot;En Couple&quot; &quot;En Couple&quot; ...\n $ NBENFANTSACHARGE  : num  0 4 0 1 0 0 0 2 2 0 ...\n $ DEUXIEMEVOITURE   : chr  &quot;FALSE&quot; &quot;FALSE&quot; &quot;FALSE&quot; &quot;FALSE&quot; ...\n $ OCCASION          : chr  &quot;FALSE&quot; &quot;FALSE&quot; &quot;FALSE&quot; &quot;TRUE&quot; ...\n\n可视化数据library(ggplot2)\n\n\nqplot(OCCASION, data&#x3D;tableOCCAFinal)\ntable(tableOCCAFinal$DEUXIEMEVOITURE,tableOCCAFinal$OCCASION)\nqplot(DEUXIEMEVOITURE, data&#x3D;tableOCCAFinal, color&#x3D;OCCASION)\nqplot(TAUX, data&#x3D;tableOCCAFinal, fill&#x3D;OCCASION, bins &#x3D;5)\nboxplot(AGE~OCCASION, data&#x3D;tableOCCAFinal, col&#x3D;c(&quot;red&quot;,&quot;blue&quot;))\nqplot(SEXE, data&#x3D;tableOCCAFinal, color&#x3D;OCCASION)\n\n\n\n        FALSE TRUE\n  FALSE  8408 1276\n  TRUE   1234  164\n\n\n\n\n\n\n划分训练集和验证集tableOCCAFinal &lt;- subset(tableOCCAFinal, select&#x3D;-IMMATRICULATION)\nOCCA_EA &lt;- tableOCCAFinal[1:7388,]\nOCCA_ET &lt;- tableOCCAFinal[7389:11082,]\n\n应用三种决策树和五种机器学习算法和之前价格的例子一样，我们对该训练集应用三种决策树和五种机器学习算法\n# 3种决策树\nOCCAtree1 &lt;- rpart(OCCASION~., OCCA_EA)\nOCCA_EA$OCCASION &lt;- as.factor(OCCA_EA$OCCASION)\nOCCAtree2 &lt;- C5.0(OCCASION~., OCCA_EA)\nOCCAtree3 &lt;- tree(OCCASION~., data&#x3D;OCCA_EA)\n# 随机森林\nrfOCCA &lt;- randomForest(OCCASION~., OCCA_EA)\n# 支持向量机\nsvmOCCA &lt;- svm(OCCASION~., OCCA_EA, probability&#x3D;TRUE)\n# 朴素贝叶斯\nnbOCCA &lt;- naive_bayes(OCCASION~., OCCA_EA)\n# 神经网络\nnnOCCA &lt;- nnet(OCCASION~., OCCA_EA, size&#x3D;12)\n# K近邻\nknnOCCA &lt;- kknn(OCCASION~., OCCA_EA, OCCA_ET)\n\n将学习好的模型应用于验证集## 应用于验证集，k近邻算法在训练时已经得到了结果\npredOCCA.tree1 &lt;- predict(OCCAtree1, OCCA_ET, type&#x3D;&quot;class&quot;)\npredOCCA.tree2 &lt;- predict(OCCAtree2, OCCA_ET, type&#x3D;&quot;class&quot;)\npredOCCA.tree3 &lt;- predict(OCCAtree3, OCCA_ET, type&#x3D;&quot;class&quot;)\nresult.rfOCCA &lt;- predict(rfOCCA,OCCA_ET, type&#x3D;&quot;response&quot;)\nresult.svmOCCA &lt;- predict(svmOCCA,OCCA_ET, type&#x3D;&quot;response&quot;)\nresult.treeNaiveOCCA &lt;- predict(nbOCCA,OCCA_ET, type&#x3D;&quot;class&quot;)\nresult.treeNnetOCCA &lt;- predict(nnOCCA, OCCA_ET,type&#x3D;&quot;class&quot;)\n\n混淆矩阵对比table(OCCA_ET$OCCASION, predOCCA.tree1)\ntable(OCCA_ET$OCCASION, predOCCA.tree2)\ntable(OCCA_ET$OCCASION, predOCCA.tree3)\ntable(OCCA_ET$OCCASION, result.rfOCCA)\ntable(OCCA_ET$OCCASION, result.svmOCCA)\ntable(OCCA_ET$OCCASION, result.treeNaiveOCCA)\ntable(OCCA_ET$OCCASION, result.treeNnetOCCA)\ntable(OCCA_ET$OCCASION, knnOCCA$fitted.values)\n\n\n       predOCCA.tree1\n        FALSE TRUE\n  FALSE  3189   37\n  TRUE    290  178\n\n\n\n       predOCCA.tree2\n        FALSE TRUE\n  FALSE  3189   37\n  TRUE    290  178\n\n\n\n       predOCCA.tree3\n        FALSE TRUE\n  FALSE  3189   37\n  TRUE    290  178\n\n\n\n       result.rfOCCA\n        FALSE TRUE\n  FALSE  3181   45\n  TRUE    290  178\n\n\n\n       result.svmOCCA\n        FALSE TRUE\n  FALSE  3186   40\n  TRUE    307  161\n\n\n\n       result.treeNaiveOCCA\n        FALSE TRUE\n  FALSE  2518  708\n  TRUE      2  466\n\n\n\n       result.treeNnetOCCA\n        FALSE\n  FALSE  3226\n  TRUE    468\n\n\n\n       \n        FALSE TRUE\n  FALSE  3020  206\n  TRUE    228  240\n\nROC曲线和AUC对比在画出ROC曲线前，我们需要把所有的模型转成概率形式\n# Test du classifieur : probabilites pour chaque prediction\np.treeRpartOCCA &lt;- predict(OCCAtree1, OCCA_ET, type&#x3D;&quot;prob&quot;)\nroc.predOCCA1 &lt;- prediction(p.treeRpartOCCA[,2], OCCA_ET$OCCASION)\n\np.treec50OCCA &lt;- predict(OCCAtree2, OCCA_ET, type&#x3D;&quot;prob&quot;)\nroc.predOCCA2 &lt;- prediction(p.treec50OCCA[,2], OCCA_ET$OCCASION)\n\np.treeTreeOCCA &lt;- predict(OCCAtree3, OCCA_ET, type&#x3D;&quot;vector&quot;)\nroc.predOCCA3 &lt;- prediction(p.treeTreeOCCA[,2], OCCA_ET$OCCASION)\n\nrf_probOCCA &lt;- predict(rfOCCA, OCCA_ET, type&#x3D;&quot;prob&quot;)\nroc.predOCCA4 &lt;- prediction(rf_probOCCA[,2], OCCA_ET$OCCASION)\n\nsvm_probOCCA &lt;- predict(svmOCCA, OCCA_ET, probability&#x3D;TRUE)\nsvm_probOCCA &lt;- attr(svm_probOCCA, &quot;probabilities&quot;)\nsvm_probOCCA &lt;- as.data.frame(svm_probOCCA)\nroc.predOCCA5 &lt;- prediction(svm_probOCCA[,2], OCCA_ET$OCCASION)\n\n\nnb_probOCCA &lt;- predict(nbOCCA, OCCA_ET, type&#x3D;&quot;prob&quot;)\nroc.predOCCA6 &lt;- prediction(nb_probOCCA[,2], OCCA_ET$OCCASION)\n\n\nnn_probOCCA &lt;- predict(nnOCCA, OCCA_ET, type&#x3D;&quot;raw&quot;)\nroc.predOCCA7 &lt;- prediction(nn_probOCCA[,1], OCCA_ET$OCCASION)\n\n\nknn_probOCCA &lt;- as.data.frame(knnOCCA$prob)\nroc.predOCCA8 &lt;- prediction(knn_probOCCA[,2], OCCA_ET$OCCASION)\n\nWarning message in pred1.tree(object, tree.matrix(newdata)):\n&quot;强制改变过程中产生了NA&quot;\nWarning message:\n&quot;predict.naive_bayes(): more features in the newdata are provided as there are probability tables in the object. Calculation is performed based on features to be found in the tables.&quot;\n\n然后计算出AUC指数\nauc.treeOCCA1 &lt;- performance(roc.predOCCA1, &quot;auc&quot;)\nauc.treeOCCA2 &lt;- performance(roc.predOCCA2, &quot;auc&quot;)\nauc.treeOCCA3 &lt;- performance(roc.predOCCA3, &quot;auc&quot;)\nauc.treeOCCA4 &lt;- performance(roc.predOCCA4, &quot;auc&quot;)\nauc.treeOCCA5 &lt;- performance(roc.predOCCA5, &quot;auc&quot;)\nauc.treeOCCA6 &lt;- performance(roc.predOCCA6, &quot;auc&quot;)\nauc.treeOCCA7 &lt;- performance(roc.predOCCA7, &quot;auc&quot;)\nauc.treeOCCA8 &lt;- performance(roc.predOCCA8, &quot;auc&quot;)\n\nattr(auc.treeOCCA1, &quot;y.values&quot;)\nattr(auc.treeOCCA2, &quot;y.values&quot;)\nattr(auc.treeOCCA3, &quot;y.values&quot;)\nattr(auc.treeOCCA4, &quot;y.values&quot;)\nattr(auc.treeOCCA5, &quot;y.values&quot;)\nattr(auc.treeOCCA6, &quot;y.values&quot;)\nattr(auc.treeOCCA7, &quot;y.values&quot;)\nattr(auc.treeOCCA8, &quot;y.values&quot;)\n\n\n\n    0.924155896800038\n\n\n\n\n\n\n    0.924155896800038\n\n\n\n\n\n\n    0.924155896800038\n\n\n\n\n\n\n    0.9247705607749\n\n\n\n\n\n\n    0.920831213802376\n\n\n\n\n\n\n    0.924622524785262\n\n\n\n\n\n\n    0.5\n\n\n\n\n\n\n    0.903374558210268\n\n\n\n\n画出ROC曲线，根据AUC指数选择该标签最适合的算法roc.perfOCCA1 &lt;- performance(roc.predOCCA1,&quot;tpr&quot;,&quot;fpr&quot;)\nroc.perfOCCA2 &lt;- performance(roc.predOCCA2,&quot;tpr&quot;,&quot;fpr&quot;)\nroc.perfOCCA3 &lt;- performance(roc.predOCCA3,&quot;tpr&quot;,&quot;fpr&quot;)\nroc_perfOCCA4 &lt;- performance(roc.predOCCA4,&quot;tpr&quot;,&quot;fpr&quot;)\nroc.perfOCCA5 &lt;- performance(roc.predOCCA5,&quot;tpr&quot;,&quot;fpr&quot;)\nroc.perfOCCA6 &lt;- performance(roc.predOCCA6,&quot;tpr&quot;,&quot;fpr&quot;)\nroc.perfOCCA7 &lt;- performance(roc.predOCCA7,&quot;tpr&quot;,&quot;fpr&quot;)\nroc.perfOCCA8 &lt;- performance(roc.predOCCA8,&quot;tpr&quot;,&quot;fpr&quot;)\n\n\n# Courbe ROC\nplot(roc.perfOCCA1, col &#x3D; &quot;green&quot;)\nplot(roc.perfOCCA2, col &#x3D; &quot;red&quot;, add&#x3D;TRUE)\nplot(roc.perfOCCA3, col &#x3D; &quot;blue&quot;, add&#x3D;TRUE)\nplot(roc_perfOCCA4, add &#x3D; TRUE, col &#x3D; &quot;magenta&quot;)\nplot(roc.perfOCCA5, add &#x3D; TRUE, col &#x3D; &quot;darkorange&quot;)\nplot(roc.perfOCCA6, add &#x3D; TRUE, col &#x3D; &quot;darkgreen&quot;)\nplot(roc.perfOCCA7, add &#x3D; TRUE, col &#x3D; &quot;black&quot;)\nplot(roc.perfOCCA8, add &#x3D; TRUE, col &#x3D; &quot;darkmagenta&quot;)\n\n\n\n\n将表现最好的模型应用于将要预测的数据集#&#x3D;&#x3D;&#x3D; RANDOM FORESTS &#x3D;&#x3D;&#x3D;#\nclass.treerfOCCA &lt;- predict(rfOCCA, tableMar, probability&#x3D;TRUE)\nresultatOCCA &lt;- data.frame(tableMar, class.treerfOCCA)\n\n# Renommage de la colonne des classes predites\nnames(resultatOCCA)[7] &lt;- &quot;OCCASION&quot;\n\nNBPortes车门数量数据准备取Immatriculation第五九列，可以得到车牌号对应车的车门数\ntableImNP&lt;- tableIm_f[c(5,9)]\nprint(tableImNP[1,]) \n\n  NBPORTES IMMATRICULATION\n1        5      5874 MF 21\n\n## 将所得结果和Clients表合并，可以得到每个用户的信息和所购买车辆的车门数量的对应关系\ntableFinal_np&lt;- merge(tableClients, tableImNP,by&#x3D; &quot;IMMATRICULATION&quot;, incomparables &#x3D;NA)\nprint(tableFinal_np[1,]) \n\n  IMMATRICULATION AGE SEXE TAUX SITUATIONFAMILIALE NBENFANTSACHARGE\n1         1 EF 20  18    F  594        celibataire                0\n  DEUXIEMEVOITURE NBPORTES\n1           FALSE        5\n\nsummary(tableFinal_np$NBPORTES)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  3.000   5.000   5.000   4.951   5.000   5.000 \n\n## 建模前观察变量的类型，如果有不对的，比如数值为string格式，需要提前变回来\nNBPORTES_EA$TAUX &lt;- as.integer(NBPORTES_EA$TAUX)\nNBPORTES_ET$TAUX &lt;- as.integer(NBPORTES_ET$TAUX)\n\n可视化数据attach(tableFinal_np)\ntableFinal_np&lt;-subset(tableFinal_np, select &#x3D; -IMMATRICULATION)\nqplot(NBPORTES, data&#x3D;tableFinal_np)\nqplot(SEXE,data&#x3D;tableFinal_np, fill&#x3D;NBPORTES)\nqplot(NBENFANTSACHARGE,data&#x3D;tableFinal_np, fill&#x3D;NBPORTES)\nqplot(TAUX,data&#x3D;tableFinal_np, fill&#x3D;NBPORTES, bins&#x3D;5)\nqplot(DEUXIEMEVOITURE, data&#x3D;tableFinal_np, fill&#x3D;NBPORTES)\nqplot(SITUATIONFAMILIALE, data&#x3D;tableFinal_np, fill&#x3D;NBPORTES)\nboxplot(AGE~NBPORTES, data&#x3D;tableFinal_np, col&#x3D;c(&quot;red&quot;,&quot;blue&quot;))\n\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning message:\n&quot;Ignoring unknown parameters: bins&quot;\n\n\n\n\n\n\n划分训练集和验证集NBPORTES_EA &lt;- tableFinal_np[1:7388,]\nNBPORTES_ET &lt;- tableFinal_np[7389:11082,]\n\n应用三种决策树和五种机器学习算法和之前价格的例子一样，我们对该训练集应用三种决策树和五种机器学习算法\nNBPORTESTree1&lt;-rpart(NBPORTES~., NBPORTES_EA)\nNBPORTESTree2&lt;-C5.0(NBPORTES~., NBPORTES_EA)\nNBPORTESTree3&lt;-tree(NBPORTES~., data&#x3D;NBPORTES_EA)\nrfNP&lt;-randomForest(NBPORTES~., NBPORTES_EA)\nsvmNP&lt;-svm(NBPORTES~., NBPORTES_EA,probability&#x3D;TRUE)\nnbNP&lt;-naive_bayes(NBPORTES~., NBPORTES_EA)\nnnNP&lt;-nnet(NBPORTES~., NBPORTES_EA,size&#x3D;12)\nknnNP&lt;-kknn(NBPORTES~., NBPORTES_EA,NBPORTES_ET)\n\n将学习好的模型应用于验证集## 应用于验证集，k近邻算法在训练时已经得到了结果\npredNBPORTES.tree1 &lt;- predict(NBPORTESTree1, NBPORTES_ET, type&#x3D;&quot;vector&quot;)\npredNBPORTES.tree2 &lt;- predict(NBPORTESTree2, NBPORTES_ET, type&#x3D;&quot;class&quot;)\npredNBPORTES.tree3 &lt;- predict(NBPORTESTree3, NBPORTES_ET, type&#x3D;&quot;class&quot;)\nrf_classNP&lt;-predict(rfNP,NBPORTES_EA,type &#x3D; &quot;response&quot;)\nsvm_classNP&lt;-predict(svmNP,NBPORTES_ET,type &#x3D; &quot;response&quot;)\nnbNP_class&lt;-predict(nbNP,NBPORTES_ET,type &#x3D; &quot;class&quot;)\nnnNP_class&lt;-predict(nnNP,NBPORTES_ET, TYPE&#x3D;&quot;class&quot;)\n\n混淆矩阵对比table(NBPORTES_ET$NBPORTES,predNBPORTES.tree1)\ntable(NBPORTES_ET$NBPORTES,predNBPORTES.tree2)\ntable(NBPORTES_ET$NBPORTES,predNBPORTES.tree3)\ntable(NBPORTES_EA$NBPORTES,rf_classNP)\ntable(NBPORTES_ET$NBPORTES,svm_classNP)\ntable(NBPORTES_ET$NBPORTES,nbNP_class)\ntable(NBPORTES_ET$NBPORTES,nnNP_class)\ntable(NBPORTES_ET$NBPORTES,knnNP$fitted.values)\n\n\n   predNBPORTES.tree1\n    1.63414634146341 1.8323782234957 1.99980525803311    2\n  3               31              58                2    0\n  5               65             275             2518  745\n\n\n\n   predNBPORTES.tree2\n       1    2\n  3    0   91\n  5    0 3603\n\n\n\n   predNBPORTES.tree3\n       1    2\n  3    0   91\n  5    0 3603\n\n\n\n   rf_classNP\n       1    2\n  1   24  154\n  2    0 7210\n\n\n\n   svm_classNP\n       1    2\n  3    0   91\n  5    0 3603\n\n\n\n   nbNP_class\n       1    2\n  3   64   27\n  5  268 3335\n\n\n\n   nnNP_class\n    0.975906842537507\n  3                91\n  5              3603\n\n\n\n   \n       1    2\n  3    9   82\n  5   34 3569\n\nROC曲线和AUC对比在画出ROC曲线前，我们需要把所有的模型转成概率形式\npNP.tree1&lt;-predict(NBPORTESTree1,NBPORTES_ET,type &#x3D; &quot;vector&quot;)\nrocNP.pred1&lt;-prediction(pNP.tree1,NBPORTES_ET$NBPORTES)\n\npNP.tree2&lt;-predict(NBPORTESTree2,NBPORTES_ET,type &#x3D; &quot;prob&quot;)\nrocNP.pred2&lt;-prediction(pNP.tree2[,2],NBPORTES_ET$NBPORTES)\n\npNP.tree3&lt;-predict(NBPORTESTree3,NBPORTES_ET,type &#x3D; &quot;vector&quot;)\nrocNP.pred3&lt;-prediction(pNP.tree3[,2],NBPORTES_ET$NBPORTES)\n\nrf_probNP&lt;-predict(rfNP,NBPORTES_ET, type &#x3D; &quot;prob&quot; )\nrf_predNP&lt;-prediction(rf_probNP[,2],NBPORTES_ET$NBPORTES)\n\nsvm_prob&lt;-predict(svmNP,NBPORTES_ET,probability&#x3D;TRUE)\nsvm_prob&lt;-attr(svm_prob,&quot;probabilities&quot;)\nsvm_prob&lt;-as.data.frame(svm_prob)\nsvm_pred&lt;-prediction(svm_prob[,2],NBPORTES_ET$NBPORTES)\n\nnbNP_prob&lt;-predict(nbNP,NBPORTES_ET,type&#x3D;&quot;prob&quot;)\nnbNP_pred&lt;-prediction(nbNP_prob[,2],NBPORTES_ET$NBPORTES)\n\nnnNP_prob&lt;-predict(nnNP,NBPORTES_ET,type &#x3D; &quot;raw&quot;)\nnnNP_pred&lt;-prediction(nnNP_prob,NBPORTES_ET$NBPORTES)\n\nknnNP_prob&lt;-as.data.frame(knnNP$prob)\nknnNP_pred&lt;-prediction(knnNP_prob[,2],NBPORTES_ET$NBPORTES)\n\n然后计算出AUC指数\naucNP.tree1&lt;-performance(rocNP.pred1,&quot;auc&quot;)\naucNP.tree2&lt;-performance(rocNP.pred2,&quot;auc&quot;)\naucNP.tree3&lt;-performance(rocNP.pred3,&quot;auc&quot;)\nrf_aucNP&lt;-performance(rf_predNP,&quot;auc&quot;)\nsvm_aucNP&lt;-performance(svm_pred,&quot;auc&quot;)\nnbNP_aucNP&lt;-performance(nbNP_pred,&quot;auc&quot;)\nnnNP_aucNP&lt;-performance(nnNP_pred,&quot;auc&quot;)\nknnNP_aucNP&lt;-performance(knnNP_pred,&quot;auc&quot;)\n\n\nattr(aucNP.tree1,&quot;y.values&quot;)\nattr(aucNP.tree2,&quot;y.values&quot;)\nattr(aucNP.tree3,&quot;y.values&quot;)\nattr(rf_aucNP,&quot;y.values&quot;)\nattr(svm_aucNP,&quot;y.values&quot;)\nattr(nbNP_aucNP,&quot;y.values&quot;)\nattr(nnNP_aucNP,&quot;y.values&quot;)\nattr(knnNP_aucNP,&quot;y.values&quot;)\n\n\n\n    0.951351590402381\n\n\n\n\n\n\n    0.5\n\n\n\n\n\n\n    0.903157625056043\n\n\n\n\n\n\n    0.891982566420535\n\n\n\n\n\n\n    0.0633049991917602\n\n\n\n\n\n\n    0.93451122843296\n\n\n\n\n\n\n    0.5\n\n\n\n\n\n\n    0.85450006557417\n\n\n\n\n画出ROC曲线，根据AUC指数选择该标签最适合的算法rocNP.pref1&lt;-performance(rocNP.pred1,&quot;tpr&quot;,&quot;fpr&quot;)\nrocNP.pref2&lt;-performance(rocNP.pred2,&quot;tpr&quot;,&quot;fpr&quot;)  \nrocNP.pref3&lt;-performance(rocNP.pred3,&quot;tpr&quot;,&quot;fpr&quot;) \nrf_prefNP&lt;-performance(rf_predNP,&quot;tpr&quot;,&quot;fpr&quot;)\nsvm_pref&lt;-performance(rf_predNP,&quot;tpr&quot;,&quot;fpr&quot;)\nnbNP_pref&lt;-performance(nbNP_pred,&quot;tpr&quot;,&quot;fpr&quot;)\nnnNP_pref&lt;-performance(nnNP_pred,&quot;tpr&quot;,&quot;fpr&quot;)\nknnNP_pref&lt;-performance(knnNP_pred,&quot;tpr&quot;,&quot;fpr&quot;)\n\n\nplot(rocNP.pref1,col&#x3D;&quot;green&quot;)\nplot(rocNP.pref2,add &#x3D; TRUE,col&#x3D;&quot;blue&quot;)\nplot(rocNP.pref3,add &#x3D; TRUE,col&#x3D;&quot;red&quot;)\nplot(rf_prefNP,add &#x3D; TRUE, col&#x3D;&quot;yellow&quot;)\nplot(svm_pref,add &#x3D; TRUE, col&#x3D;&quot;black&quot;)\nplot(nbNP_pref,add &#x3D; TRUE, col&#x3D;&quot;purple&quot;)\nplot(nnNP_pref,add &#x3D; TRUE, col&#x3D;&quot;orange&quot;)\nplot(nnNP_pref,add &#x3D; TRUE, col&#x3D;&quot;black&quot;)\n\n\n\n\n将表现最好的模型应用于将要预测的数据集class.treerpartNB &lt;- predict(nbNP, tableMar, probability&#x3D;TRUE)\nresultatNB &lt;- data.frame(tableMar, class.treerpartNB)\n\n# Renommage de la colonne des classes predites\nnames(resultatNB)[7] &lt;- &quot;NBPORTES&quot;\n\nLongueur车长数据准备取Immatriculation第四九列，可以得到车牌号对应车的车长\ntableImLG &lt;- tableIm_f[c(4, 9)]\ntableLGFinal &lt;- merge(tableClients, tableImLG, by &#x3D; &quot;IMMATRICULATION&quot;, incomparables &#x3D; NA)\n\n\n## 更改变量类型\nstr(tableLGFinal)\ntableLGFinal$TAUX &lt;- as.integer(tableLGFinal$TAUX)\n## 剔除不需要的列\ntableLGFinal &lt;- subset(tableLGFinal, select&#x3D;-IMMATRICULATION)\n\n&#39;data.frame&#39;:    11082 obs. of  8 variables:\n $ IMMATRICULATION   : chr  &quot;1 EF 20&quot; &quot;10 ZM 12&quot; &quot;100 XL 72&quot; &quot;100 YT 70&quot; ...\n $ AGE               : num  18 33 26 76 20 27 45 28 28 38 ...\n $ SEXE              : chr  &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;F&quot; ...\n $ TAUX              : chr  &quot;594&quot; &quot;1380&quot; &quot;597&quot; &quot;587&quot; ...\n $ SITUATIONFAMILIALE: chr  &quot;celibataire&quot; &quot;En Couple&quot; &quot;En Couple&quot; &quot;En Couple&quot; ...\n $ NBENFANTSACHARGE  : num  0 4 0 1 0 0 0 2 2 0 ...\n $ DEUXIEMEVOITURE   : chr  &quot;FALSE&quot; &quot;FALSE&quot; &quot;FALSE&quot; &quot;FALSE&quot; ...\n $ LONGUEUR          : chr  &quot;moyenne&quot; &quot;tres longue&quot; &quot;tres longue&quot; &quot;tres longue&quot; ...\n\n可视化数据# 从可视化可以大致看出相关性，如果变量相关性很好，那么出来的模型准确率越高\nlibrary(ggplot2)\nqplot(LONGUEUR, data&#x3D;tableLGFinal)\ntable(tableLGFinal$DEUXIEMEVOITURE,tableLGFinal$LONGUEUR)\nqplot(DEUXIEMEVOITURE, data&#x3D;tableLGFinal, color&#x3D;LONGUEUR)\n\n\n\n        courte longue moyenne tres longue\n  FALSE   2569   3009     777        3329\n  TRUE     944      0       0         454\n\n\n\nqplot(TAUX, data&#x3D;tableLGFinal, fill&#x3D;LONGUEUR, bins &#x3D;5)\nboxplot(AGE~LONGUEUR, data&#x3D;tableLGFinal, col&#x3D;c(&quot;red&quot;,&quot;blue&quot;))\n\n\n\n\n\nqplot(SEXE, data&#x3D;tableLGFinal, color&#x3D;LONGUEUR)\n\n\n\n\n划分训练集和验证集LG_EA &lt;- tableLGFinal[1:7388,]\nLG_ET &lt;- tableLGFinal[7389:11082,]\nLG_EA$LONGUEUR &lt;- as.factor(LG_EA$LONGUEUR)\n\n应用三种决策树和五种机器学习算法和之前价格的例子一样，我们对该训练集应用三种决策树和五种机器学习算法\nLGtree1 &lt;- rpart(LONGUEUR~., LG_EA)\nLGtree2 &lt;- C5.0(LONGUEUR~., LG_EA)\nLGtree3 &lt;- tree(LONGUEUR~., data&#x3D;LG_EA)\nrfLG &lt;- randomForest(LONGUEUR~., LG_EA)\nsvmLG &lt;- svm(LONGUEUR~., LG_EA, probability&#x3D;TRUE)\nnbLG &lt;- naive_bayes(LONGUEUR~., LG_EA)\nnnLG &lt;- nnet(LONGUEUR~., LG_EA, size&#x3D;12)\nknnLG &lt;- kknn(LONGUEUR~., LG_EA, LG_ET)\n\nWarning message in tree(LONGUEUR ~ ., data = LG_EA):\n&quot;强制改变过程中产生了NA&quot;\nWarning message:\n&quot;naive_bayes(): Feature SITUATIONFAMILIALE - zero probabilities are present. Consider Laplace smoothing.&quot;\nWarning message:\n&quot;naive_bayes(): Feature DEUXIEMEVOITURE - zero probabilities are present. Consider Laplace smoothing.&quot;\n\n\n# weights:  184\ninitial  value 12969.213412 \niter  10 value 9469.911371\niter  20 value 9305.689396\niter  30 value 8970.449826\niter  40 value 8689.100430\niter  50 value 8419.405933\niter  60 value 8266.730127\niter  70 value 8167.329971\niter  80 value 7592.378033\niter  90 value 7546.309856\niter 100 value 7458.459896\nfinal  value 7458.459896 \nstopped after 100 iterations\n\n将学习好的模型应用于验证集predLG.tree1 &lt;- predict(LGtree1, LG_ET, type&#x3D;&quot;class&quot;)\npredLG.tree2 &lt;- predict(LGtree2, LG_ET, type&#x3D;&quot;class&quot;)\npredLG.tree3 &lt;- predict(LGtree3, LG_ET, type&#x3D;&quot;class&quot;)\nresult.rfLG &lt;- predict(rfLG,LG_ET, type&#x3D;&quot;response&quot;)\nresult.svmLG &lt;- predict(svmLG,LG_ET, type&#x3D;&quot;response&quot;)\nresult.treeNaiveLG &lt;- predict(nbLG,LG_ET, type&#x3D;&quot;class&quot;)\nresult.treeNnetLG &lt;- predict(nnLG, LG_ET,type&#x3D;&quot;class&quot;)\n\nWarning message in pred1.tree(object, tree.matrix(newdata)):\n&quot;强制改变过程中产生了NA&quot;\nWarning message:\n&quot;predict.naive_bayes(): more features in the newdata are provided as there are probability tables in the object. Calculation is performed based on features to be found in the tables.&quot;\n\n混淆矩阵对比table(LG_ET$LONGUEUR, predLG.tree1)\ntable(LG_ET$LONGUEUR, predLG.tree2)\ntable(LG_ET$LONGUEUR, predLG.tree3)\ntable(LG_ET$LONGUEUR, result.rfLG)\ntable(LG_ET$LONGUEUR, result.svmLG)\ntable(LG_ET$LONGUEUR, result.treeNaiveLG)\ntable(LG_ET$LONGUEUR, result.treeNnetLG)\ntable(LG_ET$LONGUEUR, knnLG$fitted.values)\n\n\n             predLG.tree1\n              courte longue moyenne tres longue\n  courte        1166      1       0           1\n  longue           1   1021       0           0\n  moyenne        270      0       0           0\n  tres longue      1    493       0         740\n\n\n\n             predLG.tree2\n              courte longue moyenne tres longue\n  courte        1166      1       0           1\n  longue           1   1021       0           0\n  moyenne        270      0       0           0\n  tres longue      1    493       0         740\n\n\n\n             predLG.tree3\n              courte longue moyenne tres longue\n  courte         940    227       0           1\n  longue         304    675       0          43\n  moyenne        270      0       0           0\n  tres longue    163    310       0         761\n\n\n\n             result.rfLG\n              courte longue moyenne tres longue\n  courte        1156      1      10           1\n  longue           1   1015       0           6\n  moyenne        261      0       9           0\n  tres longue      1    493       0         740\n\n\n\n             result.svmLG\n              courte longue moyenne tres longue\n  courte        1166      1       0           1\n  longue           2   1020       0           0\n  moyenne        270      0       0           0\n  tres longue      1    493       0         740\n\n\n\n             result.treeNaiveLG\n              courte longue moyenne tres longue\n  courte         332      0     684         152\n  longue          12    661       2         347\n  moyenne          5      0     265           0\n  tres longue      7    320       0         907\n\n\n\n             result.treeNnetLG\n              courte tres longue\n  courte        1043         125\n  longue         406         616\n  moyenne        270           0\n  tres longue    209        1025\n\n\n\n             \n              courte longue moyenne tres longue\n  courte        1063      1     103           1\n  longue           1    744       0         277\n  moyenne        154      0     116           0\n  tres longue      2    355       0         877\n\n可视化决策树结构其实其他五种结构都能查看，这里我就不去运行了\nprp(LGtree1, type&#x3D;4, extra&#x3D;1, box.col&#x3D;c(&quot;tomato&quot;, &quot;skyblue&quot;)[LGtree1$frame$yval])\nplot(LGtree2, type&#x3D;&quot;simple&quot;)\nplot(LGtree3)\ntext(LGtree3, pretty&#x3D;0)\n\n将表现最好的模型应用于将要预测的数据集#&#x3D;&#x3D;&#x3D; rpart &#x3D;&#x3D;&#x3D;#\nclass.treerpLG &lt;- predict(LGtree1, tableMar, type&#x3D;&quot;class&quot;)\n\nresultatLG &lt;- data.frame(tableMar, class.treerpLG)\n\n# Renommage de la colonne des classes predites\nnames(resultatLG)[7] &lt;- &quot;LONGUEUR&quot;\n\nCouleur颜色数据准备取Immatriculation第六九列，可以得到车牌号对应车的车长\ntableImCL &lt;- tableIm_f[c(6, 9)]\ntableCLFinal &lt;- merge(tableClients, tableImCL, by &#x3D; &quot;IMMATRICULATION&quot;, incomparables &#x3D; NA)\n\n\nsummary(tableCLFinal$COULEUR)\n\n\n   Length     Class      Mode \n    11082 character character \n\nstr(tableCLFinal)\ntableCLFinal$TAUX &lt;- as.integer(tableCLFinal$TAUX)\ntableCLFinal &lt;- subset(tableCLFinal, select&#x3D;-IMMATRICULATION)\n\n&#39;data.frame&#39;:    11082 obs. of  8 variables:\n $ IMMATRICULATION   : chr  &quot;1 EF 20&quot; &quot;10 ZM 12&quot; &quot;100 XL 72&quot; &quot;100 YT 70&quot; ...\n $ AGE               : num  18 33 26 76 20 27 45 28 28 38 ...\n $ SEXE              : chr  &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;F&quot; ...\n $ TAUX              : chr  &quot;594&quot; &quot;1380&quot; &quot;597&quot; &quot;587&quot; ...\n $ SITUATIONFAMILIALE: chr  &quot;celibataire&quot; &quot;En Couple&quot; &quot;En Couple&quot; &quot;En Couple&quot; ...\n $ NBENFANTSACHARGE  : num  0 4 0 1 0 0 0 2 2 0 ...\n $ DEUXIEMEVOITURE   : chr  &quot;FALSE&quot; &quot;FALSE&quot; &quot;FALSE&quot; &quot;FALSE&quot; ...\n $ COULEUR           : chr  &quot;bleu&quot; &quot;gris&quot; &quot;bleu&quot; &quot;gris&quot; ...\n\n数据可视化table(tableOCCAFinal$DEUXIEMEVOITURE,tableOCCAFinal$OCCASION)\n\n\n\n        FALSE TRUE\n  FALSE  8408 1276\n  TRUE   1234  164\n\n可视化后发现该变量和各参数没有太大的相关性，故而不做分析。\n整合模型应用于将要预测的数据集resultatTOTAL &lt;- data.frame(tableMar, resultatLG[7],resultatNB[7],resultatOCCA[7],resultatPrix[7])\n\n\n# 将结果整合到xlsx格式文件中\ninstall.packages(&quot;openxlsx&quot;) \nlibrary(openxlsx)\nwrite.xlsx(resultatTOTAL,&quot;predictions.xlsx&quot;)\n\n\n\n","slug":"基于R推荐系统搭建","date":"2022-03-21T14:23:39.000Z","categories_index":"机器学习Machine Learning","tags_index":"机器学习,推荐系统,R语言","author_index":"Quanito"},{"id":"edb5c2518fc05606a5ee5737b98e178c","title":"pdf提取关键词-词频统计","content":"项目背景这个算是之前帮助同事解决的一个小问题。因为所在公司为外企，有大量没有归类的英文pdf文档。所以需求就是给所有的pdf生成关键词，放在txt文件或者excel文件中，方便后续的查找。\n项目仓库地址\n需求分析生成关键词的话，需要对词频有一个统计，然后过滤掉没有意义且频率较大的常用词。寻找合适的算法是解决问题的关键。最后要有一个文档的输出。所以我们要做的事情有以下几个：\n\npython读取pdf文字部分\n自然语言处理分词方法\n设定停用词，后期要根据实际情况调整\n应用词频统计算法，输出前n频率的关键词\n将结果写入txt文件或者excel\n\n具体步骤读取pdf文字部分import glob\nimport os\n\n#pdf文件的路径\n#我们先放四个英文pdf文件方便测试\npdf_path &#x3D; &quot;pdf&#x2F;&quot;\npdfs &#x3D; glob.glob(&quot;&#123;&#125;&#x2F;*.pdf&quot;.format(pdf_path))\npdfs\n\n[&#39;pdf/NASA UAM market Study 2018.pdf&#39;,\n &#39;pdf/Innovation Driving Sustainable Aviation - November 2021.pdf&#39;,\n &#39;pdf/roland_berger_urban_air_mobility_1.pdf&#39;,\n &#39;pdf/Roland_Berger_Urban_Air_Mobility 2018.pdf&#39;]\n\nfrom pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\nfrom pdfminer.converter import TextConverter\nfrom pdfminer.layout import LAParams\nfrom pdfminer.pdfpage import PDFPage\nfrom io import StringIO\n\n#平时我们用的是hanlp，在这里我们使用nltk的分词包\n# 因为是英文文档，所以直接使用hanlp的英文包\n#import hanlp\n#tokenizer &#x3D; hanlp.utils.rules.tokenize_english \n#from hanlp.utils.lang.en.english_tokenizer import tokenize_english\n#tokenizer &#x3D; tokenize_english\n\n#平时我们用的是hanlp，在这里我们使用nltk的分词包\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nfrom nltk.tokenize import WhitespaceTokenizer \nfrom collections import defaultdict\n\n# 首先要下载停用词，nltk自然语言处理包具有16种不同语言存储的停用词列表。\n# 当然，我们下载之后在&#x2F;Users&#x2F;zhangquan&#x2F;nltk_data&#x2F;corpora&#x2F;stopwords&#x2F;中可以找到16种不同语言的停用词库，自己根据业务添加需要过滤的标点符号和单词\nimport nltk\nnltk.download(&#39;stopwords&#39;)\n\n\ndef extract_pdf_content(pdf):\n    rsrcmgr &#x3D; PDFResourceManager()\n    codec &#x3D; &#39;utf-8&#39;\n    outfp &#x3D; StringIO()\n    laparams &#x3D; LAParams()\n    device &#x3D; TextConverter(rsrcmgr&#x3D;rsrcmgr, outfp&#x3D;outfp, laparams&#x3D;laparams)\n    with open(pdf, &#39;rb&#39;) as fp:\n        interpreter &#x3D; PDFPageInterpreter(rsrcmgr, device)\n        password &#x3D; &quot;&quot;\n        maxpages &#x3D; 0\n        caching &#x3D; True\n        pagenos&#x3D;set()\n        for page in PDFPage.get_pages(fp, pagenos, maxpages&#x3D;maxpages, password&#x3D;password,caching&#x3D;caching, check_extractable&#x3D;True):\n            interpreter.process_page(page)\n    stop_words &#x3D; set(stopwords.words(&#39;english&#39;)) \n    #word_tokens &#x3D; word_tokenize(outfp.getvalue()) \n    word_tokens &#x3D; WhitespaceTokenizer().tokenize(outfp.getvalue()) \n    mystr &#x3D; [w for w in word_tokens if not w in stop_words]\n    device.close()\n    outfp.close()\n    return mystr\n\nTF-IDF算法词频统计import math\nimport operator\nimport pandas as pd\nimport xlwt\n\ndef feature_select(list_words):\n    #总词频统计\n    doc_frequency&#x3D;defaultdict(int)\n    for word_list in list_words:\n        for i in word_list:\n            doc_frequency[i]+&#x3D;1\n \n    #计算每个词的TF值\n    word_tf&#x3D;&#123;&#125;  #存储没个词的tf值\n    for i in doc_frequency:\n        word_tf[i]&#x3D;doc_frequency[i]&#x2F;sum(doc_frequency.values())\n \n    #计算每个词的IDF值\n    doc_num&#x3D;len(list_words)\n    word_idf&#x3D;&#123;&#125; #存储每个词的idf值\n    word_doc&#x3D;defaultdict(int) #存储包含该词的文档数\n    for i in doc_frequency:\n        for j in list_words:\n            if i in j:\n                word_doc[i]+&#x3D;1\n    for i in doc_frequency:\n        word_idf[i]&#x3D;math.log(doc_num&#x2F;(word_doc[i]+1))\n \n    #计算每个词的TF*IDF的值\n    word_tf_idf&#x3D;&#123;&#125;\n    for i in doc_frequency:\n        word_tf_idf[i]&#x3D;word_tf[i]*word_idf[i]\n \n    # 对字典按值由大到小排序\n    # 这里可以调整输出关键词的个数\n    dict_feature_select&#x3D;sorted(word_tf_idf.items(),key&#x3D;operator.itemgetter(1),reverse&#x3D;True)\n    return dict_feature_select[-10:]\n\n\n保存结果#  将数据写入新文件\ndef data_write(file_path, datas, pdf):\n    f &#x3D; xlwt.Workbook()\n    sheet1 &#x3D; f.add_sheet(u&#39;sheet1&#39;,cell_overwrite_ok&#x3D;True) #创建sheet\n    \n    #将数据写入第 i 行，第 j 列\n    i &#x3D; 0\n    for data in datas:\n        for j in range(len(data)):\n                sheet1.write(i,j,data[j])\n        i &#x3D; i + 1        \n    f.save(file_path) #保存文件\n总函数调用运行mydict &#x3D; &#123;&#125;\ndatas &#x3D; []\nj &#x3D; 0\nfor pdf in pdfs:    \n    key &#x3D; pdf.split(&#39;&#x2F;&#39;)[-1]    \n    if not key in mydict:        \n        print(&quot;Extracting content from &#123;&#125; ...&quot;.format(pdf))  \n        mydict[key] &#x3D; extract_pdf_content(pdf)\n        features&#x3D;feature_select([mydict[key]])\n        #print(features[0])\n        data&#x3D;[pdf,features[9][0],features[8][0],features[7][0],features[6][0],features[5][0],features[4][0],features[3][0],features[2][0],features[1][0],features[0][0]]\n        for i in range (0, len(data)):\n            data[i] &#x3D; str (data[i])\n        str1 &#x3D; &quot; \\n&quot;\n        str1 &#x3D; str1.join(data)\n        with open(&quot;test.txt&quot;,&quot;a&quot;) as f:\n                f.write(str1)\n                f.write(&quot;\\n&quot;)\nExtracting content from pdf&#x2F;NASA UAM market Study 2018.pdf …Extracting content from pdf&#x2F;Innovation Driving Sustainable Aviation - November 2021.pdf …Extracting content from pdf&#x2F;roland_berger_urban_air_mobility_1.pdf …Extracting content from pdf&#x2F;Roland_Berger_Urban_Air_Mobility 2018.pdf …\n结果展示完美！\npdf&#x2F;NASA UAM market Study 2018.pdf \nsolely \nAir \nUAM \naddressed. \nclient \nintended \nconfidential \ndocument \nmarket \naircraft\npdf&#x2F;Innovation Driving Sustainable Aviation - November 2021.pdf \naviation \naircraft \nSAF \nICAO \nemissions \nsustainable \nAviation \nfuel \npresented \ntechnology\npdf&#x2F;roland_berger_urban_air_mobility_1.pdf \nUAM \npassenger \nmarket \nindustry \nbusiness \nmodel \neVTOL \nvalue \ndrone \nAir\npdf&#x2F;Roland_Berger_Urban_Air_Mobility 2018.pdf \nair \nmobility \nurban \naircraft \nRoland \nUrban \nlanding \nBerger \nUAM \nuse","slug":"pdf提取关键词-词频统计","date":"2022-03-17T16:39:10.000Z","categories_index":"自然语言处理NLP","tags_index":"自然语言处理,词频统计,关键词,NLP","author_index":"Quanito"},{"id":"adec8ed7d2062f729277d84bfa0e38f0","title":"Markdown使用手册","content":"注意：本博客搭建使用的框架是基于Node.js的hexo，主题是Aurora。所以无法兼容Markdown的某些功能，比如流程图，Latex公式，注脚，还有一部分HTML原生代码😊\n为了提高我自己的开发效率，我打算从头写一边Markdown开发笔记。并且加入一些个人的看法和备注。\n因为之前有过Latex和前端的基础。对于这种新接触到的排版类型的语言接受能力还是很高的。在此非常感谢Latex陪伴我走过很多年，写过大大小小的报告和论文，有机会我会把我常用的latex模板放上来！！！\n废话不多说，步入正题！\n\n字体*斜体*或_斜体_\n**粗体**\n***加粗斜体***\n~~删除线~~\n效果：斜体或_斜体_粗体加粗斜体删除线\n分级标题注意空格！！！\n# 一级标题\n## 二级标题\n### 三级标题\n#### 四级标题\n##### 五级标题\n###### 六级标题\n效果就不展示了，导航栏会乱。\n超链接有很多种写法，只列出我自己喜欢的行内式。如果在文本中多次引用可以使用参考式。但是我个人觉得参考式虽然代码清爽了那么一丢丢，后期如果出错会很麻烦！！因为IDEA对markdown文件中的对象不支持跳转，所以更改的时候要先看看改哪里，然后记住代号，然后去文件末尾找一堆堆的链接。。。。\n[]里写链接文字，()里写链接地址, ()中的”“中可以为链接指定title属性，title属性可加可不加。title属性的效果是鼠标悬停在链接上会出现指定的 title文字。[链接文字](链接地址 “链接标题”)’这样的形式。链接地址与链接标题前有一个空格。\n欢迎来到[梵居闹市](http:&#x2F;&#x2F;blog.leanote.com&#x2F;freewalk)\n欢迎来到[梵居闹市](http:&#x2F;&#x2F;blog.leanote.com&#x2F;freewalk &quot;梵居闹市&quot;)\n&lt;http:&#x2F;&#x2F;example.com&#x2F;&gt;\n&lt;address@example.com&gt;\n\n效果：欢迎来到梵居闹市欢迎来到梵居闹市http://example.com/&#x61;&#x64;&#100;&#114;&#x65;&#x73;&#115;&#64;&#x65;&#120;&#97;&#109;&#x70;&#108;&#x65;&#x2e;&#99;&#x6f;&#x6d;\n页内跳转超链接## 0. 目录&#123;#index&#125;\n\n跳转到[目录](#index)\n效果就不展示了。。。\n列表无序列表注意空格！！！\n- 无序列表项 一\n- 无序列表项 二\n- 无序列表项 三\n效果：\n\n无序列表项 一\n无序列表项 二\n无序列表项 三有序列表感觉智商有被侮辱到 -.-…1. 有序列表项 一\n2. 有序列表项 二\n3. 有序列表项 三\n效果：\n\n\n有序列表项 一\n有序列表项 二\n有序列表项 三\n\n至于定义型列表。。。个人感觉和这个有序列表一样没必要。不就是代码块中列定义，纯属脱裤子放屁多此一举。\n列表缩进这个可以让列表下方跟第一行缩进一致，效果还挺好看的。\n*   轻轻的我走了， 正如我轻轻的来； 我轻轻的招手， 作别西天的云彩。\n那河畔的金柳， 是夕阳中的新娘； 波光里的艳影， 在我的心头荡漾。 \n软泥上的青荇， 油油的在水底招摇； 在康河的柔波里， 我甘心做一条水草！\n     那榆荫下的一潭， 不是清泉， 是天上虹； 揉碎在浮藻间， 沉淀着彩虹似的梦。 \n寻梦？撑一支长篙， 向青草更青处漫溯； 满载一船星辉， 在星辉斑斓里放歌。 \n但我不能放歌， 悄悄是别离的笙箫； 夏虫也为我沉默， 沉默是今晚的康桥！ \n*    悄悄的我走了， 正如我悄悄的来； 我挥一挥衣袖， 不带走一片云彩。\n\n\n轻轻的我走了， 正如我轻轻的来； 我轻轻的招手， 作别西天的云彩。那河畔的金柳， 是夕阳中的新娘； 波光里的艳影， 在我的心头荡漾。软泥上的青荇， 油油的在水底招摇； 在康河的柔波里， 我甘心做一条水草！ 那榆荫下的一潭， 不是清泉， 是天上虹； 揉碎在浮藻间， 沉淀着彩虹似的梦。寻梦？撑一支长篙， 向青草更青处漫溯； 满载一船星辉， 在星辉斑斓里放歌。但我不能放歌， 悄悄是别离的笙箫； 夏虫也为我沉默， 沉默是今晚的康桥！ \n悄悄的我走了， 正如我悄悄的来； 我挥一挥衣袖， 不带走一片云彩。\n\n大概有用的就是这些，其他的设计代码排版还不如直接截图\n引用普通引用Markdown 允许你只在整个段落的第一行最前面加上 &gt; ，效果如下：\n\n\n\n\n\n\n\n\n\n这是一个有两段文字的引用,无意义的占行文字1.无意义的占行文字2.\n无意义的占行文字3.无意义的占行文字4.\n多层引用&gt;&gt;&gt; 请问 Markdwon 怎么用？ - 小白\n&gt;&gt;&gt;\n&gt;&gt; 自己看教程！ - 愤青\n&gt;&gt;\n&gt; 教程在哪？ - 小白\n&gt;\n\n\n\n\n\n\n\n\n\n\n\n请问 Markdwon 怎么用？ - 小白\n自己看教程！ - 愤青\n\n教程在哪？ - 小白\n\n引用中添加代码&gt; 1.   这是第一行列表项。\n&gt; 2.   这是第二行列表项。\n&gt; \n&gt; 给出一些例子代码：\n&gt; \n&gt;     return shell_exec(&quot;echo $input | $markdown_script&quot;);\n效果：\n\n\n\n\n\n\n\n\n\n\n这是第一行列表项。\n这是第二行列表项。\n\n给出一些例子代码：\nreturn shell_exec(&quot;echo $input | $markdown_script&quot;);\n\n插入图像我只使用行内式，参考式虽然代码看起来清爽一丢丢，但是后期有问题的话维护成本太高。\n![波尔多](https:&#x2F;&#x2F;user-images.githubusercontent.com&#x2F;59725125&#x2F;145532785-07737376-50bd-4754-8fe2-2aa7ed01fa4d.jpg &quot;波尔多&quot;)\n这个时候有一个问题，Markdown只会把图片原尺寸原封不动的展示，无法调节图片大小。幸运的是Markdown可以直接写HTML！！！然后就没问题了是不。\n&lt;p style&#x3D;&quot;text-align:center&quot;&gt;\n&lt;img src&#x3D;&quot;https:&#x2F;&#x2F;user-images.githubusercontent.com&#x2F;59725125&#x2F;145532785-07737376-50bd-4754-8fe2-2aa7ed01fa4d.jpg&quot;  height&#x3D;&quot;330&quot; width&#x3D;&quot;495&quot;&gt;\n&lt;&#x2F;p&gt;\n\n\n\n\n注脚使用 Markdown[^1]可以效率的书写文档, 直接转换成 HTML[^2], 你可以使用 Leanote[^Le] 编辑器进行书写。\n\n[^1]:Markdown是一种纯文本标记语言\n[^2]:HyperText Markup Language 超文本标记语言\n[^Le]:开源笔记平台，支持Markdown和笔记直接发为博文\n效果：使用 Markdown^1可以效率的书写文档, 直接转换成 HTML[^2], 你可以使用 Leanote[^Le] 编辑器进行书写。\n[^2]:HyperText Markup Language 超文本标记语言[^Le]:开源笔记平台，支持Markdown和笔记直接发为博文\nLatex 公式这是我万万没有想到的。有了Latex支持，岂不是数学公式什么的轻轻松松搞定。直接用美元符号$框起来就行，两个美元是整行公式。懂Latex的朋友就知道多简单了。\n质能守恒方程可以用一个很简洁的方程式 $E&#x3D;mc^2$ 来表达。\n$$f(x_1,x_x,\\ldots,x_n) &#x3D; x_1^2 + x_2^2 + \\cdots + x_n^2 $$\n$$\\sum^&#123;j-1&#125;_&#123;k&#x3D;0&#125;&#123;\\widehat&#123;\\gamma&#125;_&#123;kj&#125; z_k&#125;$$\n质能守恒方程可以用一个很简洁的方程式 $E&#x3D;mc^2$ 来表达。$$f(x_1,x_x,\\ldots,x_n) &#x3D; x_1^2 + x_2^2 + \\cdots + x_n^2 $$$$\\sum^{j-1}{k&#x3D;0}{\\widehat{\\gamma}{kj} z_k}$$\n流程图暂时没找到案例，网上给的以往的代码都实现不了。\nflow\nst&#x3D;&gt;start: Start:&gt;https:&#x2F;&#x2F;www.zybuluo.com\nio&#x3D;&gt;inputoutput: verification\nop&#x3D;&gt;operation: Your Operation\ncond&#x3D;&gt;condition: Yes or No?\nsub&#x3D;&gt;subroutine: Your Subroutine\ne&#x3D;&gt;end\nst-&gt;io-&gt;op-&gt;cond\ncond(yes)-&gt;e\ncond(no)-&gt;sub-&gt;io\n\n如果有人知道怎么做，欢迎留言板告诉我哟~\n表格|学号|姓名|分数|\n|-|-|-|\n|小明|男|75|\n|小红|女|79|\n|小陆|男|92|\n\n\n\n\n学号\n姓名\n分数\n\n\n\n小明\n男\n75\n\n\n小红\n女\n79\n\n\n小陆\n男\n92\n\n\n","slug":"Markdown","date":"2021-12-10T07:39:10.000Z","categories_index":"Web development","tags_index":"Markdown","author_index":"Quanito"}]