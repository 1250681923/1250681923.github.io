{"title":"pdf提取关键词-词频统计","uid":"edb5c2518fc05606a5ee5737b98e178c","slug":"pdf提取关键词-词频统计","date":"2022-03-17T16:39:10.000Z","updated":"2024-11-05T08:21:03.408Z","comments":true,"path":"api/articles/pdf提取关键词-词频统计.json","keywords":null,"cover":"/img/IMG_1267.JPG","content":"<h2 id=\"项目背景\"><a href=\"#项目背景\" class=\"headerlink\" title=\"项目背景\"></a>项目背景</h2><p>这个算是之前帮助同事解决的一个小问题。<br>因为所在公司为外企，有大量没有归类的英文pdf文档。<br>所以需求就是给所有的pdf生成关键词，放在txt文件或者excel文件中，方便后续的查找。</p>\n<p><a href=\"https://github.com/1250681923/Keyword-extraction.git\">项目仓库地址</a></p>\n<h2 id=\"需求分析\"><a href=\"#需求分析\" class=\"headerlink\" title=\"需求分析\"></a>需求分析</h2><p>生成关键词的话，需要对词频有一个统计，然后过滤掉没有意义且频率较大的常用词。<br>寻找合适的算法是解决问题的关键。<br>最后要有一个文档的输出。<br>所以我们要做的事情有以下几个：</p>\n<ul>\n<li>python读取pdf文字部分</li>\n<li>自然语言处理分词方法</li>\n<li>设定停用词，后期要根据实际情况调整</li>\n<li>应用词频统计算法，输出前n频率的关键词</li>\n<li>将结果写入txt文件或者excel</li>\n</ul>\n<h2 id=\"具体步骤\"><a href=\"#具体步骤\" class=\"headerlink\" title=\"具体步骤\"></a>具体步骤</h2><h3 id=\"读取pdf文字部分\"><a href=\"#读取pdf文字部分\" class=\"headerlink\" title=\"读取pdf文字部分\"></a>读取pdf文字部分</h3><pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">import glob\nimport os\n\n#pdf文件的路径\n#我们先放四个英文pdf文件方便测试\npdf_path &#x3D; &quot;pdf&#x2F;&quot;\npdfs &#x3D; glob.glob(&quot;&#123;&#125;&#x2F;*.pdf&quot;.format(pdf_path))\npdfs</code></pre>\n\n<pre><code>[&#39;pdf/NASA UAM market Study 2018.pdf&#39;,\n &#39;pdf/Innovation Driving Sustainable Aviation - November 2021.pdf&#39;,\n &#39;pdf/roland_berger_urban_air_mobility_1.pdf&#39;,\n &#39;pdf/Roland_Berger_Urban_Air_Mobility 2018.pdf&#39;]\n</code></pre>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\nfrom pdfminer.converter import TextConverter\nfrom pdfminer.layout import LAParams\nfrom pdfminer.pdfpage import PDFPage\nfrom io import StringIO\n\n#平时我们用的是hanlp，在这里我们使用nltk的分词包\n# 因为是英文文档，所以直接使用hanlp的英文包\n#import hanlp\n#tokenizer &#x3D; hanlp.utils.rules.tokenize_english \n#from hanlp.utils.lang.en.english_tokenizer import tokenize_english\n#tokenizer &#x3D; tokenize_english\n\n#平时我们用的是hanlp，在这里我们使用nltk的分词包\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nfrom nltk.tokenize import WhitespaceTokenizer \nfrom collections import defaultdict\n\n# 首先要下载停用词，nltk自然语言处理包具有16种不同语言存储的停用词列表。\n# 当然，我们下载之后在&#x2F;Users&#x2F;zhangquan&#x2F;nltk_data&#x2F;corpora&#x2F;stopwords&#x2F;中可以找到16种不同语言的停用词库，自己根据业务添加需要过滤的标点符号和单词\nimport nltk\nnltk.download(&#39;stopwords&#39;)\n\n\ndef extract_pdf_content(pdf):\n    rsrcmgr &#x3D; PDFResourceManager()\n    codec &#x3D; &#39;utf-8&#39;\n    outfp &#x3D; StringIO()\n    laparams &#x3D; LAParams()\n    device &#x3D; TextConverter(rsrcmgr&#x3D;rsrcmgr, outfp&#x3D;outfp, laparams&#x3D;laparams)\n    with open(pdf, &#39;rb&#39;) as fp:\n        interpreter &#x3D; PDFPageInterpreter(rsrcmgr, device)\n        password &#x3D; &quot;&quot;\n        maxpages &#x3D; 0\n        caching &#x3D; True\n        pagenos&#x3D;set()\n        for page in PDFPage.get_pages(fp, pagenos, maxpages&#x3D;maxpages, password&#x3D;password,caching&#x3D;caching, check_extractable&#x3D;True):\n            interpreter.process_page(page)\n    stop_words &#x3D; set(stopwords.words(&#39;english&#39;)) \n    #word_tokens &#x3D; word_tokenize(outfp.getvalue()) \n    word_tokens &#x3D; WhitespaceTokenizer().tokenize(outfp.getvalue()) \n    mystr &#x3D; [w for w in word_tokens if not w in stop_words]\n    device.close()\n    outfp.close()\n    return mystr</code></pre>\n\n<h3 id=\"TF-IDF算法词频统计\"><a href=\"#TF-IDF算法词频统计\" class=\"headerlink\" title=\"TF-IDF算法词频统计\"></a>TF-IDF算法词频统计</h3><pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">import math\nimport operator\nimport pandas as pd\nimport xlwt\n\ndef feature_select(list_words):\n    #总词频统计\n    doc_frequency&#x3D;defaultdict(int)\n    for word_list in list_words:\n        for i in word_list:\n            doc_frequency[i]+&#x3D;1\n \n    #计算每个词的TF值\n    word_tf&#x3D;&#123;&#125;  #存储没个词的tf值\n    for i in doc_frequency:\n        word_tf[i]&#x3D;doc_frequency[i]&#x2F;sum(doc_frequency.values())\n \n    #计算每个词的IDF值\n    doc_num&#x3D;len(list_words)\n    word_idf&#x3D;&#123;&#125; #存储每个词的idf值\n    word_doc&#x3D;defaultdict(int) #存储包含该词的文档数\n    for i in doc_frequency:\n        for j in list_words:\n            if i in j:\n                word_doc[i]+&#x3D;1\n    for i in doc_frequency:\n        word_idf[i]&#x3D;math.log(doc_num&#x2F;(word_doc[i]+1))\n \n    #计算每个词的TF*IDF的值\n    word_tf_idf&#x3D;&#123;&#125;\n    for i in doc_frequency:\n        word_tf_idf[i]&#x3D;word_tf[i]*word_idf[i]\n \n    # 对字典按值由大到小排序\n    # 这里可以调整输出关键词的个数\n    dict_feature_select&#x3D;sorted(word_tf_idf.items(),key&#x3D;operator.itemgetter(1),reverse&#x3D;True)\n    return dict_feature_select[-10:]\n</code></pre>\n\n<h3 id=\"保存结果\"><a href=\"#保存结果\" class=\"headerlink\" title=\"保存结果\"></a>保存结果</h3><pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">#  将数据写入新文件\ndef data_write(file_path, datas, pdf):\n    f &#x3D; xlwt.Workbook()\n    sheet1 &#x3D; f.add_sheet(u&#39;sheet1&#39;,cell_overwrite_ok&#x3D;True) #创建sheet\n    \n    #将数据写入第 i 行，第 j 列\n    i &#x3D; 0\n    for data in datas:\n        for j in range(len(data)):\n                sheet1.write(i,j,data[j])\n        i &#x3D; i + 1        \n    f.save(file_path) #保存文件</code></pre>\n<h3 id=\"总函数调用运行\"><a href=\"#总函数调用运行\" class=\"headerlink\" title=\"总函数调用运行\"></a>总函数调用运行</h3><pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">mydict &#x3D; &#123;&#125;\ndatas &#x3D; []\nj &#x3D; 0\nfor pdf in pdfs:    \n    key &#x3D; pdf.split(&#39;&#x2F;&#39;)[-1]    \n    if not key in mydict:        \n        print(&quot;Extracting content from &#123;&#125; ...&quot;.format(pdf))  \n        mydict[key] &#x3D; extract_pdf_content(pdf)\n        features&#x3D;feature_select([mydict[key]])\n        #print(features[0])\n        data&#x3D;[pdf,features[9][0],features[8][0],features[7][0],features[6][0],features[5][0],features[4][0],features[3][0],features[2][0],features[1][0],features[0][0]]\n        for i in range (0, len(data)):\n            data[i] &#x3D; str (data[i])\n        str1 &#x3D; &quot; \\n&quot;\n        str1 &#x3D; str1.join(data)\n        with open(&quot;test.txt&quot;,&quot;a&quot;) as f:\n                f.write(str1)\n                f.write(&quot;\\n&quot;)</code></pre>\n<p>Extracting content from pdf&#x2F;NASA UAM market Study 2018.pdf …<br>Extracting content from pdf&#x2F;Innovation Driving Sustainable Aviation - November 2021.pdf …<br>Extracting content from pdf&#x2F;roland_berger_urban_air_mobility_1.pdf …<br>Extracting content from pdf&#x2F;Roland_Berger_Urban_Air_Mobility 2018.pdf …</p>\n<h2 id=\"结果展示\"><a href=\"#结果展示\" class=\"headerlink\" title=\"结果展示\"></a>结果展示</h2><p>完美！</p>\n<pre class=\"line-numbers language-text\" data-language=\"text\"><code class=\"language-text\">pdf&#x2F;NASA UAM market Study 2018.pdf \nsolely \nAir \nUAM \naddressed. \nclient \nintended \nconfidential \ndocument \nmarket \naircraft\npdf&#x2F;Innovation Driving Sustainable Aviation - November 2021.pdf \naviation \naircraft \nSAF \nICAO \nemissions \nsustainable \nAviation \nfuel \npresented \ntechnology\npdf&#x2F;roland_berger_urban_air_mobility_1.pdf \nUAM \npassenger \nmarket \nindustry \nbusiness \nmodel \neVTOL \nvalue \ndrone \nAir\npdf&#x2F;Roland_Berger_Urban_Air_Mobility 2018.pdf \nair \nmobility \nurban \naircraft \nRoland \nUrban \nlanding \nBerger \nUAM \nuse</code></pre>","text":"项目背景这个算是之前帮助同事解决的一个小问题。因为所在公司为外企，有大量没有归类的英文pdf文档。所以需求就是给所有的pdf生成关键词，放在txt文件或者excel文件中，方便后续的查找。 项目仓库地址 需求分析生成关键词的话，需要对词频有一个统计，然后过滤掉没有意义且频率较大的...","link":"","photos":[],"count_time":{"symbolsCount":"5.7k","symbolsTime":"5 mins."},"categories":[{"name":"自然语言处理NLP","slug":"自然语言处理NLP","count":1,"path":"api/categories/自然语言处理NLP.json"}],"tags":[{"name":"自然语言处理","slug":"自然语言处理","count":1,"path":"api/tags/自然语言处理.json"},{"name":"词频统计","slug":"词频统计","count":1,"path":"api/tags/词频统计.json"},{"name":"关键词","slug":"关键词","count":1,"path":"api/tags/关键词.json"},{"name":"NLP","slug":"NLP","count":1,"path":"api/tags/NLP.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E9%A1%B9%E7%9B%AE%E8%83%8C%E6%99%AF\"><span class=\"toc-text\">项目背景</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90\"><span class=\"toc-text\">需求分析</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%85%B7%E4%BD%93%E6%AD%A5%E9%AA%A4\"><span class=\"toc-text\">具体步骤</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%AF%BB%E5%8F%96pdf%E6%96%87%E5%AD%97%E9%83%A8%E5%88%86\"><span class=\"toc-text\">读取pdf文字部分</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#TF-IDF%E7%AE%97%E6%B3%95%E8%AF%8D%E9%A2%91%E7%BB%9F%E8%AE%A1\"><span class=\"toc-text\">TF-IDF算法词频统计</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E4%BF%9D%E5%AD%98%E7%BB%93%E6%9E%9C\"><span class=\"toc-text\">保存结果</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E6%80%BB%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8%E8%BF%90%E8%A1%8C\"><span class=\"toc-text\">总函数调用运行</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E7%BB%93%E6%9E%9C%E5%B1%95%E7%A4%BA\"><span class=\"toc-text\">结果展示</span></a></li></ol>","author":{"name":"Quanito","slug":"blog-author","avatar":"/staticImg/avatar.jpg","link":"/","description":"大道五十，天衍四十九，人遁其一","socials":{"github":"https://github.com/1250681923","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{"GoogleMail":{"icon":"/img/email.png","link":"mailto:code.quan666zhang@gmail.com"}}}},"mapped":true,"prev_post":{"title":"基于R推荐系统搭建","uid":"53b940f6ea7d5afb2d81c3c681083d3b","slug":"基于R推荐系统搭建","date":"2022-03-21T14:23:39.000Z","updated":"2024-11-05T09:23:33.279Z","comments":true,"path":"api/articles/基于R推荐系统搭建.json","keywords":null,"cover":"/img/IMG_1267.JPG","text":"前言首先，R语言牛皮之处就不过多阐述，总之很方便很方便。R语言就是为了数据处理而生的，可以轻松链接数据库，可以轻松对数据进行处理和分析。 这篇博客主要归纳一下如何使用R语言搭建一个简单的推荐系统。首先导入包： #install.packages(&quot;RJDBC&quot;...","link":"","photos":[],"count_time":{"symbolsCount":"43k","symbolsTime":"39 mins."},"categories":[{"name":"机器学习Machine Learning","slug":"机器学习Machine-Learning","count":3,"path":"api/categories/机器学习Machine-Learning.json"}],"tags":[{"name":"机器学习","slug":"机器学习","count":3,"path":"api/tags/机器学习.json"},{"name":"推荐系统","slug":"推荐系统","count":3,"path":"api/tags/推荐系统.json"},{"name":"R语言","slug":"R语言","count":1,"path":"api/tags/R语言.json"}],"author":{"name":"Quanito","slug":"blog-author","avatar":"/staticImg/avatar.jpg","link":"/","description":"大道五十，天衍四十九，人遁其一","socials":{"github":"https://github.com/1250681923","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{"GoogleMail":{"icon":"/img/email.png","link":"mailto:code.quan666zhang@gmail.com"}}}}},"next_post":{}}